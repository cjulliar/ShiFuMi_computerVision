{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle chargé avec succès\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 laptop, 217.3ms\n",
      "Speed: 3.6ms preprocess, 217.3ms inference, 487.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[51.31414794921875, 148.71612548828125, 550.3404541015625, 479.36407470703125], [0.1330718994140625, 304.0411682128906, 72.59317016601562, 479.1033630371094], [75.76909637451172, 358.4218444824219, 211.118896484375, 436.0804748535156], [43.17866134643555, 359.08502197265625, 85.10755920410156, 417.37884521484375]], 'scores': [0.957360029220581, 0.8797934651374817, 0.3386702537536621, 0.31413382291793823], 'class_indices': [0.0, 0.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[06/27/24 15:14:28] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do not  <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         match the expected format of object detection prediction that could be    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[06/27/24 15:14:28]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do not  \u001b]8;id=165067;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=71039;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         match the expected format of object detection prediction that could be    \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m                    \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 laptop, 209.4ms\n",
      "Speed: 4.7ms preprocess, 209.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[50.82951354980469, 150.46929931640625, 544.2621459960938, 479.3453369140625], [0.13271331787109375, 304.4547119140625, 72.3946304321289, 478.0994873046875], [82.9581069946289, 358.69293212890625, 213.20803833007812, 425.79718017578125]], 'scores': [0.9592157006263733, 0.8211302757263184, 0.45310115814208984], 'class_indices': [0.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 208.8ms\n",
      "Speed: 2.3ms preprocess, 208.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[50.80511474609375, 150.3782958984375, 543.5784301757812, 479.351318359375], [0.1595611572265625, 303.987060546875, 72.90586853027344, 477.79766845703125], [85.08219146728516, 358.9324951171875, 205.57479858398438, 422.67578125]], 'scores': [0.9549925923347473, 0.7066422700881958, 0.47620832920074463], 'class_indices': [0.0, 0.0, 63.0]}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:29] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=613965;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=206454;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=331721;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=719964;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 laptop, 207.4ms\n",
      "Speed: 3.1ms preprocess, 207.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[50.66363525390625, 150.30477905273438, 543.788330078125, 479.3279724121094], [0.160736083984375, 303.823486328125, 72.50770568847656, 479.1737060546875], [83.63182067871094, 358.86944580078125, 208.92176818847656, 424.39349365234375], [43.56217575073242, 359.86749267578125, 87.74656677246094, 437.425048828125]], 'scores': [0.9587002396583557, 0.8871150016784668, 0.37150710821151733, 0.3680805563926697], 'class_indices': [0.0, 0.0, 63.0, 0.0]}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:30] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:30]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=615185;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=718580;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 laptop, 206.7ms\n",
      "Speed: 3.1ms preprocess, 206.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[50.62178039550781, 150.2748565673828, 543.4921264648438, 479.32928466796875], [0.16397857666015625, 304.24969482421875, 72.59256744384766, 457.60626220703125], [84.94188690185547, 359.5340270996094, 191.7010498046875, 424.2268371582031], [43.151153564453125, 359.76202392578125, 88.39369201660156, 438.51007080078125]], 'scores': [0.9565332531929016, 0.6953204274177551, 0.44080421328544617, 0.3962019383907318], 'class_indices': [0.0, 0.0, 63.0, 0.0]}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=951947;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=566992;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 laptop, 208.9ms\n",
      "Speed: 2.8ms preprocess, 208.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[49.70204162597656, 150.51307678222656, 543.056396484375, 479.3145751953125], [0.1948699951171875, 303.97735595703125, 72.27429962158203, 478.47625732421875], [44.89839172363281, 359.38287353515625, 87.21327209472656, 437.01800537109375], [83.99774932861328, 358.83740234375, 191.63751220703125, 424.3897705078125]], 'scores': [0.9575397968292236, 0.8627575635910034, 0.3441985249519348, 0.2938273549079895], 'class_indices': [0.0, 0.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:31] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:31]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=290316;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=941009;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tv, 210.2ms\n",
      "Speed: 2.4ms preprocess, 210.2ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[49.67616271972656, 150.29620361328125, 542.3084716796875, 479.33587646484375], [0.1554412841796875, 303.9560241699219, 72.6591567993164, 478.8922424316406], [43.541202545166016, 359.45611572265625, 88.00180053710938, 436.62786865234375], [85.3053970336914, 359.0182800292969, 191.86068725585938, 421.5140075683594]], 'scores': [0.9617555737495422, 0.8700838685035706, 0.5478164553642273, 0.27895793318748474], 'class_indices': [0.0, 0.0, 0.0, 62.0]}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=333131;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=285231;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 208.8ms\n",
      "Speed: 2.7ms preprocess, 208.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[48.40203857421875, 150.91769409179688, 540.61083984375, 479.3495178222656], [0.1891326904296875, 303.98358154296875, 73.10228729248047, 457.40899658203125], [85.30145263671875, 359.016845703125, 201.09027099609375, 417.9862060546875]], 'scores': [0.9601407647132874, 0.6587873101234436, 0.2531898021697998], 'class_indices': [0.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=699907;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=242575;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 laptop, 213.3ms\n",
      "Speed: 2.6ms preprocess, 213.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[48.26615905761719, 150.72938537597656, 541.0347290039062, 479.343017578125], [0.2121124267578125, 304.49273681640625, 71.92007446289062, 479.312255859375], [45.0769157409668, 358.711669921875, 85.37690734863281, 433.811279296875], [81.5884017944336, 358.8096923828125, 210.12814331054688, 426.7791748046875]], 'scores': [0.9574550986289978, 0.8784012198448181, 0.3932475745677948, 0.29115936160087585], 'class_indices': [0.0, 0.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=348194;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=318277;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 1 laptop, 210.7ms\n",
      "Speed: 2.1ms preprocess, 210.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[48.8941650390625, 150.861572265625, 540.2092895507812, 479.3328857421875], [0.20896148681640625, 304.06793212890625, 72.11819458007812, 478.19671630859375], [44.045379638671875, 358.9962158203125, 86.57490539550781, 437.75946044921875], [44.114280700683594, 359.0790100097656, 83.87277221679688, 417.5668640136719], [480.33636474609375, 413.1805419921875, 518.6486206054688, 448.4327392578125], [83.82913208007812, 358.9161376953125, 206.75250244140625, 422.4293212890625]], 'scores': [0.9594573974609375, 0.8733286261558533, 0.31268638372421265, 0.27598482370376587, 0.25923842191696167, 0.2540346384048462], 'class_indices': [0.0, 0.0, 0.0, 0.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:32] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:32]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=722187;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=662351;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 1 tv, 1 laptop, 212.4ms\n",
      "Speed: 2.5ms preprocess, 212.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[48.43669128417969, 150.77191162109375, 539.7608032226562, 479.31402587890625], [0.128387451171875, 304.54193115234375, 72.07489013671875, 454.85247802734375], [44.05669021606445, 359.25128173828125, 86.76487731933594, 435.71405029296875], [83.39117431640625, 359.1993408203125, 209.99554443359375, 423.25006103515625], [480.46099853515625, 413.20367431640625, 518.8530883789062, 449.72723388671875], [83.70231628417969, 359.0911865234375, 210.47120666503906, 422.2696533203125]], 'scores': [0.9554399251937866, 0.8477546572685242, 0.418219655752182, 0.398193895816803, 0.25999367237091064, 0.25901561975479126], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=534542;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=728335;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 1 laptop, 213.1ms\n",
      "Speed: 2.4ms preprocess, 213.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[46.32246398925781, 150.16192626953125, 539.2811889648438, 479.21551513671875], [0.21176910400390625, 304.036376953125, 72.7821273803711, 478.7490234375], [82.36646270751953, 358.86749267578125, 206.30157470703125, 424.01434326171875], [44.06280517578125, 359.29736328125, 84.45857238769531, 421.228271484375], [480.132080078125, 412.83294677734375, 518.5347290039062, 451.23419189453125]], 'scores': [0.9581485986709595, 0.8849135637283325, 0.36548566818237305, 0.34713760018348694, 0.303530752658844], 'class_indices': [0.0, 0.0, 63.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=182301;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=764822;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 1 tv, 1 laptop, 209.5ms\n",
      "Speed: 2.7ms preprocess, 209.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[40.59556579589844, 152.62728881835938, 535.3131103515625, 479.4104309082031], [0.16399383544921875, 303.87335205078125, 71.83285522460938, 477.79449462890625], [45.39451217651367, 359.28009033203125, 85.96708679199219, 433.75469970703125], [85.58319091796875, 358.9761962890625, 201.58120727539062, 415.7496337890625], [85.49664306640625, 359.288818359375, 201.24905395507812, 415.38671875], [475.423583984375, 413.4579772949219, 518.638916015625, 455.4837341308594]], 'scores': [0.9613654613494873, 0.7533981800079346, 0.49681100249290466, 0.4221353530883789, 0.3937804102897644, 0.33849167823791504], 'class_indices': [0.0, 0.0, 0.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=479337;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=570672;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 tvs, 2 laptops, 213.6ms\n",
      "Speed: 3.1ms preprocess, 213.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[39.02001953125, 156.5519561767578, 524.8614501953125, 479.22784423828125], [471.12060546875, 413.49725341796875, 519.3387451171875, 466.71923828125], [384.488037109375, 372.52923583984375, 451.23675537109375, 415.44610595703125], [0.20320892333984375, 304.12542724609375, 72.80982971191406, 456.50421142578125], [44.742218017578125, 358.9042053222656, 85.18855285644531, 422.7324523925781], [85.46601867675781, 358.7115783691406, 203.74778747558594, 413.9649353027344], [85.71735382080078, 358.82135009765625, 203.32135009765625, 413.65411376953125], [384.151611328125, 372.7027587890625, 451.3740234375, 415.0975341796875]], 'scores': [0.958295464515686, 0.7912731766700745, 0.6319283843040466, 0.6083402633666992, 0.5542443990707397, 0.44821590185165405, 0.3813493847846985, 0.3060077130794525], 'class_indices': [0.0, 56.0, 63.0, 0.0, 0.0, 62.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:33] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:33]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=964469;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=624080;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 1 tv, 2 laptops, 209.9ms\n",
      "Speed: 2.7ms preprocess, 209.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[45.16162109375, 159.6733856201172, 531.20361328125, 479.26898193359375], [0.181671142578125, 304.32220458984375, 73.0256118774414, 478.86041259765625], [472.0262451171875, 413.52105712890625, 518.557373046875, 463.83270263671875], [387.39678955078125, 372.567626953125, 451.35369873046875, 409.9315185546875], [45.37287902832031, 359.5410461425781, 86.45664978027344, 429.0868225097656], [84.41300964355469, 359.2472229003906, 204.5225372314453, 418.9210510253906], [84.58795928955078, 359.08343505859375, 204.22723388671875, 418.1978759765625]], 'scores': [0.9614711403846741, 0.8944821357727051, 0.6465340852737427, 0.6417438983917236, 0.4971911907196045, 0.42316558957099915, 0.35710957646369934], 'class_indices': [0.0, 0.0, 56.0, 63.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=11927;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=405048;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 1 tv, 2 laptops, 212.2ms\n",
      "Speed: 2.5ms preprocess, 212.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[40.04127502441406, 159.91653442382812, 528.238037109375, 479.2640075683594], [0.1893310546875, 303.80841064453125, 72.38519287109375, 476.801513671875], [471.736328125, 413.43597412109375, 518.9423217773438, 465.37896728515625], [45.3109130859375, 359.5238037109375, 86.39714050292969, 428.16064453125], [85.47966766357422, 358.9620666503906, 204.1353759765625, 416.0827941894531], [385.07769775390625, 373.0113525390625, 451.23980712890625, 413.33837890625], [85.61289978027344, 358.8208312988281, 203.9837188720703, 416.0106506347656]], 'scores': [0.9610687494277954, 0.8092625737190247, 0.7172608375549316, 0.5807714462280273, 0.3781029284000397, 0.36464831233024597, 0.3342790901660919], 'class_indices': [0.0, 0.0, 56.0, 0.0, 63.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=117930;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=330575;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 tvs, 2 laptops, 208.9ms\n",
      "Speed: 2.2ms preprocess, 208.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[42.0504150390625, 156.86578369140625, 526.81396484375, 479.32208251953125], [0.176513671875, 303.5704345703125, 72.0816421508789, 477.962890625], [471.60595703125, 413.2931213378906, 518.7276611328125, 466.9678039550781], [45.673828125, 359.3011169433594, 86.6304931640625, 429.4471740722656], [85.13851928710938, 359.0733337402344, 203.38284301757812, 416.4121398925781], [384.847412109375, 372.6541748046875, 451.1292724609375, 413.22686767578125], [84.91915130615234, 358.9403076171875, 204.14944458007812, 416.59222412109375], [384.4130859375, 372.7318115234375, 451.19207763671875, 413.1436767578125]], 'scores': [0.9605802297592163, 0.8214735984802246, 0.7171780467033386, 0.5302168726921082, 0.5225362777709961, 0.4588615596294403, 0.3595031201839447, 0.3514975905418396], 'class_indices': [0.0, 0.0, 56.0, 0.0, 63.0, 63.0, 62.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=289387;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=744380;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 1 tv, 2 laptops, 213.8ms\n",
      "Speed: 2.9ms preprocess, 213.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[41.00189208984375, 155.6605224609375, 525.4513549804688, 479.18902587890625], [0.17433929443359375, 304.031005859375, 72.50666046142578, 478.2213134765625], [471.51983642578125, 413.3797607421875, 518.9393310546875, 466.38348388671875], [386.71221923828125, 372.6321716308594, 451.65399169921875, 412.0071105957031], [43.96674346923828, 359.73443603515625, 86.40705108642578, 427.8172607421875], [85.3580322265625, 358.882080078125, 204.823974609375, 415.9161376953125], [85.01789093017578, 358.6491394042969, 205.64910888671875, 416.1245422363281]], 'scores': [0.9600004553794861, 0.860722005367279, 0.6862561106681824, 0.6159319281578064, 0.5160231590270996, 0.4063214957714081, 0.37595903873443604], 'class_indices': [0.0, 0.0, 56.0, 63.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=157150;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=77902;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 tvs, 2 laptops, 212.5ms\n",
      "Speed: 2.3ms preprocess, 212.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[40.98484802246094, 154.75048828125, 524.7926025390625, 479.4034423828125], [0.18033599853515625, 303.77520751953125, 72.69789123535156, 479.41094970703125], [471.34503173828125, 413.0369873046875, 518.4380493164062, 467.67333984375], [45.364234924316406, 359.100830078125, 87.13069915771484, 428.25067138671875], [85.36114501953125, 358.5655517578125, 205.14398193359375, 415.859375], [384.97369384765625, 372.1478271484375, 451.29296875, 412.3953857421875], [385.02386474609375, 372.0888671875, 451.27813720703125, 412.4755859375], [84.94287872314453, 358.3490905761719, 206.274169921875, 416.1279602050781]], 'scores': [0.9567193388938904, 0.8668249249458313, 0.6158053874969482, 0.5014524459838867, 0.4287572503089905, 0.3757885694503784, 0.2736130654811859, 0.25345349311828613], 'class_indices': [0.0, 0.0, 56.0, 0.0, 63.0, 62.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:34] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=403436;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=547568;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 2 laptops, 214.5ms\n",
      "Speed: 3.1ms preprocess, 214.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[41.13262939453125, 153.10797119140625, 523.1616821289062, 479.4129638671875], [0.1594085693359375, 304.14862060546875, 72.08468627929688, 477.99554443359375], [471.459716796875, 413.6425476074219, 519.2733154296875, 465.8469543457031], [44.21778869628906, 359.29840087890625, 85.96722412109375, 429.50531005859375], [379.5836181640625, 372.8188781738281, 451.35614013671875, 414.3939514160156], [85.04025268554688, 359.39013671875, 205.55429077148438, 416.8831787109375], [380.0125732421875, 372.96771240234375, 451.5718994140625, 414.11029052734375], [225.679443359375, 423.889404296875, 266.88818359375, 479.6328125], [84.64657592773438, 359.01556396484375, 205.78912353515625, 416.9993896484375]], 'scores': [0.9510141015052795, 0.8464047908782959, 0.6665585041046143, 0.4952351450920105, 0.4847783148288727, 0.42103448510169983, 0.39754435420036316, 0.34649965167045593, 0.3105210065841675], 'class_indices': [0.0, 0.0, 56.0, 0.0, 63.0, 63.0, 62.0, 27.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=325552;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=838282;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 tvs, 214.3ms\n",
      "Speed: 1.8ms preprocess, 214.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[43.54803466796875, 151.34344482421875, 520.0723266601562, 479.40838623046875], [0.18378448486328125, 303.76824951171875, 73.0147476196289, 478.59454345703125], [470.4749755859375, 413.070068359375, 518.9515380859375, 464.1607666015625], [372.92462158203125, 372.49432373046875, 451.19927978515625, 417.34844970703125], [43.10276794433594, 358.80718994140625, 86.21002197265625, 425.69293212890625], [84.509765625, 358.7716064453125, 205.53125, 417.4027099609375]], 'scores': [0.947748064994812, 0.8701179623603821, 0.7654906511306763, 0.5929564237594604, 0.4717738628387451, 0.3763144016265869], 'class_indices': [0.0, 0.0, 56.0, 62.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=889836;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=401972;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 tvs, 1 laptop, 214.6ms\n",
      "Speed: 2.7ms preprocess, 214.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[40.36827087402344, 150.4290313720703, 518.05712890625, 479.44720458984375], [0.1235504150390625, 304.14715576171875, 72.26113891601562, 477.52154541015625], [470.68994140625, 413.6260986328125, 518.5218505859375, 463.9503173828125], [372.8022155761719, 373.10174560546875, 451.0917663574219, 418.6448974609375], [44.45573806762695, 359.48809814453125, 86.65242004394531, 429.12994384765625], [85.20770263671875, 359.30621337890625, 205.72970581054688, 416.14190673828125], [84.92153930664062, 359.4461669921875, 206.06716918945312, 416.13128662109375]], 'scores': [0.9489184617996216, 0.7430411577224731, 0.6866538524627686, 0.667519211769104, 0.5377374887466431, 0.39600223302841187, 0.3245944678783417], 'class_indices': [0.0, 0.0, 56.0, 62.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=92616;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=861839;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 tvs, 1 laptop, 215.2ms\n",
      "Speed: 2.7ms preprocess, 215.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[42.10105895996094, 149.69903564453125, 524.5418701171875, 479.4234619140625], [471.1185302734375, 413.37689208984375, 518.760009765625, 466.05560302734375], [84.71762084960938, 359.38128662109375, 206.182861328125, 419.12060546875], [0.20001983642578125, 304.1629638671875, 73.4212875366211, 468.1156005859375], [375.1419677734375, 372.84271240234375, 451.1810302734375, 414.18426513671875], [45.512237548828125, 359.07220458984375, 86.53598022460938, 430.10528564453125], [85.05226135253906, 359.170654296875, 205.9699249267578, 418.4267578125]], 'scores': [0.9482042193412781, 0.7328138947486877, 0.5815572142601013, 0.5636008977890015, 0.5515170693397522, 0.3064298629760742, 0.2929161489009857], 'class_indices': [0.0, 56.0, 63.0, 0.0, 62.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:35] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=519240;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=973655;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 tvs, 1 laptop, 216.5ms\n",
      "Speed: 2.0ms preprocess, 216.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[42.43695068359375, 148.62310791015625, 525.0999145507812, 479.422607421875], [0.1870269775390625, 303.9549865722656, 73.14924621582031, 475.8678894042969], [471.4931640625, 412.715087890625, 518.8866577148438, 465.91357421875], [44.83934783935547, 359.05487060546875, 85.9275894165039, 428.26422119140625], [377.87115478515625, 372.2710876464844, 451.792236328125, 412.0426940917969], [83.92493438720703, 358.7364501953125, 205.92413330078125, 419.4769287109375], [83.88633728027344, 359.22998046875, 206.1832733154297, 419.8929443359375]], 'scores': [0.952399730682373, 0.6986633539199829, 0.5992488265037537, 0.4582146108150482, 0.4228932559490204, 0.39087292551994324, 0.3472391664981842], 'class_indices': [0.0, 0.0, 56.0, 0.0, 62.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=645815;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=716433;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 tvs, 1 laptop, 209.1ms\n",
      "Speed: 3.1ms preprocess, 209.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[39.61878967285156, 149.0096435546875, 520.7122802734375, 479.43499755859375], [0.2007598876953125, 303.6898193359375, 73.63838195800781, 475.782958984375], [470.27178955078125, 413.0193176269531, 518.7192993164062, 465.9385070800781], [374.71826171875, 372.16339111328125, 451.464599609375, 417.97039794921875], [45.55101013183594, 358.7569885253906, 87.00370788574219, 427.1736755371094], [84.87395477294922, 358.9568786621094, 205.96197509765625, 416.9670715332031], [84.59368896484375, 358.8495788574219, 205.63778686523438, 417.2372131347656]], 'scores': [0.9501011371612549, 0.8023111820220947, 0.7919488549232483, 0.6635640263557434, 0.5132026672363281, 0.4189285635948181, 0.3837714195251465], 'class_indices': [0.0, 0.0, 56.0, 62.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=33623;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=896585;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 tvs, 213.3ms\n",
      "Speed: 2.5ms preprocess, 213.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[37.49974060058594, 153.26651000976562, 511.4693603515625, 479.4239807128906], [0.16770172119140625, 304.228759765625, 72.02529907226562, 478.6717529296875], [467.9512939453125, 413.8357849121094, 518.9081420898438, 464.3149719238281], [365.74456787109375, 373.2227783203125, 451.24395751953125, 428.7349853515625], [44.23371887207031, 359.86627197265625, 87.17849731445312, 435.40728759765625], [84.87026977539062, 359.6578369140625, 204.8226318359375, 415.49169921875]], 'scores': [0.9484745860099792, 0.8365271687507629, 0.7460002303123474, 0.6407669186592102, 0.4814627468585968, 0.33413776755332947], 'class_indices': [0.0, 0.0, 56.0, 62.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=920457;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=55760;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 wine glass, 1 tv, 1 laptop, 213.2ms\n",
      "Speed: 1.9ms preprocess, 213.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[34.797119140625, 157.62142944335938, 569.337158203125, 479.2382507324219], [0.204620361328125, 303.6300354003906, 74.03218078613281, 479.6321105957031], [367.81585693359375, 312.74444580078125, 479.82647705078125, 479.18951416015625], [85.74053192138672, 359.13848876953125, 204.68414306640625, 413.62567138671875], [44.7855224609375, 359.38092041015625, 85.47370910644531, 423.9896240234375], [85.58467102050781, 359.147216796875, 204.43873596191406, 413.1502685546875]], 'scores': [0.9463257193565369, 0.8501321077346802, 0.42828524112701416, 0.4145113527774811, 0.34410005807876587, 0.2774863839149475], 'class_indices': [0.0, 0.0, 40.0, 63.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=81833;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=128737;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tv, 1 laptop, 211.3ms\n",
      "Speed: 5.0ms preprocess, 211.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[39.8670654296875, 161.04244995117188, 520.0220336914062, 479.1307067871094], [0.1834716796875, 304.1434326171875, 72.8010482788086, 479.2427978515625], [84.99293518066406, 358.7857666015625, 205.7271270751953, 418.2216796875], [84.74105834960938, 359.29547119140625, 205.8511962890625, 418.16265869140625], [42.8240966796875, 359.6597900390625, 87.06269836425781, 430.1390380859375]], 'scores': [0.9573756456375122, 0.8629969358444214, 0.3841891884803772, 0.3384492099285126, 0.30524158477783203], 'class_indices': [0.0, 0.0, 62.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:36] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:36]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=347244;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=489164;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tv, 1 laptop, 211.7ms\n",
      "Speed: 3.1ms preprocess, 211.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[40.05097961425781, 154.74993896484375, 510.33453369140625, 479.2479248046875], [0.148590087890625, 304.133056640625, 71.96575927734375, 458.1451416015625], [85.01235961914062, 359.408447265625, 206.30535888671875, 416.5906982421875], [42.507110595703125, 359.11798095703125, 86.96142578125, 430.40692138671875], [85.19584655761719, 359.01251220703125, 206.12217712402344, 416.80828857421875]], 'scores': [0.953596830368042, 0.6435741186141968, 0.6217690110206604, 0.4005120098590851, 0.27761217951774597], 'class_indices': [0.0, 0.0, 63.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=725001;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=387828;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 laptop, 209.7ms\n",
      "Speed: 1.9ms preprocess, 209.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[46.4227294921875, 147.2845001220703, 520.9051513671875, 479.23162841796875], [0.15772247314453125, 303.56573486328125, 72.52943420410156, 479.29095458984375], [43.67178726196289, 358.9989013671875, 87.7322998046875, 430.40863037109375], [84.63207244873047, 359.0086669921875, 205.24981689453125, 421.4056396484375]], 'scores': [0.9564578533172607, 0.9035669565200806, 0.4697485566139221, 0.4243719279766083], 'class_indices': [0.0, 0.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=564223;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=263168;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 laptop, 207.7ms\n",
      "Speed: 2.5ms preprocess, 207.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[48.319580078125, 146.42120361328125, 524.8526611328125, 479.41241455078125], [0.161865234375, 303.64837646484375, 73.37599182128906, 478.0975341796875], [84.15852355957031, 359.0555725097656, 205.27403259277344, 420.8877868652344], [44.319419860839844, 359.10247802734375, 86.71256256103516, 425.21905517578125]], 'scores': [0.9556826949119568, 0.8475661277770996, 0.6159087419509888, 0.35841938853263855], 'class_indices': [0.0, 0.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=387834;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=327453;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 laptop, 212.6ms\n",
      "Speed: 2.4ms preprocess, 212.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[48.76348876953125, 145.92874145507812, 523.839111328125, 479.2112731933594], [0.1562042236328125, 303.8417663574219, 72.14754486083984, 479.2438659667969], [290.01055908203125, 427.42779541015625, 328.38922119140625, 479.77398681640625], [84.47845458984375, 359.22308349609375, 205.40570068359375, 423.50323486328125], [43.74679946899414, 358.4040832519531, 87.79287719726562, 435.5321350097656]], 'scores': [0.9583139419555664, 0.8949311971664429, 0.4177529513835907, 0.32258787751197815, 0.2831205725669861], 'class_indices': [0.0, 0.0, 27.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:37] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:37]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=41988;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=965354;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 laptop, 214.4ms\n",
      "Speed: 2.4ms preprocess, 214.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[49.052490234375, 146.16610717773438, 522.509765625, 479.4322814941406], [0.19283294677734375, 303.8519287109375, 73.18154907226562, 470.57421875], [82.2852783203125, 358.90203857421875, 205.42733764648438, 425.24822998046875], [43.36496353149414, 359.2884521484375, 84.90219116210938, 423.9605712890625]], 'scores': [0.9581331610679626, 0.5925259590148926, 0.3609955310821533, 0.35597655177116394], 'class_indices': [0.0, 0.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=160850;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=86199;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 laptop, 209.7ms\n",
      "Speed: 2.1ms preprocess, 209.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[49.05462646484375, 145.75148010253906, 524.770263671875, 479.45458984375], [0.14013671875, 303.5242919921875, 72.33518981933594, 477.25775146484375], [84.38482666015625, 358.9189453125, 205.37908935546875, 423.34716796875], [44.56053161621094, 358.5567321777344, 87.52522277832031, 435.4720153808594]], 'scores': [0.9570081233978271, 0.7121273875236511, 0.46863245964050293, 0.3237881064414978], 'class_indices': [0.0, 0.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=830511;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=300918;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 laptop, 211.5ms\n",
      "Speed: 2.2ms preprocess, 211.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[49.29850769042969, 146.14915466308594, 527.8886108398438, 479.2828369140625], [0.13701629638671875, 304.6470642089844, 73.47802734375, 456.8531799316406], [85.0831298828125, 358.83966064453125, 205.548095703125, 419.13970947265625], [43.21380615234375, 358.8619689941406, 85.73674011230469, 419.6030578613281], [43.37403869628906, 358.50079345703125, 92.46969604492188, 436.6053466796875]], 'scores': [0.9567360281944275, 0.5899493098258972, 0.5497274398803711, 0.3447153568267822, 0.31886813044548035], 'class_indices': [0.0, 0.0, 63.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=136444;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=348881;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 laptop, 213.5ms\n",
      "Speed: 2.3ms preprocess, 213.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[49.15301513671875, 146.85675048828125, 522.5658569335938, 479.401611328125], [0.124786376953125, 303.5491943359375, 72.7289810180664, 477.34930419921875], [80.96781158447266, 358.867431640625, 205.48153686523438, 426.82568359375], [44.34706115722656, 358.35546875, 84.80209350585938, 419.82330322265625]], 'scores': [0.9603956937789917, 0.745779275894165, 0.3946800231933594, 0.367410808801651], 'class_indices': [0.0, 0.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=475237;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=735908;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 laptop, 210.4ms\n",
      "Speed: 2.5ms preprocess, 210.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[49.85472106933594, 147.369384765625, 521.5972900390625, 479.4161376953125], [0.19145965576171875, 303.95465087890625, 74.38341522216797, 472.03399658203125], [76.32237243652344, 359.26446533203125, 205.94822692871094, 434.69122314453125], [43.468238830566406, 358.97186279296875, 84.9859390258789, 419.93695068359375]], 'scores': [0.9531962275505066, 0.5808154940605164, 0.45540082454681396, 0.34901925921440125], 'class_indices': [0.0, 0.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:38] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:38]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=414396;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=231991;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 1 laptop, 208.3ms\n",
      "Speed: 2.5ms preprocess, 208.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[49.63169860839844, 147.25372314453125, 520.95751953125, 479.44732666015625], [0.15735626220703125, 304.19732666015625, 72.88591766357422, 457.77569580078125], [84.35452270507812, 359.2449035644531, 205.27908325195312, 424.4493103027344], [44.112754821777344, 358.53009033203125, 86.70230865478516, 436.38214111328125], [0.36534881591796875, 446.9861755371094, 67.4873046875, 479.7394104003906]], 'scores': [0.9510982036590576, 0.8084085583686829, 0.5404577255249023, 0.5132743120193481, 0.2509581446647644], 'class_indices': [0.0, 0.0, 63.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=961825;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=553175;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tv, 1 laptop, 211.5ms\n",
      "Speed: 2.7ms preprocess, 211.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[49.47576904296875, 147.3929443359375, 520.128662109375, 479.427978515625], [0.16014862060546875, 303.97589111328125, 73.06053924560547, 478.0584716796875], [43.110862731933594, 358.565673828125, 86.11872100830078, 422.61376953125], [82.803955078125, 359.4273681640625, 205.51092529296875, 424.7696533203125], [84.39884185791016, 358.7523498535156, 205.35574340820312, 423.5533142089844]], 'scores': [0.9572848677635193, 0.7780379056930542, 0.3798390328884125, 0.3675885796546936, 0.2627668082714081], 'class_indices': [0.0, 0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=715642;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=687096;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 laptop, 213.4ms\n",
      "Speed: 2.3ms preprocess, 213.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[49.56402587890625, 147.34791564941406, 520.0103759765625, 479.405029296875], [0.196807861328125, 303.7576904296875, 71.97532653808594, 469.6641845703125], [43.535884857177734, 358.3039245605469, 85.11572265625, 433.2284240722656], [81.88186645507812, 358.9718322753906, 205.88900756835938, 426.2223815917969]], 'scores': [0.956142246723175, 0.6549915075302124, 0.3189023733139038, 0.29589807987213135], 'class_indices': [0.0, 0.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=29529;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=251361;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 213.6ms\n",
      "Speed: 2.2ms preprocess, 213.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[48.89971923828125, 147.5872802734375, 519.8154296875, 479.39666748046875], [0.1773834228515625, 303.78076171875, 72.82502746582031, 478.70635986328125], [193.92437744140625, 289.92919921875, 240.91668701171875, 375.9776611328125], [44.19157409667969, 358.4217529296875, 83.08503723144531, 417.6973876953125]], 'scores': [0.956436812877655, 0.8726910352706909, 0.7736654281616211, 0.2571357488632202], 'class_indices': [0.0, 0.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:39] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:39]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=770554;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=252970;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 211.6ms\n",
      "Speed: 2.3ms preprocess, 211.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[49.04798889160156, 147.93792724609375, 518.0455322265625, 479.3804931640625], [0.1980438232421875, 303.91143798828125, 73.21358489990234, 478.89044189453125], [185.53265380859375, 289.8398132324219, 241.22161865234375, 375.9141540527344], [44.1370735168457, 358.1075439453125, 86.30345153808594, 435.286376953125]], 'scores': [0.9552550911903381, 0.8714596629142761, 0.6928813457489014, 0.2586458921432495], 'class_indices': [0.0, 0.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=702327;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=643728;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 laptop, 210.0ms\n",
      "Speed: 2.7ms preprocess, 210.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[49.56024169921875, 148.463134765625, 518.161865234375, 479.42578125], [0.14300537109375, 304.10418701171875, 73.05574798583984, 457.33599853515625], [181.46835327148438, 293.52911376953125, 241.15475463867188, 376.49420166015625], [74.08001708984375, 359.8705749511719, 191.07144165039062, 434.5494689941406], [187.75759887695312, 293.80078125, 238.64309692382812, 356.91046142578125]], 'scores': [0.9550588130950928, 0.805482804775238, 0.7440947890281677, 0.4002417325973511, 0.2690051794052124], 'class_indices': [0.0, 0.0, 0.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=653770;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=38657;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 laptop, 212.8ms\n",
      "Speed: 2.6ms preprocess, 212.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[50.19404602050781, 150.60919189453125, 516.2653198242188, 479.2711181640625], [176.2713165283203, 295.89593505859375, 241.85231018066406, 377.33837890625], [0.18924713134765625, 303.78399658203125, 72.60124206542969, 476.133544921875], [81.77550506591797, 359.724853515625, 185.92193603515625, 428.2138671875], [45.66233825683594, 358.8316650390625, 86.25735473632812, 435.45257568359375]], 'scores': [0.9496731162071228, 0.7595410943031311, 0.6822770833969116, 0.49177029728889465, 0.45004817843437195], 'class_indices': [0.0, 0.0, 0.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=1671;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=515302;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 213.9ms\n",
      "Speed: 2.7ms preprocess, 213.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[50.001953125, 150.8312225341797, 514.3567504882812, 479.42291259765625], [0.24149322509765625, 304.271728515625, 73.2834243774414, 477.57745361328125], [173.19479370117188, 296.6991271972656, 241.50775146484375, 377.6163024902344], [45.167236328125, 358.607421875, 83.00167846679688, 422.8494873046875]], 'scores': [0.949836790561676, 0.8568872809410095, 0.8519342541694641, 0.2765088677406311], 'class_indices': [0.0, 0.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=951814;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=570631;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 208.1ms\n",
      "Speed: 2.7ms preprocess, 208.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[50.19891357421875, 151.60906982421875, 514.7417602539062, 479.4527587890625], [171.8109130859375, 298.45416259765625, 241.2525634765625, 377.56195068359375], [0.1375732421875, 304.1290283203125, 74.4949951171875, 456.66766357421875]], 'scores': [0.9462863206863403, 0.7948278188705444, 0.7124201655387878], 'class_indices': [0.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:40] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=564787;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=26129;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 laptop, 207.1ms\n",
      "Speed: 2.5ms preprocess, 207.1ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[49.69682312011719, 152.24803161621094, 513.1692504882812, 479.513671875], [170.1669158935547, 298.41485595703125, 240.4835662841797, 377.35546875], [0.129974365234375, 303.4339904785156, 73.5821533203125, 456.2030944824219], [44.32048034667969, 358.664306640625, 89.11299133300781, 437.48333740234375], [84.33096313476562, 359.96209716796875, 181.10763549804688, 420.47613525390625]], 'scores': [0.9521161317825317, 0.8356617093086243, 0.555476725101471, 0.27828294038772583, 0.27060216665267944], 'class_indices': [0.0, 0.0, 0.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=686931;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=471006;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 211.3ms\n",
      "Speed: 2.3ms preprocess, 211.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[49.93719482421875, 151.82386779785156, 513.6026000976562, 479.414794921875], [0.1965789794921875, 304.17425537109375, 73.27705383300781, 478.0419921875], [169.66656494140625, 298.65838623046875, 240.4630126953125, 377.59649658203125], [45.11908721923828, 358.6292724609375, 85.25580596923828, 424.2550048828125]], 'scores': [0.9506884813308716, 0.8719602227210999, 0.8200404644012451, 0.3708093464374542], 'class_indices': [0.0, 0.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=586367;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=247189;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tv, 210.3ms\n",
      "Speed: 2.1ms preprocess, 210.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[50.97265625, 152.19810485839844, 512.8363647460938, 479.4305419921875], [169.43992614746094, 298.5324401855469, 241.0485076904297, 378.5334167480469], [0.20459747314453125, 304.19012451171875, 72.90172576904297, 476.77825927734375], [84.76988983154297, 359.82794189453125, 181.0013427734375, 421.23541259765625]], 'scores': [0.9455675482749939, 0.8187549114227295, 0.7609323263168335, 0.2960016131401062], 'class_indices': [0.0, 0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=137381;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=349246;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 1 laptop, 210.3ms\n",
      "Speed: 2.1ms preprocess, 210.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.04423522949219, 154.96051025390625, 510.99652099609375, 479.0936279296875], [170.43946838378906, 298.01934814453125, 241.44972229003906, 380.66412353515625], [0.13409423828125, 303.880859375, 73.33934783935547, 454.3582763671875], [0.35155487060546875, 447.76092529296875, 73.73614501953125, 479.7386474609375], [75.55802917480469, 359.666015625, 184.7291717529297, 442.235107421875]], 'scores': [0.9502413272857666, 0.8859913945198059, 0.8672076463699341, 0.40515220165252686, 0.29269203543663025], 'class_indices': [0.0, 0.0, 0.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:41] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:41]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=962367;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=869488;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 5 persons, 1 hot dog, 2 chairs, 1 laptop, 209.6ms\n",
      "Speed: 3.2ms preprocess, 209.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.09611511230469, 157.0909423828125, 506.42535400390625, 479.51318359375], [0.1537017822265625, 303.5794982910156, 73.30473327636719, 454.4670104980469], [173.88067626953125, 297.6226806640625, 241.4642333984375, 382.12713623046875], [0.4571533203125, 447.8640441894531, 76.80416870117188, 479.7582702636719], [75.10816955566406, 359.63916015625, 187.8694305419922, 444.44146728515625], [473.491455078125, 413.444580078125, 518.7098388671875, 465.3243408203125], [274.2954406738281, 324.5482482910156, 348.7349548339844, 367.9012145996094], [45.41002655029297, 359.13250732421875, 83.6714859008789, 417.06976318359375], [45.27999496459961, 359.15887451171875, 86.20266723632812, 440.82720947265625]], 'scores': [0.9569349884986877, 0.8908693194389343, 0.8741429448127747, 0.5021378993988037, 0.4941104054450989, 0.47366172075271606, 0.30584651231765747, 0.2958761155605316, 0.26255321502685547], 'class_indices': [0.0, 0.0, 0.0, 56.0, 63.0, 56.0, 52.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=797231;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=896354;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 2 chairs, 1 laptop, 205.9ms\n",
      "Speed: 2.4ms preprocess, 205.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.3831787109375, 159.20542907714844, 504.90863037109375, 479.5350341796875], [0.1481475830078125, 304.02874755859375, 72.44284057617188, 455.75274658203125], [179.9085235595703, 301.65020751953125, 240.66404724121094, 383.1448974609375], [472.5633544921875, 413.14666748046875, 518.6956787109375, 464.64813232421875], [75.88079833984375, 359.7799072265625, 193.178955078125, 445.3016357421875], [0.3824615478515625, 447.55657958984375, 78.59424591064453, 479.73907470703125], [43.288818359375, 358.56732177734375, 87.78451538085938, 442.15728759765625]], 'scores': [0.9578477144241333, 0.8581547141075134, 0.7716797590255737, 0.5497416853904724, 0.3228246569633484, 0.3219178020954132, 0.2932516932487488], 'class_indices': [0.0, 0.0, 0.0, 56.0, 63.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=665648;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=49707;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 2 chairs, 2 tvs, 214.2ms\n",
      "Speed: 2.3ms preprocess, 214.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[59.23223876953125, 162.27703857421875, 502.14105224609375, 479.47802734375], [0.0962677001953125, 306.363037109375, 75.55420684814453, 454.0169677734375], [471.62652587890625, 413.36627197265625, 518.8660278320312, 464.31512451171875], [0.3264617919921875, 447.4857482910156, 79.26657104492188, 479.7362976074219], [405.99493408203125, 372.754638671875, 451.43084716796875, 404.489013671875], [85.39724731445312, 359.62384033203125, 197.1546630859375, 417.50579833984375], [43.453086853027344, 358.99969482421875, 83.29532623291016, 416.32391357421875]], 'scores': [0.946810245513916, 0.8944462537765503, 0.6485396027565002, 0.42012789845466614, 0.4033263027667999, 0.37327370047569275, 0.2886067032814026], 'class_indices': [0.0, 0.0, 56.0, 56.0, 62.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=509813;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=178526;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 3 chairs, 1 tv, 1 laptop, 213.6ms\n",
      "Speed: 3.7ms preprocess, 213.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[61.02496337890625, 161.427490234375, 490.73468017578125, 479.46551513671875], [0.1077423095703125, 309.833740234375, 71.39471435546875, 454.9622802734375], [471.14129638671875, 413.52618408203125, 518.8101196289062, 464.38958740234375], [399.6600646972656, 373.0506286621094, 451.5483093261719, 405.9200744628906], [85.56236267089844, 359.5126953125, 200.3627166748047, 417.083251953125], [0.360321044921875, 447.7864685058594, 78.6744155883789, 479.7446594238281], [483.78619384765625, 462.7153015136719, 542.2335815429688, 479.7625427246094]], 'scores': [0.9484391808509827, 0.8902976512908936, 0.7074553966522217, 0.6493070125579834, 0.4598241150379181, 0.3586091995239258, 0.2979826033115387], 'class_indices': [0.0, 0.0, 56.0, 63.0, 62.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=973441;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=303874;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 tie, 2 chairs, 2 laptops, 206.6ms\n",
      "Speed: 2.2ms preprocess, 206.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[58.04833984375, 158.79266357421875, 490.73760986328125, 479.56585693359375], [0.10955810546875, 309.1671142578125, 74.58966064453125, 454.31097412109375], [472.13427734375, 413.41229248046875, 518.9176025390625, 464.69549560546875], [394.6947021484375, 372.779296875, 451.51220703125, 405.1151123046875], [76.6705322265625, 359.31219482421875, 201.72354125976562, 449.60040283203125], [0.5471649169921875, 448.31787109375, 78.63320922851562, 479.73779296875], [220.6512451171875, 435.4908447265625, 291.32635498046875, 479.7156982421875]], 'scores': [0.9502455592155457, 0.8991474509239197, 0.5738610029220581, 0.5551359057426453, 0.4902547001838684, 0.42581507563591003, 0.2895427346229553], 'class_indices': [0.0, 0.0, 56.0, 63.0, 63.0, 56.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:42] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:42]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=960705;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=726373;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 1 tv, 2 laptops, 206.8ms\n",
      "Speed: 2.3ms preprocess, 206.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[57.13916015625, 157.12237548828125, 494.13134765625, 479.53509521484375], [0.1606903076171875, 311.3922119140625, 72.89283752441406, 455.464599609375], [399.598388671875, 372.5775146484375, 451.3082275390625, 401.892333984375], [472.87652587890625, 413.3065490722656, 518.8233032226562, 464.6265563964844], [84.49876403808594, 359.1387939453125, 202.0196075439453, 417.0018310546875], [75.46885681152344, 359.7020263671875, 202.16148376464844, 450.0352783203125], [46.287200927734375, 359.92877197265625, 82.55474853515625, 416.36474609375]], 'scores': [0.9422127604484558, 0.8092411160469055, 0.5801861882209778, 0.475435733795166, 0.3102691173553467, 0.299147367477417, 0.2668571174144745], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=477268;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=883530;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 2 laptops, 209.9ms\n",
      "Speed: 2.2ms preprocess, 209.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[59.4215087890625, 158.4049072265625, 495.27056884765625, 479.500244140625], [0.19719696044921875, 311.78411865234375, 72.4772720336914, 476.47735595703125], [404.0000915527344, 372.56402587890625, 451.5290222167969, 401.76361083984375], [223.78314208984375, 436.97113037109375, 295.613037109375, 479.74383544921875], [472.70111083984375, 413.2442321777344, 519.36767578125, 465.0142517089844], [84.12882995605469, 358.77581787109375, 203.6335906982422, 418.49554443359375], [74.40963745117188, 359.53985595703125, 203.8463134765625, 449.92669677734375], [45.306846618652344, 359.1431579589844, 84.54215240478516, 414.8308410644531], [403.83544921875, 372.71649169921875, 451.3631591796875, 401.74664306640625]], 'scores': [0.9450491666793823, 0.7332833409309387, 0.539386510848999, 0.5392249226570129, 0.5057754516601562, 0.33841076493263245, 0.310822069644928, 0.3015744984149933, 0.2541559040546417], 'class_indices': [0.0, 0.0, 63.0, 27.0, 56.0, 62.0, 63.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=350701;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=656876;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 2 laptops, 209.9ms\n",
      "Speed: 2.2ms preprocess, 209.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[58.49012756347656, 159.938720703125, 494.4093017578125, 479.53118896484375], [0.1315765380859375, 311.29766845703125, 72.09534454345703, 454.36077880859375], [396.9449462890625, 372.5557861328125, 451.5882568359375, 402.29534912109375], [472.3499755859375, 413.15509033203125, 518.82177734375, 464.86614990234375], [75.43183135986328, 359.2716369628906, 205.47412109375, 450.0381774902344], [223.14324951171875, 435.9750671386719, 293.80206298828125, 479.7381896972656], [0.31003570556640625, 447.5917663574219, 79.9194107055664, 479.7445983886719], [45.44184112548828, 359.24676513671875, 82.57166290283203, 416.47540283203125]], 'scores': [0.9455333352088928, 0.8878495097160339, 0.599853515625, 0.5981171131134033, 0.5785577297210693, 0.4198142886161804, 0.4181726574897766, 0.2836218774318695], 'class_indices': [0.0, 0.0, 63.0, 56.0, 63.0, 27.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=800093;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=475739;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 2 chairs, 2 tvs, 1 laptop, 209.2ms\n",
      "Speed: 2.9ms preprocess, 209.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[58.4564208984375, 159.96156311035156, 494.020751953125, 479.49267578125], [0.1472015380859375, 310.47869873046875, 72.24198913574219, 454.494384765625], [392.38623046875, 372.6763610839844, 451.318603515625, 402.8600769042969], [84.8664779663086, 359.1837158203125, 205.30441284179688, 416.98779296875], [472.50439453125, 413.4942932128906, 519.211181640625, 464.8031921386719], [392.6563720703125, 372.935791015625, 451.1951904296875, 402.91485595703125], [0.38262176513671875, 447.4708251953125, 80.72671508789062, 479.735595703125]], 'scores': [0.9464545845985413, 0.8719227910041809, 0.5420293807983398, 0.4184595048427582, 0.3875015377998352, 0.32604002952575684, 0.3227662742137909], 'class_indices': [0.0, 0.0, 63.0, 62.0, 56.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:43] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:43]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=960946;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=805372;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 2 tvs, 1 laptop, 209.4ms\n",
      "Speed: 2.2ms preprocess, 209.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.9122314453125, 159.48406982421875, 490.78094482421875, 479.50201416015625], [0.15478515625, 309.85906982421875, 73.8712158203125, 454.85614013671875], [472.184814453125, 413.390869140625, 519.0770263671875, 464.6005859375], [387.43914794921875, 372.95501708984375, 451.68157958984375, 404.80059814453125], [219.81674194335938, 436.205078125, 290.7724914550781, 479.7958984375], [85.5626220703125, 359.1708984375, 204.83804321289062, 417.03753662109375], [388.90203857421875, 373.27166748046875, 451.50433349609375, 404.61663818359375], [0.4296875, 448.1269226074219, 78.50303649902344, 479.7522888183594], [46.545475006103516, 359.46978759765625, 82.17849731445312, 414.80328369140625]], 'scores': [0.9455321431159973, 0.8270049691200256, 0.5918978452682495, 0.47954654693603516, 0.4689673185348511, 0.4122987985610962, 0.39139866828918457, 0.31423792243003845, 0.3094788193702698], 'class_indices': [0.0, 0.0, 56.0, 63.0, 27.0, 62.0, 62.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=891221;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=744943;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 tie, 2 chairs, 1 tv, 2 laptops, 208.1ms\n",
      "Speed: 2.1ms preprocess, 208.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.70794677734375, 159.7496795654297, 488.9794921875, 479.26605224609375], [0.1575927734375, 309.94561767578125, 74.03617858886719, 454.8533935546875], [383.0621032714844, 372.76312255859375, 451.3356018066406, 406.07904052734375], [471.28009033203125, 413.1502685546875, 519.10986328125, 464.53875732421875], [217.36239624023438, 436.0320129394531, 286.8176574707031, 479.7576599121094], [0.32564544677734375, 447.28326416015625, 79.40731811523438, 479.73297119140625], [74.84113311767578, 359.5306396484375, 204.6929931640625, 448.64117431640625], [84.17459106445312, 358.77569580078125, 204.78921508789062, 418.94683837890625]], 'scores': [0.9401884078979492, 0.854080319404602, 0.7016208171844482, 0.6205583214759827, 0.501319169998169, 0.44690343737602234, 0.38019874691963196, 0.2524065375328064], 'class_indices': [0.0, 0.0, 63.0, 56.0, 27.0, 56.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=464591;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=98001;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 2 chairs, 1 tv, 1 laptop, 207.3ms\n",
      "Speed: 2.0ms preprocess, 207.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.523956298828125, 159.77996826171875, 487.0077209472656, 479.4306640625], [0.1542510986328125, 310.3218078613281, 74.01417541503906, 453.9455871582031], [379.89434814453125, 372.66253662109375, 451.34832763671875, 407.1766357421875], [470.7984619140625, 413.4635925292969, 519.241943359375, 464.4331970214844], [85.34193420410156, 358.66937255859375, 205.1917266845703, 415.62005615234375], [45.00604248046875, 359.2645263671875, 82.75569915771484, 415.0352783203125], [0.30290985107421875, 447.5752868652344, 78.40145874023438, 479.7344055175781]], 'scores': [0.9464399814605713, 0.8785303831100464, 0.5721724033355713, 0.5565029382705688, 0.49410462379455566, 0.4843990206718445, 0.4243423342704773], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=424148;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=835976;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 2 chairs, 2 tvs, 210.5ms\n",
      "Speed: 2.0ms preprocess, 210.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.029052734375, 159.77536010742188, 484.83392333984375, 479.5610656738281], [0.14447021484375, 310.23101806640625, 72.85658264160156, 454.77105712890625], [211.4156494140625, 435.25067138671875, 281.8018493652344, 479.72515869140625], [469.62457275390625, 413.48583984375, 519.2169189453125, 464.3597412109375], [378.0654296875, 373.0776672363281, 451.5859375, 409.9150695800781], [45.014888763427734, 359.9862060546875, 84.17660522460938, 416.466552734375], [0.2970123291015625, 447.07855224609375, 78.37112426757812, 479.7403564453125], [45.152099609375, 359.49322509765625, 85.92413330078125, 437.86077880859375], [83.102294921875, 358.8177490234375, 205.13348388671875, 425.19482421875]], 'scores': [0.9476039409637451, 0.8362139463424683, 0.5558812022209167, 0.5272181034088135, 0.45984140038490295, 0.3197033405303955, 0.2879701554775238, 0.278372198343277, 0.267374187707901], 'class_indices': [0.0, 0.0, 27.0, 56.0, 62.0, 0.0, 56.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=548808;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=785011;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 2 chairs, 1 tv, 2 laptops, 210.5ms\n",
      "Speed: 2.9ms preprocess, 210.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.2684326171875, 159.99127197265625, 484.71710205078125, 479.51116943359375], [0.13458251953125, 310.57269287109375, 71.52887725830078, 454.89190673828125], [210.78790283203125, 435.4757080078125, 280.1678466796875, 479.71435546875], [376.8676452636719, 372.289794921875, 451.1798400878906, 411.939208984375], [45.06305694580078, 360.33782958984375, 82.10879516601562, 415.51385498046875], [469.69036865234375, 413.12176513671875, 518.8294677734375, 464.19732666015625], [84.87384033203125, 358.95001220703125, 204.43115234375, 415.38214111328125], [0.5887069702148438, 447.82598876953125, 77.68026733398438, 479.74066162109375], [73.36166381835938, 359.7337646484375, 204.58685302734375, 445.02850341796875], [44.58320617675781, 359.64178466796875, 82.98944091796875, 436.732421875]], 'scores': [0.9484356641769409, 0.8868619203567505, 0.6613912582397461, 0.5660614371299744, 0.441650927066803, 0.4177926480770111, 0.40846681594848633, 0.3698783814907074, 0.2625514566898346, 0.25036191940307617], 'class_indices': [0.0, 0.0, 27.0, 63.0, 0.0, 56.0, 62.0, 56.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:44] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:44]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=68812;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=596134;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 3 chairs, 1 tv, 2 laptops, 207.1ms\n",
      "Speed: 2.1ms preprocess, 207.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.42071533203125, 159.92385864257812, 484.17449951171875, 479.5599060058594], [0.14276885986328125, 310.4935607910156, 73.41609954833984, 453.6602478027344], [377.0628662109375, 372.2894592285156, 451.36962890625, 410.9283142089844], [78.8437728881836, 359.3870849609375, 204.99429321289062, 443.24151611328125], [469.2509765625, 413.44622802734375, 518.745361328125, 464.42291259765625], [376.87152099609375, 372.455810546875, 451.50994873046875, 410.5552978515625], [0.6158981323242188, 448.0272216796875, 76.81470489501953, 479.737548828125], [44.17719650268555, 360.2266845703125, 83.85456848144531, 417.04150390625], [43.97701644897461, 359.9342041015625, 86.43841552734375, 438.0230712890625], [469.0052490234375, 413.3026428222656, 542.1288452148438, 479.7361755371094], [211.0445556640625, 436.5067443847656, 280.6282958984375, 479.7768249511719]], 'scores': [0.9492087960243225, 0.8797276020050049, 0.5101569294929504, 0.5071842074394226, 0.5006487369537354, 0.38651803135871887, 0.36565861105918884, 0.3380492627620697, 0.2553122937679291, 0.2511870563030243, 0.2504747807979584], 'class_indices': [0.0, 0.0, 63.0, 63.0, 56.0, 62.0, 56.0, 0.0, 0.0, 56.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=465853;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=491217;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 2 tvs, 1 laptop, 210.7ms\n",
      "Speed: 2.0ms preprocess, 210.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.86927795410156, 160.35401916503906, 484.4132080078125, 479.6153564453125], [0.1054229736328125, 310.18231201171875, 70.26764678955078, 453.69158935546875], [469.720947265625, 413.4839172363281, 519.0433959960938, 464.3545837402344], [0.3733978271484375, 447.715087890625, 78.22607421875, 479.7462158203125], [377.2858581542969, 372.78466796875, 451.2273254394531, 411.5069580078125], [44.371849060058594, 359.0494384765625, 82.19498443603516, 415.433349609375], [85.02000427246094, 358.53564453125, 204.94529724121094, 419.543212890625], [377.0565185546875, 372.9508056640625, 451.30322265625, 410.95654296875], [211.28427124023438, 434.9588623046875, 280.4693908691406, 479.7236328125]], 'scores': [0.9496991634368896, 0.9014309048652649, 0.5020043849945068, 0.45708203315734863, 0.4225683808326721, 0.38236719369888306, 0.37480679154396057, 0.32378315925598145, 0.27937957644462585], 'class_indices': [0.0, 0.0, 56.0, 56.0, 63.0, 0.0, 62.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=774442;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=306988;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 2 laptops, 209.0ms\n",
      "Speed: 2.8ms preprocess, 209.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.21095275878906, 160.7755126953125, 483.9434814453125, 479.60205078125], [211.24209594726562, 436.08929443359375, 281.367431640625, 479.76922607421875], [469.04205322265625, 413.25921630859375, 518.6073608398438, 464.18011474609375], [0.20230865478515625, 309.88372802734375, 74.21916198730469, 470.67730712890625], [378.42169189453125, 372.7945556640625, 451.58758544921875, 411.75439453125], [44.21269226074219, 359.51495361328125, 83.52861785888672, 416.67877197265625], [84.66661071777344, 358.4778137207031, 204.79579162597656, 415.7791442871094], [378.3401794433594, 372.987548828125, 451.6499328613281, 411.2105712890625], [73.93718719482422, 359.5022277832031, 204.78863525390625, 445.4673767089844]], 'scores': [0.950638473033905, 0.6501703858375549, 0.5604178309440613, 0.5420476198196411, 0.4764838218688965, 0.41642484068870544, 0.3658566176891327, 0.31640949845314026, 0.26000285148620605], 'class_indices': [0.0, 27.0, 56.0, 0.0, 63.0, 0.0, 62.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=508553;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=852126;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 3 chairs, 1 tv, 1 laptop, 211.6ms\n",
      "Speed: 2.6ms preprocess, 211.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.13580322265625, 160.45274353027344, 483.4365234375, 479.535888671875], [0.11098480224609375, 309.86962890625, 70.79463195800781, 454.0853271484375], [468.8671875, 413.3094177246094, 518.801025390625, 464.4861145019531], [211.28704833984375, 435.75665283203125, 280.22991943359375, 479.72503662109375], [380.8307800292969, 372.58721923828125, 451.4089050292969, 411.83526611328125], [380.458251953125, 372.51531982421875, 451.6162109375, 411.28839111328125], [0.4027099609375, 447.4511413574219, 77.40620422363281, 479.7606506347656], [44.83331298828125, 359.62701416015625, 82.16375732421875, 415.11199951171875], [44.81850814819336, 359.24835205078125, 83.87722778320312, 438.03778076171875], [475.72119140625, 462.2980651855469, 540.0379028320312, 479.7363586425781]], 'scores': [0.9506456255912781, 0.8883539438247681, 0.614356517791748, 0.4651086628437042, 0.3980000913143158, 0.3628765940666199, 0.35192593932151794, 0.3482590913772583, 0.28717240691185, 0.2576690912246704], 'class_indices': [0.0, 0.0, 56.0, 27.0, 63.0, 62.0, 56.0, 0.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:45] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:45]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=263580;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=720407;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 2 laptops, 219.4ms\n",
      "Speed: 2.2ms preprocess, 219.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.73176574707031, 159.58164978027344, 483.59552001953125, 479.5128173828125], [211.4150390625, 435.7802734375, 281.094970703125, 479.7564697265625], [0.1473541259765625, 310.55999755859375, 72.72406005859375, 455.9844970703125], [383.166015625, 372.70318603515625, 451.3653564453125, 411.34161376953125], [45.1378059387207, 359.22320556640625, 84.06523132324219, 416.35235595703125], [74.92118072509766, 359.7336730957031, 204.97250366210938, 444.9469909667969], [469.14642333984375, 413.295166015625, 519.0985717773438, 464.0421142578125], [0.301300048828125, 447.645263671875, 77.96649169921875, 479.75146484375]], 'scores': [0.943856954574585, 0.7355021238327026, 0.6847098469734192, 0.6464900374412537, 0.4148898422718048, 0.32150575518608093, 0.3095460534095764, 0.3016711473464966], 'class_indices': [0.0, 27.0, 0.0, 63.0, 0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=355534;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=862295;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 1 tv, 2 laptops, 212.4ms\n",
      "Speed: 2.9ms preprocess, 212.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.05218505859375, 159.156005859375, 484.35791015625, 479.56195068359375], [0.13289642333984375, 310.3098449707031, 72.01029968261719, 456.3288879394531], [383.9358825683594, 372.8978271484375, 451.3975524902344, 411.69976806640625], [211.2158203125, 436.644287109375, 280.340576171875, 479.72412109375], [468.841796875, 413.4373779296875, 519.1958618164062, 464.4342041015625], [44.18001937866211, 359.29449462890625, 84.38543701171875, 422.75079345703125], [74.6886215209961, 359.6971435546875, 204.96231079101562, 444.8724365234375], [384.5346374511719, 372.7424011230469, 451.3648376464844, 411.7001037597656], [0.4513702392578125, 447.8742370605469, 76.40634155273438, 479.7481994628906]], 'scores': [0.9533589482307434, 0.8237578868865967, 0.5926534533500671, 0.580369234085083, 0.4695482850074768, 0.4311862587928772, 0.4018794298171997, 0.31377488374710083, 0.29808753728866577], 'class_indices': [0.0, 0.0, 63.0, 27.0, 56.0, 0.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=77339;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=688703;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 2 laptops, 207.8ms\n",
      "Speed: 2.2ms preprocess, 207.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.81340026855469, 158.71878051757812, 483.80194091796875, 479.6269836425781], [0.1467132568359375, 310.23040771484375, 72.70327758789062, 455.56829833984375], [210.10699462890625, 436.21124267578125, 279.3798828125, 479.75592041015625], [384.1718444824219, 372.4931640625, 451.1555480957031, 412.2081298828125], [468.4632568359375, 413.21435546875, 519.025634765625, 464.07568359375], [44.630462646484375, 359.41650390625, 84.30917358398438, 415.7572021484375], [74.28244018554688, 359.87640380859375, 205.20437622070312, 443.17041015625], [0.37290191650390625, 448.075927734375, 75.88020324707031, 479.74072265625]], 'scores': [0.9565404057502747, 0.851658046245575, 0.6037598252296448, 0.5121217370033264, 0.5046080350875854, 0.4972975552082062, 0.3761526048183441, 0.3510544002056122], 'class_indices': [0.0, 0.0, 27.0, 63.0, 56.0, 0.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=951508;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=875810;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 2 tvs, 1 laptop, 210.9ms\n",
      "Speed: 1.9ms preprocess, 210.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.69618225097656, 158.4248046875, 483.287353515625, 479.5963134765625], [0.1502685546875, 310.44256591796875, 72.94869232177734, 456.5858154296875], [468.33673095703125, 413.4489440917969, 518.8230590820312, 464.3233337402344], [208.85751342773438, 435.86065673828125, 278.2652282714844, 479.89886474609375], [383.54998779296875, 372.700439453125, 451.207763671875, 412.83837890625], [44.375, 359.3492431640625, 84.64198303222656, 417.57763671875], [0.31459808349609375, 447.85284423828125, 77.96027374267578, 479.74444580078125], [383.716796875, 372.832763671875, 451.4107666015625, 412.3592529296875], [84.05026245117188, 358.9670104980469, 204.67779541015625, 421.6053161621094]], 'scores': [0.9561892151832581, 0.6973487138748169, 0.6043499112129211, 0.4902963638305664, 0.4881363809108734, 0.3974543511867523, 0.38428014516830444, 0.3260519802570343, 0.2862943410873413], 'class_indices': [0.0, 0.0, 56.0, 27.0, 63.0, 0.0, 56.0, 62.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=145409;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=709127;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 1 tv, 2 laptops, 215.7ms\n",
      "Speed: 2.2ms preprocess, 215.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.43940734863281, 158.45301818847656, 482.85101318359375, 479.525390625], [208.42108154296875, 435.9937744140625, 277.6827392578125, 479.7681884765625], [0.15374755859375, 309.91021728515625, 74.42098999023438, 455.02679443359375], [468.2301025390625, 413.28302001953125, 518.741455078125, 464.07623291015625], [383.5987548828125, 372.44891357421875, 451.6368408203125, 412.21478271484375], [44.10017013549805, 359.3992919921875, 84.81874084472656, 415.71826171875], [383.2223815917969, 372.3109130859375, 451.4438171386719, 412.72503662109375], [75.32928466796875, 359.71820068359375, 204.86965942382812, 441.97332763671875], [0.275787353515625, 447.5096130371094, 75.48452758789062, 479.7375793457031]], 'scores': [0.954147458076477, 0.7423166632652283, 0.5916002988815308, 0.5555746555328369, 0.37854814529418945, 0.34765172004699707, 0.3403160274028778, 0.33915209770202637, 0.29081305861473083], 'class_indices': [0.0, 27.0, 0.0, 56.0, 62.0, 0.0, 63.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:46] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:46]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=463686;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=390723;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 tie, 2 chairs, 2 laptops, 207.2ms\n",
      "Speed: 2.7ms preprocess, 207.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.90284729003906, 158.59429931640625, 483.0379638671875, 479.60040283203125], [0.12339019775390625, 310.54547119140625, 72.07435607910156, 454.18963623046875], [383.4891357421875, 372.54901123046875, 451.4283447265625, 413.46856689453125], [468.4849853515625, 413.4761962890625, 518.8974609375, 464.30615234375], [0.30709075927734375, 447.74566650390625, 76.28981018066406, 479.73333740234375], [207.85894775390625, 435.9884033203125, 277.67449951171875, 479.8323974609375], [74.0260009765625, 359.7142333984375, 204.97677612304688, 441.759765625]], 'scores': [0.9569603204727173, 0.8846913576126099, 0.645224928855896, 0.5635639429092407, 0.42794644832611084, 0.36076104640960693, 0.26346689462661743], 'class_indices': [0.0, 0.0, 63.0, 56.0, 56.0, 27.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=272357;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=198602;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 2 tvs, 2 laptops, 208.3ms\n",
      "Speed: 2.0ms preprocess, 208.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.01115417480469, 158.06849670410156, 483.40020751953125, 479.560302734375], [0.159149169921875, 310.09954833984375, 74.35334777832031, 453.85760498046875], [467.78387451171875, 413.15093994140625, 518.7549438476562, 464.2154541015625], [208.90744018554688, 435.8713073730469, 278.1283874511719, 479.7506408691406], [382.7804260253906, 372.4585266113281, 451.4616394042969, 413.1331481933594], [0.3782501220703125, 447.6720886230469, 75.04546356201172, 479.7342834472656], [84.78611755371094, 359.50567626953125, 205.1859893798828, 416.57830810546875], [85.1585693359375, 358.79986572265625, 204.91793823242188, 415.93170166015625], [383.483642578125, 372.53167724609375, 451.5736083984375, 412.79388427734375], [44.710304260253906, 358.7294921875, 82.29702758789062, 416.0311279296875]], 'scores': [0.9560409188270569, 0.8583707809448242, 0.5488071441650391, 0.49159103631973267, 0.47359251976013184, 0.41024765372276306, 0.32039856910705566, 0.2994562089443207, 0.28950682282447815, 0.2596776485443115], 'class_indices': [0.0, 0.0, 56.0, 27.0, 63.0, 56.0, 63.0, 62.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=55148;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=174681;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 2 tvs, 2 laptops, 210.5ms\n",
      "Speed: 2.0ms preprocess, 210.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.64656066894531, 158.34690856933594, 482.74542236328125, 479.574462890625], [468.3115234375, 413.4375, 518.4833984375, 464.263671875], [0.168060302734375, 310.58624267578125, 72.06365966796875, 458.24884033203125], [383.65252685546875, 372.81573486328125, 451.40093994140625, 412.97808837890625], [208.66778564453125, 436.2771911621094, 278.18267822265625, 479.7053527832031], [383.667236328125, 372.97027587890625, 451.6256103515625, 412.39556884765625], [73.31387329101562, 359.67034912109375, 205.098388671875, 442.20098876953125], [44.31425476074219, 359.78045654296875, 84.47967529296875, 415.99688720703125], [84.48540496826172, 358.85723876953125, 204.92559814453125, 417.79229736328125], [0.36898040771484375, 447.6912841796875, 75.46111297607422, 479.7464599609375]], 'scores': [0.9595730900764465, 0.6068404316902161, 0.5468453168869019, 0.478637158870697, 0.4689406752586365, 0.40693631768226624, 0.33877789974212646, 0.32044246792793274, 0.3022780418395996, 0.28167954087257385], 'class_indices': [0.0, 56.0, 0.0, 63.0, 27.0, 62.0, 63.0, 0.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=264703;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=541109;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 laptops, 213.6ms\n",
      "Speed: 2.3ms preprocess, 213.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.8682861328125, 158.65232849121094, 482.80078125, 479.602783203125], [0.1419219970703125, 310.0675048828125, 74.3382568359375, 454.800537109375], [75.17445373535156, 359.5710754394531, 205.3090057373047, 442.3293762207031], [383.13983154296875, 372.1354675292969, 451.23956298828125, 413.3166809082031], [43.241790771484375, 359.0678405761719, 83.6253433227539, 420.0738830566406], [467.55157470703125, 413.0384521484375, 518.8284301757812, 464.3729248046875], [209.0428466796875, 436.1722412109375, 277.36956787109375, 479.701171875]], 'scores': [0.956760585308075, 0.8438426852226257, 0.673502504825592, 0.533245325088501, 0.41776084899902344, 0.38255518674850464, 0.3384212255477905], 'class_indices': [0.0, 0.0, 63.0, 63.0, 0.0, 56.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:47] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:47]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=896800;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=522860;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 1 tv, 3 laptops, 209.7ms\n",
      "Speed: 2.8ms preprocess, 209.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.08271789550781, 158.95587158203125, 482.53717041015625, 479.5740966796875], [0.14119720458984375, 308.201904296875, 72.90827178955078, 454.610107421875], [383.0765075683594, 372.75018310546875, 451.7414855957031, 413.42364501953125], [208.46612548828125, 436.54840087890625, 278.0694885253906, 479.79150390625], [468.07025146484375, 413.4104919433594, 518.8947143554688, 464.4366760253906], [38.21833801269531, 358.6575927734375, 82.8851089477539, 415.088134765625], [83.86308288574219, 359.437744140625, 204.4329376220703, 416.3216552734375], [0.36753082275390625, 447.44482421875, 75.52549743652344, 479.740478515625], [73.92642211914062, 359.58978271484375, 205.59414672851562, 442.01434326171875], [84.87944030761719, 358.8836669921875, 204.77943420410156, 415.6456298828125]], 'scores': [0.9570397138595581, 0.8475025296211243, 0.605103075504303, 0.5963727831840515, 0.45464545488357544, 0.4208453595638275, 0.3251257836818695, 0.3136003911495209, 0.279793918132782, 0.2644594609737396], 'class_indices': [0.0, 0.0, 63.0, 27.0, 56.0, 0.0, 63.0, 56.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=502255;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=70815;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 2 chairs, 2 tvs, 2 laptops, 210.8ms\n",
      "Speed: 2.5ms preprocess, 210.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.200927734375, 158.72012329101562, 483.35943603515625, 479.5819396972656], [0.1585235595703125, 303.4229736328125, 72.02784729003906, 453.4512939453125], [383.4876708984375, 372.93310546875, 451.15484619140625, 413.2081298828125], [33.381561279296875, 358.7156982421875, 84.45492553710938, 418.29052734375], [468.36224365234375, 413.4527587890625, 518.9168701171875, 464.296142578125], [0.412353515625, 448.4827880859375, 74.87600708007812, 479.729248046875], [84.08226013183594, 358.8869323730469, 205.14366149902344, 417.0636291503906], [73.97586059570312, 359.7295837402344, 205.28106689453125, 442.5215148925781], [383.59600830078125, 372.6728515625, 451.33050537109375, 412.791259765625]], 'scores': [0.9564211368560791, 0.8640323877334595, 0.4956165850162506, 0.40964025259017944, 0.40747570991516113, 0.3473396897315979, 0.29677823185920715, 0.27832135558128357, 0.26222550868988037], 'class_indices': [0.0, 0.0, 63.0, 0.0, 56.0, 56.0, 62.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=926916;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=19317;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 3 chairs, 1 tv, 1 laptop, 208.6ms\n",
      "Speed: 2.8ms preprocess, 208.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.02726745605469, 158.78839111328125, 482.84307861328125, 479.58392333984375], [0.151031494140625, 301.19219970703125, 71.0313720703125, 454.87762451171875], [383.54302978515625, 372.7514343261719, 451.573486328125, 413.0496520996094], [468.38641357421875, 413.45965576171875, 518.735595703125, 464.24285888671875], [209.60369873046875, 436.9206237792969, 279.57537841796875, 479.7672424316406], [31.18547821044922, 359.09185791015625, 82.53221130371094, 414.53167724609375], [85.5533447265625, 359.26995849609375, 205.0032958984375, 419.32867431640625], [0.280975341796875, 448.15625, 74.79415130615234, 479.7763671875], [474.75579833984375, 462.4351806640625, 542.4946899414062, 479.75946044921875]], 'scores': [0.9564462900161743, 0.8079637885093689, 0.7409842014312744, 0.7008771896362305, 0.6141651272773743, 0.4372410774230957, 0.38247114419937134, 0.3545295298099518, 0.2546624541282654], 'class_indices': [0.0, 0.0, 63.0, 56.0, 27.0, 0.0, 62.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=344537;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=332366;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 1 tv, 2 laptops, 214.0ms\n",
      "Speed: 2.8ms preprocess, 214.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.34117126464844, 158.48731994628906, 482.7047119140625, 479.5870361328125], [0.15021514892578125, 362.90216064453125, 75.35169219970703, 455.06439208984375], [31.22682762145996, 358.7847900390625, 84.46952056884766, 414.760498046875], [383.7328796386719, 372.87640380859375, 451.5962829589844, 413.24737548828125], [210.2615966796875, 436.6048278808594, 279.6421203613281, 479.7216491699219], [85.08755493164062, 359.65106201171875, 204.6741943359375, 416.04046630859375], [468.85162353515625, 413.5054931640625, 518.7785034179688, 464.293212890625], [85.42620849609375, 359.1541748046875, 204.71820068359375, 416.5928955078125], [0.33699798583984375, 448.41436767578125, 75.25465393066406, 479.72186279296875]], 'scores': [0.954472541809082, 0.8185392618179321, 0.7365471720695496, 0.6188839077949524, 0.5702027678489685, 0.4495491087436676, 0.4151296615600586, 0.329984575510025, 0.29459118843078613], 'class_indices': [0.0, 0.0, 0.0, 63.0, 27.0, 63.0, 56.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=852299;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=527549;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 2 chairs, 1 tv, 3 laptops, 209.4ms\n",
      "Speed: 2.5ms preprocess, 209.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.22344970703125, 158.7205352783203, 482.888916015625, 479.57269287109375], [0.1881866455078125, 363.46368408203125, 80.62911224365234, 455.02789306640625], [28.692237854003906, 358.7696228027344, 83.71758270263672, 414.5243835449219], [0.12569808959960938, 408.340576171875, 27.686872482299805, 455.47686767578125], [384.4183349609375, 372.7831726074219, 451.5377197265625, 412.0223083496094], [85.43356323242188, 359.5455017089844, 204.39859008789062, 419.2976379394531], [468.98712158203125, 413.45184326171875, 518.8828735351562, 464.24517822265625], [85.09341430664062, 359.09088134765625, 204.57403564453125, 418.22515869140625], [0.30419158935546875, 448.44097900390625, 74.78569030761719, 479.73187255859375]], 'scores': [0.9511316418647766, 0.7770137786865234, 0.6355839967727661, 0.5952848792076111, 0.5368568897247314, 0.41196194291114807, 0.38519057631492615, 0.3021828830242157, 0.27861911058425903], 'class_indices': [0.0, 0.0, 0.0, 63.0, 63.0, 63.0, 56.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:48] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:48]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=47610;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=973391;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 3 laptops, 209.2ms\n",
      "Speed: 2.3ms preprocess, 209.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.98515319824219, 158.5260009765625, 483.64654541015625, 479.59381103515625], [26.039094924926758, 358.919189453125, 85.78237915039062, 437.979248046875], [0.162506103515625, 408.44439697265625, 28.220840454101562, 454.72576904296875], [468.77294921875, 413.4591064453125, 518.8246459960938, 464.43310546875], [385.49029541015625, 372.4217529296875, 451.40997314453125, 411.64892578125], [0.1520538330078125, 364.09515380859375, 79.09654235839844, 453.44964599609375], [210.90185546875, 436.9523010253906, 280.29681396484375, 479.7162170410156], [84.8924789428711, 359.25054931640625, 204.74496459960938, 419.2486572265625], [385.4453125, 372.5189208984375, 451.498291015625, 411.0982666015625], [0.12707138061523438, 363.99188232421875, 36.7368049621582, 411.19952392578125]], 'scores': [0.9522585868835449, 0.7693129181861877, 0.709080159664154, 0.5777754783630371, 0.526633083820343, 0.47013208270072937, 0.42508426308631897, 0.3663475215435028, 0.30595850944519043, 0.2658185064792633], 'class_indices': [0.0, 0.0, 63.0, 56.0, 63.0, 0.0, 27.0, 63.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=531971;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=168401;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 1 tv, 3 laptops, 207.3ms\n",
      "Speed: 5.1ms preprocess, 207.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.25946044921875, 158.6817626953125, 484.0126953125, 479.57061767578125], [0.15847015380859375, 367.1800537109375, 78.66156005859375, 455.8214111328125], [26.265457153320312, 358.28839111328125, 85.71343231201172, 436.51153564453125], [82.02330017089844, 359.3455810546875, 205.1885223388672, 434.55767822265625], [386.04803466796875, 372.6263427734375, 451.22625732421875, 411.5631103515625], [469.46728515625, 413.359619140625, 518.7027587890625, 464.43505859375], [0.1854991912841797, 408.6261901855469, 28.472537994384766, 454.6894226074219], [210.8251953125, 436.11669921875, 280.43316650390625, 479.7833251953125], [386.0326843261719, 372.68682861328125, 451.3074645996094, 410.91632080078125], [0.2942657470703125, 448.52655029296875, 71.22607421875, 479.74090576171875]], 'scores': [0.9536768198013306, 0.7688886523246765, 0.7436555624008179, 0.5480256080627441, 0.5462698936462402, 0.428646981716156, 0.41178008913993835, 0.3385562002658844, 0.28849685192108154, 0.2656567692756653], 'class_indices': [0.0, 0.0, 0.0, 63.0, 63.0, 56.0, 63.0, 27.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=678742;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=403476;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 laptops, 208.4ms\n",
      "Speed: 2.4ms preprocess, 208.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.85774230957031, 158.87350463867188, 483.98968505859375, 479.5509338378906], [29.498382568359375, 358.3429260253906, 72.32026672363281, 392.7241516113281], [79.19821166992188, 359.40838623046875, 205.23056030273438, 442.07806396484375], [385.85748291015625, 372.489990234375, 451.30474853515625, 410.6512451171875], [469.57598876953125, 413.2235107421875, 518.8228149414062, 463.97857666015625], [0.25311279296875, 365.20709228515625, 86.00370788574219, 478.86895751953125]], 'scores': [0.9519291520118713, 0.7933799624443054, 0.5782702565193176, 0.49428629875183105, 0.3715395927429199, 0.3394356667995453], 'class_indices': [0.0, 0.0, 63.0, 63.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=593855;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=454925;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 1 tv, 2 laptops, 216.1ms\n",
      "Speed: 1.9ms preprocess, 216.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.96656799316406, 159.18553161621094, 483.8040771484375, 479.603759765625], [385.64520263671875, 372.5762939453125, 451.15692138671875, 411.5999755859375], [469.24371337890625, 413.38812255859375, 518.829345703125, 464.64569091796875], [77.70388793945312, 359.3898010253906, 205.12631225585938, 444.8446960449219], [0.26922607421875, 366.0707092285156, 87.23199462890625, 453.7068176269531], [30.71619415283203, 358.99139404296875, 71.3173599243164, 387.60821533203125], [385.5459899902344, 372.66357421875, 451.2358703613281, 411.07147216796875]], 'scores': [0.9493665099143982, 0.583173930644989, 0.5543856620788574, 0.5418930053710938, 0.46869784593582153, 0.4008117616176605, 0.2880616784095764], 'class_indices': [0.0, 63.0, 56.0, 63.0, 0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:49] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:49]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=225427;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=348336;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 1 tv, 2 laptops, 214.0ms\n",
      "Speed: 2.1ms preprocess, 214.0ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.76121520996094, 158.9758758544922, 483.681884765625, 479.57574462890625], [0.2480316162109375, 367.291748046875, 83.41436767578125, 451.9501953125], [29.377302169799805, 359.075927734375, 71.52305603027344, 388.93511962890625], [385.57293701171875, 372.50250244140625, 451.42889404296875, 412.33551025390625], [468.8736572265625, 413.3465576171875, 518.4617309570312, 464.420166015625], [77.68712615966797, 359.3199768066406, 205.36016845703125, 444.7700500488281], [79.98715209960938, 358.5603942871094, 205.0277099609375, 442.6034851074219]], 'scores': [0.9503100514411926, 0.6533529758453369, 0.6099293231964111, 0.5796616077423096, 0.5637263059616089, 0.3820632994174957, 0.2550047039985657], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=971874;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=24192;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 1 laptop, 215.4ms\n",
      "Speed: 1.9ms preprocess, 215.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.15629577636719, 159.16421508789062, 483.38726806640625, 479.5710144042969], [29.413606643676758, 359.1351318359375, 71.99862670898438, 390.95513916015625], [0.2027740478515625, 368.0637512207031, 78.48019409179688, 453.8352355957031], [385.3255920410156, 372.3233337402344, 451.2447204589844, 411.9813537597656], [469.1439208984375, 413.31549072265625, 518.629638671875, 464.13433837890625], [210.97042846679688, 437.03326416015625, 278.5798034667969, 479.70855712890625]], 'scores': [0.9531528949737549, 0.682427167892456, 0.6817556619644165, 0.6019951105117798, 0.41196122765541077, 0.26142996549606323], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=523346;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=168687;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 1 tv, 2 laptops, 211.0ms\n",
      "Speed: 2.5ms preprocess, 211.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.24320983886719, 159.08322143554688, 483.49700927734375, 479.5879211425781], [0.13866424560546875, 359.938232421875, 82.65502166748047, 451.015869140625], [468.75408935546875, 413.2568359375, 519.3641967773438, 464.2574462890625], [384.9813232421875, 372.5321044921875, 451.27978515625, 411.9267578125], [29.737943649291992, 359.8045959472656, 70.8748779296875, 387.0994567871094], [210.70269775390625, 436.22576904296875, 279.2900695800781, 479.72235107421875], [83.08123779296875, 359.2751770019531, 204.59585571289062, 415.6772155761719], [384.9176940917969, 372.610595703125, 451.3818664550781, 411.7572021484375], [475.76947021484375, 462.5853271484375, 540.9398803710938, 479.762451171875]], 'scores': [0.9471902847290039, 0.7723391652107239, 0.614664614200592, 0.5513198971748352, 0.5254963636398315, 0.4005860686302185, 0.3091620206832886, 0.2764602303504944, 0.25244882702827454], 'class_indices': [0.0, 0.0, 56.0, 63.0, 0.0, 27.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=820836;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=570980;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 3 laptops, 216.7ms\n",
      "Speed: 2.4ms preprocess, 216.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.78057861328125, 159.08673095703125, 483.18804931640625, 479.594482421875], [28.708709716796875, 360.39251708984375, 69.43977355957031, 390.2459716796875], [468.63916015625, 413.2370300292969, 518.6978759765625, 464.4432678222656], [384.42266845703125, 372.44439697265625, 451.16424560546875, 412.61676025390625], [87.17890167236328, 359.39068603515625, 205.25457763671875, 419.8035888671875], [0.239471435546875, 359.1537780761719, 103.52777862548828, 454.3216247558594], [0.09657478332519531, 408.6726989746094, 28.0703182220459, 454.4751892089844], [211.36163330078125, 437.16595458984375, 279.8417663574219, 479.78643798828125]], 'scores': [0.9540373682975769, 0.7484638094902039, 0.6437730193138123, 0.6109346151351929, 0.5451878309249878, 0.47105225920677185, 0.3967759907245636, 0.3810650110244751], 'class_indices': [0.0, 0.0, 56.0, 63.0, 63.0, 0.0, 63.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=121392;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=569386;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 2 laptops, 209.5ms\n",
      "Speed: 3.1ms preprocess, 209.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.48785400390625, 158.97967529296875, 483.4573974609375, 479.5789794921875], [84.47976684570312, 359.3056335449219, 205.07244873046875, 426.5699768066406], [0.27555084228515625, 357.55169677734375, 94.79472351074219, 456.77459716796875], [468.57733154296875, 413.291015625, 518.7796020507812, 464.1468505859375], [384.6622314453125, 372.6282958984375, 451.2679443359375, 412.502197265625], [31.588836669921875, 358.9837341308594, 71.97003936767578, 393.5788879394531], [210.78952026367188, 437.06524658203125, 279.5279235839844, 479.80352783203125], [0.11786651611328125, 301.6677551269531, 93.5606918334961, 457.5702209472656], [384.9819641113281, 372.64056396484375, 451.4619445800781, 412.14129638671875]], 'scores': [0.9536198973655701, 0.6753530502319336, 0.5114641189575195, 0.5107038021087646, 0.49708202481269836, 0.43709561228752136, 0.4129081070423126, 0.26359331607818604, 0.2578553855419159], 'class_indices': [0.0, 63.0, 0.0, 56.0, 63.0, 0.0, 27.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:50] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=600326;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=754804;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 2 laptops, 212.0ms\n",
      "Speed: 2.8ms preprocess, 212.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.01988220214844, 159.1033935546875, 482.9525146484375, 479.55145263671875], [0.20075225830078125, 280.56170654296875, 86.03071594238281, 457.96087646484375], [384.3983154296875, 372.5307922363281, 451.2838134765625, 412.3616027832031], [468.99835205078125, 413.435546875, 518.88916015625, 464.573974609375], [210.58132934570312, 436.46136474609375, 279.9755859375, 479.78436279296875], [84.50045013427734, 359.33477783203125, 205.25131225585938, 432.51116943359375], [33.265296936035156, 358.8727111816406, 72.9689712524414, 387.8031921386719], [84.53495025634766, 358.91943359375, 205.20156860351562, 431.1585693359375], [33.988616943359375, 358.73468017578125, 83.08422088623047, 414.29779052734375]], 'scores': [0.9542892575263977, 0.7118774056434631, 0.5546068549156189, 0.5303763747215271, 0.5244163274765015, 0.49744299054145813, 0.3913806974887848, 0.3522716462612152, 0.2595142424106598], 'class_indices': [0.0, 0.0, 63.0, 56.0, 27.0, 63.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=650761;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=577230;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 2 chairs, 3 laptops, 208.8ms\n",
      "Speed: 3.3ms preprocess, 208.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.46281433105469, 159.2207794189453, 483.48138427734375, 479.59930419921875], [0.22025299072265625, 282.59124755859375, 88.91317749023438, 454.0318603515625], [385.37274169921875, 372.51300048828125, 451.14849853515625, 412.27911376953125], [469.23394775390625, 413.4200439453125, 518.95849609375, 464.06671142578125], [86.69270324707031, 359.27606201171875, 205.13938903808594, 429.40814208984375], [0.28157806396484375, 446.7207336425781, 77.59201049804688, 479.7364196777344], [32.217613220214844, 359.1122741699219, 72.23404693603516, 382.4455261230469], [32.637847900390625, 358.623046875, 86.70237731933594, 404.7919921875], [0.19184494018554688, 408.1767272949219, 28.119518280029297, 444.6199645996094], [211.11044311523438, 436.10003662109375, 279.0209655761719, 479.71319580078125]], 'scores': [0.9567188024520874, 0.8665531277656555, 0.6154104471206665, 0.5686553716659546, 0.5379998683929443, 0.34309321641921997, 0.31184685230255127, 0.2837147116661072, 0.27346524596214294, 0.2653196156024933], 'class_indices': [0.0, 0.0, 63.0, 56.0, 63.0, 56.0, 0.0, 0.0, 63.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=216761;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=816476;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 2 laptops, 212.5ms\n",
      "Speed: 2.0ms preprocess, 212.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.63018798828125, 159.226318359375, 483.31109619140625, 479.60980224609375], [0.0761871337890625, 285.32891845703125, 95.42829895019531, 477.833740234375], [385.0687561035156, 372.628662109375, 451.2688293457031, 412.7396240234375], [468.98046875, 413.337646484375, 518.76220703125, 464.1275634765625], [87.48098754882812, 359.4847412109375, 204.91751098632812, 424.65704345703125], [87.35891723632812, 359.24639892578125, 204.97213745117188, 424.2462158203125], [38.18543243408203, 359.4808349609375, 70.16535186767578, 382.6883544921875], [38.435546875, 359.1802978515625, 85.74568939208984, 400.3150634765625], [210.47564697265625, 437.515380859375, 279.92236328125, 479.7586669921875], [385.3074951171875, 372.6890869140625, 451.4405517578125, 412.148193359375]], 'scores': [0.9535878896713257, 0.790642499923706, 0.5843665599822998, 0.4703310430049896, 0.4126969873905182, 0.4053606688976288, 0.3563995063304901, 0.349629670381546, 0.3416981101036072, 0.33949586749076843], 'class_indices': [0.0, 0.0, 63.0, 56.0, 63.0, 62.0, 0.0, 0.0, 27.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=8689;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=385957;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 1 tv, 2 laptops, 211.6ms\n",
      "Speed: 2.1ms preprocess, 211.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.90605163574219, 159.2913360595703, 483.34735107421875, 479.57867431640625], [0.2506866455078125, 289.398681640625, 96.9393310546875, 478.58380126953125], [385.23651123046875, 372.82537841796875, 451.48553466796875, 412.23809814453125], [40.408050537109375, 360.44525146484375, 85.87495422363281, 401.84356689453125], [469.1395263671875, 413.5108642578125, 518.9725341796875, 464.2913818359375], [86.93234252929688, 359.29644775390625, 205.36727905273438, 422.89874267578125], [210.8736572265625, 437.302978515625, 280.08966064453125, 479.7288818359375], [86.94189453125, 358.8773193359375, 205.16342163085938, 422.2939453125]], 'scores': [0.9577427506446838, 0.904141902923584, 0.6548822522163391, 0.6351392269134521, 0.6091602444648743, 0.4620022475719452, 0.3617110252380371, 0.34218889474868774], 'class_indices': [0.0, 0.0, 63.0, 0.0, 56.0, 63.0, 27.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:51] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:51]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=564247;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=450399;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 1 tv, 2 laptops, 216.4ms\n",
      "Speed: 2.2ms preprocess, 216.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.457794189453125, 159.1239013671875, 483.3878479003906, 479.603271484375], [0.18874359130859375, 288.8353271484375, 93.00125122070312, 478.0120849609375], [468.86053466796875, 413.396240234375, 519.1798706054688, 464.113037109375], [385.5992431640625, 372.446533203125, 451.2850341796875, 412.4132080078125], [37.065757751464844, 360.66119384765625, 87.01167297363281, 406.69122314453125], [210.5128173828125, 437.47332763671875, 279.9582824707031, 479.7283935546875], [86.46928405761719, 359.54156494140625, 205.2060089111328, 425.93438720703125], [86.55879211425781, 359.4126892089844, 205.0822296142578, 425.5433044433594]], 'scores': [0.9587046504020691, 0.794683575630188, 0.614555299282074, 0.5816590785980225, 0.5577220320701599, 0.5135915279388428, 0.4385160803794861, 0.38493767380714417], 'class_indices': [0.0, 0.0, 56.0, 63.0, 0.0, 27.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=588114;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=650955;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 212.8ms\n",
      "Speed: 1.6ms preprocess, 212.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.594818115234375, 159.193359375, 483.7044372558594, 479.59539794921875], [0.30718231201171875, 290.718505859375, 89.02969360351562, 479.49774169921875], [86.03321075439453, 359.58428955078125, 205.37982177734375, 428.49169921875], [385.8105163574219, 372.37762451171875, 451.1171569824219, 412.47357177734375], [210.92202758789062, 437.34405517578125, 279.73193359375, 479.71734619140625], [469.51043701171875, 413.4827880859375, 519.0034790039062, 464.3121337890625], [43.31468200683594, 360.53857421875, 86.92494201660156, 413.4398193359375], [385.31304931640625, 372.6084899902344, 451.21795654296875, 411.9087829589844], [43.57438659667969, 360.853271484375, 69.33253479003906, 387.3765869140625]], 'scores': [0.9569289684295654, 0.9292283654212952, 0.5746510028839111, 0.533750593662262, 0.5011696815490723, 0.48652806878089905, 0.479408860206604, 0.36646702885627747, 0.30866584181785583], 'class_indices': [0.0, 0.0, 62.0, 63.0, 27.0, 56.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=932980;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=430549;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 213.8ms\n",
      "Speed: 2.6ms preprocess, 213.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.83665466308594, 159.17529296875, 483.0487060546875, 479.60791015625], [0.23081207275390625, 293.4039306640625, 93.74297332763672, 479.4676513671875], [385.5618896484375, 372.42706298828125, 451.3472900390625, 412.65594482421875], [211.42300415039062, 435.9006652832031, 280.1036071777344, 479.7840270996094], [468.5592041015625, 413.36810302734375, 519.2261962890625, 464.52618408203125], [87.13058471679688, 359.55859375, 205.3668212890625, 426.92034912109375], [48.1253662109375, 362.74017333984375, 86.56838989257812, 401.69073486328125]], 'scores': [0.9580274224281311, 0.9392563104629517, 0.667778730392456, 0.5900425910949707, 0.5451889634132385, 0.5270773768424988, 0.30367735028266907], 'class_indices': [0.0, 0.0, 63.0, 27.0, 56.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=995793;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=477908;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 2 laptops, 217.4ms\n",
      "Speed: 2.7ms preprocess, 217.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.89366149902344, 159.1964569091797, 483.3458251953125, 479.54290771484375], [0.2291107177734375, 295.23974609375, 95.00048828125, 479.4744873046875], [468.86602783203125, 413.6878967285156, 518.7281494140625, 464.2472229003906], [385.59259033203125, 372.53399658203125, 451.07647705078125, 412.18060302734375], [87.28443145751953, 359.25274658203125, 205.72564697265625, 426.10980224609375], [87.43309020996094, 359.0458984375, 205.56663513183594, 426.0382080078125], [211.05325317382812, 437.7138366699219, 279.0279235839844, 479.6643981933594], [385.4659118652344, 372.6309814453125, 451.1763000488281, 412.01416015625], [51.653350830078125, 363.90374755859375, 85.05943298339844, 399.70489501953125]], 'scores': [0.9573670625686646, 0.937859058380127, 0.624793529510498, 0.5409757494926453, 0.4589688181877136, 0.4424000680446625, 0.37072962522506714, 0.3284926116466522, 0.259662002325058], 'class_indices': [0.0, 0.0, 56.0, 63.0, 62.0, 63.0, 27.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=670535;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=47029;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 tie, 1 chair, 1 tv, 2 laptops, 211.2ms\n",
      "Speed: 2.3ms preprocess, 211.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.222412109375, 159.09823608398438, 483.3125, 479.5573425292969], [0.21441650390625, 295.26763916015625, 96.08992767333984, 479.48187255859375], [385.63702392578125, 372.4350280761719, 451.12518310546875, 412.5419616699219], [468.8299560546875, 413.26690673828125, 518.98193359375, 464.27423095703125], [87.24090576171875, 359.57684326171875, 205.15090942382812, 425.23492431640625], [210.95684814453125, 437.2913818359375, 280.66162109375, 479.8076171875], [87.30123138427734, 359.447021484375, 204.91342163085938, 425.189208984375]], 'scores': [0.9581225514411926, 0.9276516437530518, 0.6073350310325623, 0.5670960545539856, 0.5033840537071228, 0.39812761545181274, 0.36943328380584717], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 27.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:52] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:52]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=202320;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=380509;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 tie, 1 chair, 1 tv, 2 laptops, 209.9ms\n",
      "Speed: 2.1ms preprocess, 209.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.30413818359375, 159.25462341308594, 483.3834228515625, 479.587158203125], [0.184478759765625, 295.59478759765625, 95.37088012695312, 479.51263427734375], [469.094970703125, 413.45648193359375, 518.566650390625, 464.05621337890625], [386.2422180175781, 372.7945556640625, 451.2015075683594, 411.9146728515625], [86.97797393798828, 359.1953125, 205.321044921875, 425.7135009765625], [87.03031921386719, 359.39605712890625, 205.6388397216797, 425.790283203125], [211.76620483398438, 437.2023010253906, 280.3808898925781, 479.7091979980469]], 'scores': [0.9572933912277222, 0.9223606586456299, 0.6216180920600891, 0.5733070373535156, 0.5042473077774048, 0.42379361391067505, 0.288659930229187], 'class_indices': [0.0, 0.0, 56.0, 63.0, 63.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=904504;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=187352;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 tie, 2 chairs, 1 tv, 1 laptop, 213.2ms\n",
      "Speed: 2.1ms preprocess, 213.2ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.730499267578125, 159.19223022460938, 484.1540222167969, 479.5768127441406], [0.22228240966796875, 296.05242919921875, 94.71031188964844, 479.51141357421875], [386.3848876953125, 372.4328308105469, 451.2900390625, 411.3841247558594], [469.4581298828125, 413.29779052734375, 519.0511474609375, 464.16217041015625], [87.29119873046875, 358.8862609863281, 205.32626342773438, 426.0953674316406], [211.41183471679688, 436.18121337890625, 280.1434631347656, 479.69696044921875], [475.66583251953125, 462.1219482421875, 541.0708618164062, 479.766845703125]], 'scores': [0.9575247168540955, 0.9349019527435303, 0.7284120321273804, 0.6626044511795044, 0.5831096768379211, 0.2840988039970398, 0.26378771662712097], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 27.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=581220;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=72223;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 1 tv, 1 laptop, 208.9ms\n",
      "Speed: 2.2ms preprocess, 208.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.468505859375, 159.39419555664062, 484.39886474609375, 479.6111145019531], [0.21202850341796875, 296.04620361328125, 95.08585357666016, 479.54742431640625], [87.26927185058594, 358.9603271484375, 204.91041564941406, 424.74609375], [469.95758056640625, 413.3669738769531, 518.8956909179688, 464.3760681152344], [386.7181701660156, 372.61993408203125, 451.3750305175781, 410.76708984375], [51.728492736816406, 364.108642578125, 70.83554077148438, 388.41375732421875], [51.40663146972656, 364.09197998046875, 82.05735778808594, 399.58306884765625]], 'scores': [0.9588909149169922, 0.9353875517845154, 0.7294158935546875, 0.5285571813583374, 0.5224232077598572, 0.26394638419151306, 0.26035410165786743], 'class_indices': [0.0, 0.0, 62.0, 56.0, 63.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=263599;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=93734;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 2 laptops, 212.9ms\n",
      "Speed: 2.5ms preprocess, 212.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.70423889160156, 159.35104370117188, 484.9786376953125, 479.5552673339844], [0.1926727294921875, 295.91741943359375, 94.03363800048828, 479.30902099609375], [386.92523193359375, 372.5712890625, 451.16571044921875, 410.366943359375], [87.21947479248047, 359.33343505859375, 205.41070556640625, 426.51629638671875], [470.28619384765625, 413.53961181640625, 518.7781372070312, 464.46612548828125], [211.75, 436.18621826171875, 281.856201171875, 479.70306396484375], [87.34895324707031, 359.17828369140625, 205.2270965576172, 426.50714111328125], [50.882747650146484, 362.7716064453125, 83.305908203125, 401.219482421875], [51.06709289550781, 362.78887939453125, 71.81465911865234, 388.26470947265625]], 'scores': [0.9587514400482178, 0.919579029083252, 0.5105279684066772, 0.4845618009567261, 0.4761466979980469, 0.40069580078125, 0.40016502141952515, 0.31511878967285156, 0.2500866651535034], 'class_indices': [0.0, 0.0, 63.0, 62.0, 56.0, 27.0, 63.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:53] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=253962;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=507359;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 210.6ms\n",
      "Speed: 2.7ms preprocess, 210.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.680877685546875, 159.2482147216797, 484.1726379394531, 479.57464599609375], [0.2075653076171875, 295.8713073730469, 94.64218139648438, 479.3816833496094], [87.34183502197266, 359.28387451171875, 205.34732055664062, 425.45233154296875], [469.87921142578125, 413.456298828125, 518.8765258789062, 464.5882568359375], [386.7125244140625, 372.671630859375, 451.5511474609375, 411.1494140625], [211.95321655273438, 438.0478515625, 281.0548400878906, 479.724365234375], [386.81500244140625, 372.76422119140625, 451.63677978515625, 410.64959716796875]], 'scores': [0.9567693471908569, 0.9222745299339294, 0.5748295187950134, 0.5376819968223572, 0.5225198864936829, 0.39594700932502747, 0.3011450171470642], 'class_indices': [0.0, 0.0, 62.0, 56.0, 63.0, 27.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=319151;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=535935;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 1 tv, 1 laptop, 212.6ms\n",
      "Speed: 2.1ms preprocess, 212.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.53898620605469, 159.1366424560547, 483.48626708984375, 479.58905029296875], [0.215667724609375, 295.5696716308594, 94.97531127929688, 479.5274963378906], [386.59954833984375, 372.5028991699219, 451.10791015625, 411.5101013183594], [469.50360107421875, 413.3929443359375, 518.7575073242188, 463.8685302734375], [87.3187255859375, 359.2786560058594, 205.07235717773438, 426.0761413574219], [50.629539489746094, 363.74237060546875, 70.58927917480469, 387.32672119140625]], 'scores': [0.9556566476821899, 0.9366810321807861, 0.6476535201072693, 0.571938693523407, 0.5596244931221008, 0.4345332086086273], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=372507;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=620672;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 215.9ms\n",
      "Speed: 1.8ms preprocess, 215.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.95439147949219, 159.45285034179688, 483.43682861328125, 479.5780334472656], [0.24578857421875, 295.9593811035156, 94.67507934570312, 479.4730529785156], [469.53271484375, 413.4822998046875, 518.8259887695312, 464.2557373046875], [87.31332397460938, 359.0440979003906, 204.66693115234375, 426.3985900878906], [212.14633178710938, 437.53106689453125, 281.52447509765625, 479.72686767578125], [386.47119140625, 372.7476806640625, 451.3291015625, 412.248291015625], [386.0787353515625, 372.93878173828125, 451.4827880859375, 411.590576171875]], 'scores': [0.9607582688331604, 0.8972837328910828, 0.5990527272224426, 0.5953726172447205, 0.5823240280151367, 0.5236951112747192, 0.3847827911376953], 'class_indices': [0.0, 0.0, 56.0, 62.0, 27.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=314216;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=884649;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 214.0ms\n",
      "Speed: 3.0ms preprocess, 214.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.63877868652344, 159.34429931640625, 483.7501220703125, 479.59857177734375], [0.1964263916015625, 295.2281188964844, 94.89682006835938, 479.3530578613281], [386.6878967285156, 372.4233703613281, 451.1819152832031, 411.9200744628906], [87.50297546386719, 359.53936767578125, 204.89817810058594, 425.71697998046875], [469.1351318359375, 413.352783203125, 518.65625, 464.1204833984375], [211.3460693359375, 437.6190185546875, 280.71087646484375, 479.7103271484375], [386.7121276855469, 372.4651184082031, 451.2686462402344, 411.6731262207031], [50.924217224121094, 364.0665283203125, 86.52193450927734, 401.319580078125]], 'scores': [0.9583446383476257, 0.9233965277671814, 0.5729470252990723, 0.5673198699951172, 0.4742662012577057, 0.3302144408226013, 0.2992251515388489, 0.2918502986431122], 'class_indices': [0.0, 0.0, 63.0, 62.0, 56.0, 27.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:54] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:54]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=436527;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=958455;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 209.1ms\n",
      "Speed: 2.4ms preprocess, 209.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.01045227050781, 159.31881713867188, 483.41534423828125, 479.5702209472656], [0.20319366455078125, 295.59869384765625, 94.62284851074219, 479.34649658203125], [87.6102066040039, 359.453857421875, 204.91561889648438, 426.93414306640625], [469.5960693359375, 413.28961181640625, 518.6063232421875, 464.5452880859375], [212.04034423828125, 437.2974853515625, 281.1433410644531, 479.70672607421875], [386.59796142578125, 372.7427978515625, 451.08612060546875, 411.41741943359375], [386.26812744140625, 372.89080810546875, 451.20672607421875, 410.95452880859375], [50.87394714355469, 363.42791748046875, 70.32292938232422, 386.98883056640625]], 'scores': [0.9557556509971619, 0.9272252917289734, 0.5148648023605347, 0.5136803984642029, 0.4932922422885895, 0.44421935081481934, 0.33554643392562866, 0.29586121439933777], 'class_indices': [0.0, 0.0, 62.0, 56.0, 27.0, 63.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=294499;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=836782;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 tie, 1 chair, 2 tvs, 2 laptops, 1 toothbrush, 213.3ms\n",
      "Speed: 2.1ms preprocess, 213.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.42295837402344, 159.48699951171875, 484.1173095703125, 479.58978271484375], [0.22248077392578125, 294.94573974609375, 95.33941650390625, 479.48138427734375], [468.97564697265625, 413.21124267578125, 518.9310913085938, 463.91619873046875], [386.501708984375, 372.6637878417969, 451.208251953125, 412.1088562011719], [87.38836669921875, 359.2611083984375, 204.94247436523438, 425.5533447265625], [87.462646484375, 359.06256103515625, 204.71847534179688, 425.66748046875], [386.501708984375, 372.6516418457031, 451.2322998046875, 411.7187805175781], [211.645751953125, 437.3004455566406, 280.6362609863281, 479.7641296386719], [212.75161743164062, 321.461181640625, 274.1760559082031, 346.310302734375]], 'scores': [0.9557130932807922, 0.9368805289268494, 0.6295217871665955, 0.5594435334205627, 0.49553120136260986, 0.4483458399772644, 0.35763019323349, 0.3017985224723816, 0.2707829177379608], 'class_indices': [0.0, 0.0, 56.0, 63.0, 62.0, 63.0, 62.0, 27.0, 79.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=820111;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=299664;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 2 chairs, 2 tvs, 1 laptop, 209.4ms\n",
      "Speed: 2.5ms preprocess, 209.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.5076904296875, 159.4246826171875, 483.220458984375, 479.60296630859375], [0.18431854248046875, 296.174072265625, 95.97522735595703, 479.4630126953125], [386.6144104003906, 372.6966552734375, 451.4617004394531, 412.093994140625], [87.52884674072266, 359.36639404296875, 204.88766479492188, 425.3504638671875], [469.1961669921875, 413.30987548828125, 518.8792724609375, 464.22808837890625], [52.274497985839844, 363.59228515625, 71.69073486328125, 387.9940185546875], [386.54266357421875, 372.7566833496094, 451.51470947265625, 411.8075866699219], [52.00634002685547, 363.17852783203125, 86.96642303466797, 401.24774169921875], [475.7462158203125, 462.38311767578125, 542.8277587890625, 479.76031494140625], [211.0328369140625, 436.9588317871094, 280.90875244140625, 479.8419494628906]], 'scores': [0.9594146013259888, 0.930913507938385, 0.5783672332763672, 0.5446188449859619, 0.5269297361373901, 0.3741316795349121, 0.36575329303741455, 0.298578143119812, 0.29061856865882874, 0.2574829161167145], 'class_indices': [0.0, 0.0, 63.0, 62.0, 56.0, 0.0, 62.0, 0.0, 56.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=291160;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=525607;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 212.0ms\n",
      "Speed: 2.5ms preprocess, 212.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.876312255859375, 159.82586669921875, 483.5273742675781, 479.59112548828125], [0.19245147705078125, 298.66864013671875, 94.895751953125, 479.51641845703125], [469.82171630859375, 413.312255859375, 518.7634887695312, 464.4075927734375], [212.34170532226562, 438.4502868652344, 281.74359130859375, 479.7027282714844], [87.04815673828125, 359.1844787597656, 205.18710327148438, 426.7864074707031], [386.882080078125, 372.61541748046875, 451.234375, 412.34796142578125], [386.606689453125, 372.848876953125, 451.3140869140625, 411.7054443359375], [52.589866638183594, 363.70989990234375, 71.51079559326172, 388.72735595703125]], 'scores': [0.9590445160865784, 0.932564914226532, 0.6305229663848877, 0.5987602472305298, 0.5370160341262817, 0.5284269452095032, 0.46898138523101807, 0.3614201247692108], 'class_indices': [0.0, 0.0, 56.0, 27.0, 62.0, 63.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=507841;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=306865;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 210.2ms\n",
      "Speed: 2.2ms preprocess, 210.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.75624084472656, 159.4066619873047, 483.23095703125, 479.59649658203125], [0.19235992431640625, 299.328125, 92.86639404296875, 479.30084228515625], [212.13482666015625, 436.3994140625, 281.64520263671875, 479.74798583984375], [87.20800018310547, 359.42156982421875, 205.3671875, 427.55853271484375], [469.17730712890625, 413.21337890625, 518.7491455078125, 464.2874755859375], [386.6260986328125, 372.8548583984375, 451.2449951171875, 412.6446533203125], [50.74382019042969, 363.1446533203125, 86.54496765136719, 401.77740478515625], [386.49798583984375, 372.88287353515625, 451.31317138671875, 412.00103759765625]], 'scores': [0.9588945508003235, 0.9160124063491821, 0.6290850639343262, 0.5659540891647339, 0.5284623503684998, 0.5225412845611572, 0.4708261489868164, 0.3566904067993164], 'class_indices': [0.0, 0.0, 27.0, 62.0, 56.0, 63.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:55] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:55]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=104615;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=382874;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 1 tv, 2 laptops, 211.8ms\n",
      "Speed: 2.3ms preprocess, 211.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.24517822265625, 159.62979125976562, 483.3372802734375, 479.5787658691406], [0.25891876220703125, 298.3919677734375, 94.72126007080078, 479.2994384765625], [386.5228271484375, 372.6873779296875, 451.189697265625, 412.4566650390625], [211.60806274414062, 436.675537109375, 281.2705078125, 479.7271728515625], [469.2952880859375, 413.3616943359375, 519.2269287109375, 464.28045654296875], [87.2232666015625, 358.86407470703125, 205.68399047851562, 425.29449462890625], [87.62860107421875, 359.221435546875, 205.85693359375, 425.298095703125], [50.112030029296875, 362.83526611328125, 84.8106689453125, 400.20367431640625]], 'scores': [0.9582248330116272, 0.918655514717102, 0.5911915898323059, 0.5834351778030396, 0.5383729934692383, 0.5101109147071838, 0.3786076009273529, 0.3765556216239929], 'class_indices': [0.0, 0.0, 63.0, 27.0, 56.0, 62.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=60765;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=609012;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 2 laptops, 208.8ms\n",
      "Speed: 1.8ms preprocess, 208.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.38214111328125, 159.4187469482422, 483.47698974609375, 479.55780029296875], [0.2116546630859375, 296.212646484375, 94.48363494873047, 479.45849609375], [469.496826171875, 413.302978515625, 518.268310546875, 464.115966796875], [87.4576644897461, 359.52752685546875, 206.13546752929688, 426.51080322265625], [386.49566650390625, 372.40264892578125, 451.08575439453125, 411.92242431640625], [386.209228515625, 372.6346435546875, 451.2034912109375, 411.343017578125], [87.67648315429688, 359.3660888671875, 206.14303588867188, 426.50933837890625], [51.94103240966797, 362.58685302734375, 71.32331085205078, 388.13751220703125], [211.7152099609375, 438.297607421875, 280.11883544921875, 479.725341796875], [51.52530288696289, 362.5092468261719, 86.31341552734375, 401.1882019042969]], 'scores': [0.9585013389587402, 0.9329625964164734, 0.5007717609405518, 0.4888736307621002, 0.4204441010951996, 0.4140913188457489, 0.396250456571579, 0.35461050271987915, 0.33416956663131714, 0.28712067008018494], 'class_indices': [0.0, 0.0, 56.0, 62.0, 63.0, 62.0, 63.0, 0.0, 27.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=427884;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=393107;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 1 tv, 2 laptops, 213.4ms\n",
      "Speed: 1.7ms preprocess, 213.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.51869201660156, 159.5849609375, 483.4923095703125, 479.61102294921875], [0.21917724609375, 295.2700500488281, 94.26670837402344, 479.3921203613281], [386.69775390625, 372.6015625, 451.24609375, 412.03363037109375], [87.369384765625, 359.232421875, 204.94317626953125, 426.372802734375], [469.34759521484375, 413.40191650390625, 518.4566040039062, 464.01092529296875], [87.66973876953125, 359.5605773925781, 205.10845947265625, 426.2785949707031], [51.66584777832031, 362.0811767578125, 86.20072937011719, 401.098388671875]], 'scores': [0.9577575922012329, 0.9253728985786438, 0.7012867331504822, 0.5148233771324158, 0.4889296591281891, 0.3956056535243988, 0.3026341497898102], 'class_indices': [0.0, 0.0, 63.0, 62.0, 56.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=786869;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=582048;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 chair, 1 tv, 2 laptops, 211.4ms\n",
      "Speed: 2.2ms preprocess, 211.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.709930419921875, 159.63323974609375, 483.6019592285156, 479.58258056640625], [0.24022674560546875, 294.16510009765625, 95.75482940673828, 479.376708984375], [87.12699127197266, 358.9637756347656, 205.23214721679688, 425.8894958496094], [386.85394287109375, 372.8841247558594, 451.197021484375, 412.0289611816406], [469.68109130859375, 413.46337890625, 518.6602172851562, 464.5203857421875], [386.61102294921875, 373.05926513671875, 451.29046630859375, 411.39239501953125]], 'scores': [0.959364116191864, 0.9274266958236694, 0.5686467289924622, 0.538855254650116, 0.4971781075000763, 0.4515172839164734], 'class_indices': [0.0, 0.0, 63.0, 63.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:56] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:56]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=340727;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=487738;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 2 laptops, 210.6ms\n",
      "Speed: 2.8ms preprocess, 210.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.55815124511719, 159.44351196289062, 484.19989013671875, 479.6000061035156], [0.21401214599609375, 294.20086669921875, 95.76040649414062, 479.4141845703125], [386.9365234375, 372.4194641113281, 451.3631591796875, 411.9684143066406], [469.99151611328125, 413.12274169921875, 519.1445922851562, 464.4202880859375], [87.2183837890625, 358.81939697265625, 205.57546997070312, 423.64105224609375], [87.41754150390625, 358.7745361328125, 205.35092163085938, 424.3297119140625], [51.24452209472656, 361.56378173828125, 73.42655944824219, 389.08929443359375], [387.03656005859375, 372.554443359375, 451.37298583984375, 411.654296875], [211.30325317382812, 436.89453125, 281.2856750488281, 479.7864990234375]], 'scores': [0.9589706063270569, 0.933514416217804, 0.5331850051879883, 0.5044492483139038, 0.46943214535713196, 0.4377685785293579, 0.4030420780181885, 0.31432363390922546, 0.3114679753780365], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 63.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=655156;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=267450;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 2 laptops, 213.0ms\n",
      "Speed: 4.4ms preprocess, 213.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.7803955078125, 159.543212890625, 484.18402099609375, 479.5672607421875], [0.2162933349609375, 294.2325439453125, 95.51811218261719, 479.3590087890625], [212.68331909179688, 437.2451171875, 282.7168273925781, 479.7508544921875], [51.208892822265625, 362.12554931640625, 86.86683654785156, 401.23162841796875], [469.966064453125, 413.41912841796875, 518.7916870117188, 464.33544921875], [87.29413604736328, 358.3804626464844, 205.4783935546875, 423.5111999511719], [87.58407592773438, 358.85540771484375, 205.33685302734375, 424.62884521484375], [387.1605224609375, 372.75732421875, 451.030517578125, 411.9197998046875], [386.962646484375, 372.9549560546875, 451.1436767578125, 411.4283447265625]], 'scores': [0.9589459300041199, 0.9253560304641724, 0.527809739112854, 0.509628415107727, 0.5088868737220764, 0.43907055258750916, 0.426637202501297, 0.4165562391281128, 0.39935609698295593], 'class_indices': [0.0, 0.0, 27.0, 0.0, 56.0, 62.0, 63.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=498305;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=860895;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 216.0ms\n",
      "Speed: 2.0ms preprocess, 216.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.16465759277344, 159.7319793701172, 485.2412109375, 479.55438232421875], [0.2307281494140625, 296.617919921875, 92.46389770507812, 479.3970947265625], [387.60870361328125, 372.433837890625, 451.46978759765625, 410.1649169921875], [87.27790069580078, 358.79278564453125, 205.451416015625, 427.83306884765625], [213.102783203125, 436.0231628417969, 282.18798828125, 479.7055969238281], [469.8648681640625, 413.31298828125, 518.8292236328125, 464.3695068359375], [50.974266052246094, 361.2120056152344, 74.56741333007812, 388.9596862792969], [50.639198303222656, 361.0172119140625, 87.71097564697266, 402.392333984375]], 'scores': [0.9585026502609253, 0.9289765357971191, 0.6351298093795776, 0.5690174102783203, 0.5033368468284607, 0.4545033872127533, 0.40982556343078613, 0.35803425312042236], 'class_indices': [0.0, 0.0, 63.0, 62.0, 27.0, 56.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=433976;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=548483;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 2 laptops, 208.0ms\n",
      "Speed: 3.1ms preprocess, 208.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.83148193359375, 159.65757751464844, 484.383544921875, 479.57470703125], [0.23201751708984375, 297.82012939453125, 95.2802963256836, 479.53411865234375], [387.3768310546875, 372.5015869140625, 451.081298828125, 411.112548828125], [87.43511199951172, 358.73529052734375, 205.3812255859375, 425.44781494140625], [469.9227294921875, 413.51171875, 518.7818603515625, 464.5562744140625], [49.81523132324219, 360.61602783203125, 87.5880126953125, 400.458740234375], [212.75164794921875, 436.59716796875, 282.51446533203125, 479.714111328125], [87.39421081542969, 358.4562072753906, 205.32923889160156, 425.7929382324219], [49.87437438964844, 360.63470458984375, 74.99234008789062, 389.15899658203125]], 'scores': [0.9593735933303833, 0.9374504089355469, 0.6576781868934631, 0.4907723665237427, 0.464747816324234, 0.4340440034866333, 0.42541661858558655, 0.4218667149543762, 0.29212069511413574], 'class_indices': [0.0, 0.0, 63.0, 63.0, 56.0, 0.0, 27.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=238591;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=269176;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 1 tv, 2 laptops, 207.4ms\n",
      "Speed: 2.2ms preprocess, 207.4ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.78411865234375, 159.702392578125, 483.82757568359375, 479.60736083984375], [0.25860595703125, 298.369140625, 92.69854736328125, 479.4541015625], [469.3369140625, 413.3248291015625, 518.896240234375, 464.5462646484375], [387.39190673828125, 372.5635986328125, 451.33551025390625, 411.95721435546875], [86.89910125732422, 358.69342041015625, 205.568115234375, 428.06304931640625], [86.89686584472656, 358.51226806640625, 205.5841827392578, 427.63714599609375], [49.3126335144043, 359.621826171875, 87.27449035644531, 402.5694580078125], [212.81430053710938, 437.1722717285156, 282.4585876464844, 479.7007141113281]], 'scores': [0.9599241018295288, 0.9370583295822144, 0.6300974488258362, 0.5791865587234497, 0.4633842408657074, 0.45866456627845764, 0.4234447479248047, 0.371210515499115], 'class_indices': [0.0, 0.0, 56.0, 63.0, 62.0, 63.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:57] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:57]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=741584;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=384705;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 2 tvs, 2 laptops, 215.2ms\n",
      "Speed: 2.2ms preprocess, 215.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.88194274902344, 159.99688720703125, 483.4281005859375, 479.61090087890625], [0.23627471923828125, 298.75323486328125, 92.38878631591797, 478.25067138671875], [44.50892639160156, 359.2265625, 86.78767395019531, 404.712646484375], [387.29364013671875, 372.47760009765625, 451.29376220703125, 412.00628662109375], [469.08428955078125, 413.3919677734375, 518.9195556640625, 464.7603759765625], [86.5820541381836, 358.78167724609375, 205.60720825195312, 427.51397705078125], [387.4544677734375, 372.537109375, 451.3206787109375, 411.682373046875], [0.2276782989501953, 408.3237609863281, 28.690120697021484, 431.8292541503906], [211.790283203125, 436.008056640625, 282.40106201171875, 479.720458984375], [476.14910888671875, 462.64886474609375, 542.4087524414062, 479.75872802734375]], 'scores': [0.958034873008728, 0.8926109671592712, 0.6099271774291992, 0.5840550065040588, 0.576052725315094, 0.5173580050468445, 0.3607247471809387, 0.3492448329925537, 0.28718820214271545, 0.2551509439945221], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0, 62.0, 62.0, 63.0, 27.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=429615;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=110351;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 211.6ms\n",
      "Speed: 2.2ms preprocess, 211.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.1064453125, 159.87753295898438, 484.087646484375, 479.5650329589844], [0.27719879150390625, 299.03619384765625, 92.41532897949219, 479.3924560546875], [387.3929443359375, 372.5287780761719, 451.3408203125, 412.1050720214844], [86.51708984375, 359.05810546875, 205.64157104492188, 426.06805419921875], [469.0711669921875, 413.154052734375, 518.65966796875, 464.6429443359375], [41.91518783569336, 358.8426208496094, 86.56329345703125, 404.8016052246094], [212.60678100585938, 437.6842041015625, 281.84295654296875, 479.755615234375], [42.986846923828125, 358.9530029296875, 74.63243865966797, 388.824951171875], [387.2976989746094, 372.72796630859375, 451.4929504394531, 411.3902587890625]], 'scores': [0.9588549733161926, 0.9298723340034485, 0.5173994302749634, 0.5118855237960815, 0.5104304552078247, 0.460498571395874, 0.40658169984817505, 0.38113218545913696, 0.3501170873641968], 'class_indices': [0.0, 0.0, 63.0, 62.0, 56.0, 0.0, 27.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=888161;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=255449;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 tvs, 2 laptops, 212.9ms\n",
      "Speed: 2.4ms preprocess, 212.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.406005859375, 159.66546630859375, 484.25537109375, 479.630859375], [0.1927032470703125, 297.31036376953125, 91.23878479003906, 478.71038818359375], [469.418701171875, 413.41558837890625, 518.9508056640625, 464.26409912109375], [387.2979431152344, 372.84320068359375, 451.4020080566406, 411.378173828125], [39.0897216796875, 358.74798583984375, 87.2381820678711, 407.28863525390625], [86.3154296875, 358.6661376953125, 205.60885620117188, 426.3878173828125], [86.24237060546875, 358.3196716308594, 205.5264892578125, 426.8835144042969], [387.518798828125, 372.83428955078125, 451.4267578125, 410.96685791015625]], 'scores': [0.9584115147590637, 0.8705016374588013, 0.5663790106773376, 0.5559433698654175, 0.5117912888526917, 0.4711870849132538, 0.35542261600494385, 0.32940900325775146], 'class_indices': [0.0, 0.0, 56.0, 63.0, 0.0, 63.0, 62.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=54503;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=415978;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 2 laptops, 208.9ms\n",
      "Speed: 2.5ms preprocess, 208.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.340789794921875, 159.99896240234375, 483.7089538574219, 479.5811767578125], [0.1574249267578125, 294.58782958984375, 90.5593032836914, 479.65936279296875], [469.88232421875, 413.296875, 518.730712890625, 464.28985595703125], [37.00794219970703, 358.88336181640625, 86.8187484741211, 408.14141845703125], [86.211181640625, 358.6474609375, 205.948974609375, 428.23895263671875], [213.33935546875, 437.435546875, 282.2489013671875, 479.71649169921875], [387.3550720214844, 372.6437072753906, 451.2518615722656, 411.8431701660156], [387.28057861328125, 372.8626708984375, 451.30645751953125, 411.27117919921875], [86.15489959716797, 358.7999267578125, 206.00555419921875, 428.6888427734375], [37.530723571777344, 358.9950866699219, 75.00191497802734, 390.0790710449219]], 'scores': [0.9576729536056519, 0.8788518309593201, 0.5714020729064941, 0.4906502962112427, 0.47182926535606384, 0.4685835540294647, 0.46427130699157715, 0.42181670665740967, 0.3848620057106018, 0.2848130464553833], 'class_indices': [0.0, 0.0, 56.0, 0.0, 63.0, 27.0, 63.0, 62.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:58] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:58]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=382439;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=669774;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 2 chairs, 1 tv, 2 laptops, 211.9ms\n",
      "Speed: 2.4ms preprocess, 211.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.42161560058594, 159.93411254882812, 483.616943359375, 479.5942687988281], [0.18927764892578125, 291.4825439453125, 91.55648040771484, 455.4930419921875], [469.53863525390625, 413.2776184082031, 519.2440795898438, 464.5094299316406], [387.3242492675781, 372.3288269042969, 451.4547424316406, 412.3998718261719], [32.418548583984375, 359.71856689453125, 87.18827056884766, 410.25823974609375], [86.39415740966797, 358.9024658203125, 205.5047607421875, 426.81256103515625], [86.3175048828125, 359.0760498046875, 205.47842407226562, 427.313232421875], [0.289794921875, 447.7805480957031, 78.01853942871094, 479.7303161621094]], 'scores': [0.9566916227340698, 0.8637157678604126, 0.6310420036315918, 0.6115047931671143, 0.5036551356315613, 0.4664323627948761, 0.40886855125427246, 0.2688533067703247], 'class_indices': [0.0, 0.0, 56.0, 63.0, 0.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=898081;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=702177;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 210.8ms\n",
      "Speed: 2.8ms preprocess, 210.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.35345458984375, 159.9314422607422, 483.97406005859375, 479.57720947265625], [0.287139892578125, 290.6663818359375, 94.48651123046875, 479.0391845703125], [86.72943115234375, 359.34698486328125, 205.74539184570312, 425.97869873046875], [469.54168701171875, 413.38214111328125, 518.7157592773438, 464.11920166015625], [387.2723693847656, 372.63824462890625, 451.2266540527344, 411.88238525390625], [213.30316162109375, 437.8218688964844, 281.44976806640625, 479.7005310058594]], 'scores': [0.9560900330543518, 0.8479280471801758, 0.5929900407791138, 0.577798068523407, 0.5699040293693542, 0.30913063883781433], 'class_indices': [0.0, 0.0, 62.0, 56.0, 63.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=541713;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=372049;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 laptops, 213.0ms\n",
      "Speed: 2.0ms preprocess, 213.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.61482238769531, 159.9891357421875, 483.58453369140625, 479.58172607421875], [0.1032257080078125, 295.8382568359375, 90.18045043945312, 455.912109375], [387.41339111328125, 372.63201904296875, 451.34942626953125, 411.98541259765625], [86.5391616821289, 359.09332275390625, 205.37753295898438, 428.76251220703125], [469.77685546875, 413.464111328125, 519.0059814453125, 464.57000732421875], [213.49359130859375, 438.8777160644531, 281.73382568359375, 479.7174987792969], [32.73326873779297, 358.41839599609375, 86.96726989746094, 404.35064697265625]], 'scores': [0.9535225629806519, 0.7419127225875854, 0.6978116631507874, 0.6771770119667053, 0.5158469080924988, 0.4518432915210724, 0.37139710783958435], 'class_indices': [0.0, 0.0, 63.0, 63.0, 56.0, 27.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=39836;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=304361;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 tvs, 1 laptop, 1 toothbrush, 210.7ms\n",
      "Speed: 2.4ms preprocess, 210.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.97517395019531, 160.0675048828125, 483.87237548828125, 479.62158203125], [0.17298126220703125, 292.6826171875, 91.19977569580078, 477.7332763671875], [387.3622131347656, 372.6073303222656, 451.2879943847656, 411.9918518066406], [86.78365325927734, 358.90985107421875, 205.46566772460938, 427.4996337890625], [469.59716796875, 413.44403076171875, 518.7352294921875, 464.91607666015625], [34.33442687988281, 358.0316162109375, 87.59251403808594, 405.846435546875], [387.54949951171875, 372.646728515625, 451.31365966796875, 411.555419921875], [214.343017578125, 319.64471435546875, 275.92822265625, 341.02471923828125]], 'scores': [0.9575058221817017, 0.6714867353439331, 0.5833219885826111, 0.5725178122520447, 0.5018985271453857, 0.3746243417263031, 0.36937862634658813, 0.2573201358318329], 'class_indices': [0.0, 0.0, 63.0, 62.0, 56.0, 0.0, 62.0, 79.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=94014;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=85377;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 209.8ms\n",
      "Speed: 2.3ms preprocess, 209.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.2327880859375, 160.1769561767578, 483.5123291015625, 479.58221435546875], [0.144500732421875, 291.3060302734375, 96.71202850341797, 479.24932861328125], [87.49536895751953, 359.15887451171875, 205.534423828125, 425.231201171875], [469.8765869140625, 413.2724914550781, 518.727783203125, 464.6726379394531], [387.0546875, 372.8506164550781, 451.3017578125, 411.7226867675781], [386.8432922363281, 373.03314208984375, 451.4449768066406, 411.13201904296875], [34.07481384277344, 358.5915832519531, 76.34042358398438, 387.2540588378906], [34.21336364746094, 358.565185546875, 87.27268981933594, 403.13427734375], [212.9827880859375, 437.75201416015625, 281.7911682128906, 479.71575927734375]], 'scores': [0.9564580917358398, 0.8753997087478638, 0.6454534530639648, 0.5808396935462952, 0.48427435755729675, 0.4531954526901245, 0.42519527673721313, 0.38298550248146057, 0.37471920251846313], 'class_indices': [0.0, 0.0, 62.0, 56.0, 63.0, 62.0, 0.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:14:59] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:14:59]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=255432;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=288743;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 213.4ms\n",
      "Speed: 2.6ms preprocess, 213.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.18727111816406, 159.8816680908203, 483.4322509765625, 479.62701416015625], [0.2926788330078125, 291.986083984375, 96.8531265258789, 478.71710205078125], [386.98419189453125, 372.43365478515625, 451.16693115234375, 411.99853515625], [87.36343383789062, 358.9385986328125, 205.41937255859375, 422.412841796875], [469.78594970703125, 413.18560791015625, 518.9757690429688, 464.38739013671875], [36.76661682128906, 357.69976806640625, 76.67727661132812, 386.634765625], [212.21945190429688, 437.69146728515625, 281.7352600097656, 479.74017333984375], [36.36277770996094, 357.7323913574219, 87.2643051147461, 402.2409973144531]], 'scores': [0.9568142890930176, 0.9059960842132568, 0.6374781131744385, 0.5869191884994507, 0.47590380907058716, 0.4218595027923584, 0.34430909156799316, 0.32852792739868164], 'class_indices': [0.0, 0.0, 63.0, 62.0, 56.0, 0.0, 27.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=126274;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=705217;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 1 tv, 1 laptop, 212.0ms\n",
      "Speed: 3.2ms preprocess, 212.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.07084655761719, 159.97772216796875, 484.01397705078125, 479.576171875], [0.1663055419921875, 291.7802734375, 96.55276489257812, 478.46044921875], [387.37322998046875, 372.62451171875, 451.14459228515625, 411.7708740234375], [469.80145263671875, 413.5047912597656, 518.605224609375, 464.1451110839844], [87.05062866210938, 359.16937255859375, 205.37930297851562, 424.07000732421875], [34.69415283203125, 357.57403564453125, 77.11918640136719, 387.11212158203125], [34.768714904785156, 357.47772216796875, 87.21672058105469, 402.86663818359375]], 'scores': [0.9568188786506653, 0.7963380217552185, 0.6239530444145203, 0.6116162538528442, 0.5592787265777588, 0.45711860060691833, 0.2870745062828064], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=904635;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=173642;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 213.1ms\n",
      "Speed: 2.4ms preprocess, 213.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.3270263671875, 160.034423828125, 484.73345947265625, 479.58416748046875], [0.30609130859375, 292.11383056640625, 95.25200653076172, 478.92950439453125], [86.9547348022461, 359.10626220703125, 205.43972778320312, 424.91070556640625], [387.47479248046875, 372.3642578125, 451.58087158203125, 410.8358154296875], [469.95123291015625, 413.1666259765625, 519.0011596679688, 464.26953125], [34.66102600097656, 357.86871337890625, 77.06636047363281, 386.928955078125], [213.3033447265625, 438.0510559082031, 282.4072265625, 479.7203063964844], [34.5345458984375, 357.737060546875, 87.22782897949219, 403.44482421875]], 'scores': [0.9584082365036011, 0.9007517099380493, 0.6529884338378906, 0.6193514466285706, 0.5984535813331604, 0.4808221459388733, 0.3259141147136688, 0.25661543011665344], 'class_indices': [0.0, 0.0, 62.0, 63.0, 56.0, 0.0, 27.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=115411;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=118221;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 211.9ms\n",
      "Speed: 2.5ms preprocess, 211.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.10185241699219, 159.96115112304688, 484.70538330078125, 479.5542297363281], [0.2823028564453125, 292.2056884765625, 95.2275161743164, 479.062744140625], [387.4921875, 372.598388671875, 451.3990478515625, 410.501708984375], [86.83010864257812, 359.08966064453125, 205.6390380859375, 424.99676513671875], [470.19677734375, 413.4337463378906, 518.46875, 464.6908874511719], [35.97673797607422, 358.2681884765625, 87.29244995117188, 404.955322265625], [36.705841064453125, 358.4369201660156, 76.35050201416016, 387.9387512207031], [213.15936279296875, 436.551025390625, 282.139892578125, 479.7080078125]], 'scores': [0.9569612145423889, 0.914716899394989, 0.634887158870697, 0.5786911845207214, 0.5434709787368774, 0.3632764220237732, 0.36122897267341614, 0.329978883266449], 'class_indices': [0.0, 0.0, 63.0, 62.0, 56.0, 0.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:00] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=311580;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=308828;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 laptops, 209.9ms\n",
      "Speed: 2.6ms preprocess, 209.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.357666015625, 160.0006866455078, 484.85821533203125, 479.58978271484375], [0.1214447021484375, 291.50506591796875, 95.3600845336914, 478.92498779296875], [387.4942626953125, 372.3695068359375, 451.556640625, 409.63543701171875], [469.746826171875, 413.5618896484375, 519.0645751953125, 464.4432373046875], [86.94915771484375, 358.7472229003906, 205.58990478515625, 424.2170715332031], [35.017913818359375, 358.091796875, 86.5047378540039, 404.48388671875]], 'scores': [0.9554895758628845, 0.8641274571418762, 0.6507350206375122, 0.5500431060791016, 0.5439285635948181, 0.43953433632850647], 'class_indices': [0.0, 0.0, 63.0, 56.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=703207;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=27038;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 214.1ms\n",
      "Speed: 4.9ms preprocess, 214.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.35906982421875, 160.16973876953125, 485.2767333984375, 479.54766845703125], [0.3315277099609375, 292.0738830566406, 95.19331359863281, 479.3130187988281], [86.98616027832031, 358.9678039550781, 205.56153869628906, 425.3700866699219], [469.64739990234375, 413.409423828125, 518.7393188476562, 464.4222412109375], [387.55462646484375, 372.8236999511719, 451.48492431640625, 409.2829284667969], [33.33283233642578, 358.13018798828125, 76.11565399169922, 386.63116455078125], [387.70263671875, 372.8118896484375, 451.48797607421875, 409.1141357421875], [213.16366577148438, 436.15863037109375, 282.525146484375, 479.71466064453125], [33.43757629394531, 357.95013427734375, 86.99790954589844, 405.119140625]], 'scores': [0.9524593949317932, 0.9180392026901245, 0.6201796531677246, 0.5767945647239685, 0.4804624915122986, 0.4725838899612427, 0.41315892338752747, 0.3590041399002075, 0.30136987566947937], 'class_indices': [0.0, 0.0, 62.0, 56.0, 63.0, 0.0, 62.0, 27.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=455574;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=444305;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 2 laptops, 211.4ms\n",
      "Speed: 2.3ms preprocess, 211.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.38407897949219, 159.92669677734375, 484.77410888671875, 479.55908203125], [0.14611053466796875, 291.66009521484375, 96.25029754638672, 479.1571044921875], [387.5155029296875, 372.68829345703125, 451.38275146484375, 411.1708984375], [469.58551025390625, 413.46551513671875, 518.4632568359375, 464.25836181640625], [86.97183227539062, 358.74267578125, 205.37783813476562, 424.807861328125], [35.33116149902344, 357.9715576171875, 86.37739562988281, 404.544677734375], [86.92598724365234, 358.86151123046875, 205.29428100585938, 425.03662109375], [387.6143798828125, 372.68621826171875, 451.4632568359375, 410.64697265625], [36.169647216796875, 357.94921875, 76.06957244873047, 388.739013671875]], 'scores': [0.9557314515113831, 0.9015718102455139, 0.5469602942466736, 0.5241342186927795, 0.501183271408081, 0.41343411803245544, 0.40536943078041077, 0.3508126735687256, 0.34049439430236816], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 0.0, 63.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=604844;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=306046;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 1 tv, 1 laptop, 1 toothbrush, 210.6ms\n",
      "Speed: 4.1ms preprocess, 210.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.79243469238281, 160.28030395507812, 483.74969482421875, 479.5547790527344], [0.31449127197265625, 292.18231201171875, 95.14923095703125, 478.92437744140625], [387.27911376953125, 372.51654052734375, 451.5225830078125, 412.05194091796875], [87.09456634521484, 358.97515869140625, 205.62930297851562, 425.6837158203125], [469.64178466796875, 413.24017333984375, 519.1143798828125, 464.3360595703125], [34.90843200683594, 358.23175048828125, 86.70321655273438, 404.69732666015625], [214.21902465820312, 319.8493347167969, 274.9515075683594, 342.2463684082031]], 'scores': [0.9576248526573181, 0.9153410196304321, 0.6077789068222046, 0.5835201740264893, 0.5025224089622498, 0.45451366901397705, 0.2842528223991394], 'class_indices': [0.0, 0.0, 63.0, 62.0, 56.0, 0.0, 79.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=765911;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=508828;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 3 laptops, 215.2ms\n",
      "Speed: 4.4ms preprocess, 215.2ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.74119567871094, 160.30543518066406, 483.9459228515625, 479.5989990234375], [0.21547698974609375, 291.65911865234375, 95.35958099365234, 478.51910400390625], [469.927001953125, 413.27435302734375, 518.90087890625, 463.98773193359375], [387.346923828125, 372.7332763671875, 451.35546875, 411.6104736328125], [86.82588958740234, 358.8451232910156, 205.49581909179688, 425.1269226074219], [213.2899169921875, 437.926025390625, 282.2211608886719, 479.70220947265625], [34.31532287597656, 358.1011962890625, 86.64508819580078, 406.205810546875], [0.20956802368164062, 408.39605712890625, 28.466949462890625, 431.77203369140625]], 'scores': [0.9573305249214172, 0.7692816853523254, 0.6164103150367737, 0.6006489396095276, 0.5750681757926941, 0.5351805090904236, 0.5175999402999878, 0.2886577844619751], 'class_indices': [0.0, 0.0, 56.0, 63.0, 63.0, 27.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:01] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:01]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=283279;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=761759;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 2 chairs, 1 tv, 1 laptop, 209.3ms\n",
      "Speed: 2.1ms preprocess, 209.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.487701416015625, 159.98269653320312, 484.0434875488281, 479.5852355957031], [0.236419677734375, 292.2734375, 94.92697143554688, 477.583251953125], [387.3341064453125, 372.6092529296875, 451.24609375, 412.6134033203125], [86.97216033935547, 358.7178955078125, 205.552490234375, 425.8955078125], [469.859375, 413.44696044921875, 518.9149169921875, 464.43756103515625], [35.99463653564453, 358.547119140625, 76.3816146850586, 388.21527099609375], [213.37939453125, 438.1556091308594, 282.34124755859375, 479.7257385253906], [34.99779510498047, 358.6808166503906, 86.60102844238281, 403.9786682128906], [469.650390625, 413.24224853515625, 541.867919921875, 479.75958251953125]], 'scores': [0.9581218957901001, 0.6804622411727905, 0.6257755756378174, 0.6085717678070068, 0.4563676714897156, 0.40183335542678833, 0.3036585748195648, 0.3025708496570587, 0.2656474709510803], 'class_indices': [0.0, 0.0, 63.0, 62.0, 56.0, 0.0, 27.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=584338;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=491511;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 2 chairs, 2 tvs, 1 laptop, 211.2ms\n",
      "Speed: 2.8ms preprocess, 211.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[57.073883056640625, 160.1239013671875, 484.1492004394531, 479.60595703125], [0.30693817138671875, 292.15594482421875, 94.21144104003906, 479.30352783203125], [469.5555419921875, 413.533203125, 518.7578125, 464.59326171875], [87.09784698486328, 358.7838134765625, 205.71002197265625, 426.01409912109375], [387.461181640625, 373.00921630859375, 451.15838623046875, 412.06268310546875], [387.34942626953125, 373.158935546875, 451.28668212890625, 411.5098876953125], [213.3931884765625, 437.57958984375, 282.35919189453125, 479.7183837890625], [34.94090270996094, 358.61370849609375, 86.58219146728516, 405.66558837890625], [475.7828369140625, 462.62054443359375, 538.7810668945312, 479.75384521484375], [35.4315185546875, 358.39215087890625, 75.98567199707031, 387.94866943359375]], 'scores': [0.9611408710479736, 0.9147876501083374, 0.658447265625, 0.5406342148780823, 0.5261180996894836, 0.4699796736240387, 0.4022189676761627, 0.3492637872695923, 0.34445101022720337, 0.2839775085449219], 'class_indices': [0.0, 0.0, 56.0, 62.0, 63.0, 62.0, 27.0, 0.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=318622;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=989863;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 207.6ms\n",
      "Speed: 2.4ms preprocess, 207.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.75885009765625, 160.3367919921875, 483.970458984375, 479.58251953125], [0.1360626220703125, 292.082763671875, 94.15821838378906, 479.65264892578125], [87.00213623046875, 358.70941162109375, 205.67361450195312, 426.363037109375], [469.6004638671875, 413.3424072265625, 518.8565673828125, 464.3226318359375], [387.4677734375, 372.6343994140625, 451.3056640625, 412.518798828125], [35.47259521484375, 358.62493896484375, 87.12298583984375, 405.83685302734375], [387.396484375, 372.831298828125, 451.428955078125, 411.7113037109375], [36.079071044921875, 358.28173828125, 76.07291412353516, 387.7091064453125]], 'scores': [0.9584808349609375, 0.8557074666023254, 0.6145297884941101, 0.5137737989425659, 0.482014536857605, 0.3800734877586365, 0.3355432450771332, 0.2847157418727875], 'class_indices': [0.0, 0.0, 62.0, 56.0, 63.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=801929;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=883551;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 1 tv, 1 laptop, 212.8ms\n",
      "Speed: 2.1ms preprocess, 212.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[57.44404602050781, 160.04652404785156, 484.04656982421875, 479.5814208984375], [0.297698974609375, 291.799560546875, 96.65682983398438, 478.2401123046875], [469.6038818359375, 413.3809509277344, 518.608154296875, 464.2291564941406], [387.488525390625, 372.75146484375, 451.154541015625, 411.93316650390625], [87.31674194335938, 358.9637451171875, 205.41091918945312, 425.2362060546875], [34.648712158203125, 358.32196044921875, 76.12599182128906, 387.72210693359375], [34.35869598388672, 358.2740478515625, 86.77615356445312, 404.15478515625]], 'scores': [0.9549891948699951, 0.8799367547035217, 0.6826745271682739, 0.6221573948860168, 0.6115253567695618, 0.3575131893157959, 0.3445671498775482], 'class_indices': [0.0, 0.0, 56.0, 63.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:02] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:02]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=916504;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=239392;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 3 chairs, 1 tv, 2 laptops, 210.5ms\n",
      "Speed: 3.2ms preprocess, 210.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.84938049316406, 160.18777465820312, 483.826416015625, 479.5621032714844], [0.2166595458984375, 291.58087158203125, 95.99761199951172, 476.2557373046875], [387.4960021972656, 372.7016906738281, 451.1717224121094, 411.5223693847656], [87.02139282226562, 358.867919921875, 205.52194213867188, 424.9229736328125], [34.449363708496094, 358.33184814453125, 87.0901870727539, 404.6636962890625], [213.18316650390625, 437.815185546875, 282.1632080078125, 479.7210693359375], [87.01142120361328, 358.75494384765625, 205.6097412109375, 424.69000244140625], [470.02374267578125, 413.2933044433594, 518.8234252929688, 464.9299011230469], [0.31137847900390625, 447.16534423828125, 79.23945617675781, 479.73492431640625], [469.8140869140625, 413.11590576171875, 540.8748779296875, 479.7510986328125], [35.271209716796875, 358.16192626953125, 75.99088287353516, 387.68341064453125]], 'scores': [0.954506516456604, 0.6732299327850342, 0.6164615154266357, 0.49184632301330566, 0.48505285382270813, 0.46705830097198486, 0.4607677161693573, 0.4296242594718933, 0.28222304582595825, 0.2748468816280365, 0.2697608470916748], 'class_indices': [0.0, 0.0, 63.0, 62.0, 0.0, 27.0, 63.0, 56.0, 56.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=886878;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=975759;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 214.1ms\n",
      "Speed: 2.2ms preprocess, 214.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[57.55955505371094, 160.2481689453125, 483.8275146484375, 479.5596923828125], [0.30019378662109375, 291.95697021484375, 95.06649017333984, 477.85089111328125], [87.01019287109375, 358.6339111328125, 205.51547241210938, 425.15283203125], [387.5255126953125, 372.59490966796875, 451.5396728515625, 412.8192138671875], [213.27362060546875, 437.0283203125, 283.0849609375, 479.74072265625], [469.8590087890625, 413.45703125, 518.2703857421875, 464.3829345703125], [35.35143280029297, 358.06976318359375, 76.0634994506836, 387.64508056640625], [34.97901153564453, 357.9508056640625, 87.08734130859375, 406.6846923828125]], 'scores': [0.9549947381019592, 0.7481285333633423, 0.6722844243049622, 0.6462365388870239, 0.6176615357398987, 0.6147159337997437, 0.40168437361717224, 0.3392915725708008], 'class_indices': [0.0, 0.0, 62.0, 63.0, 27.0, 56.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=759210;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=741138;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 1 tv, 2 laptops, 211.1ms\n",
      "Speed: 2.3ms preprocess, 211.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[57.57196044921875, 160.0850830078125, 483.9337158203125, 479.600341796875], [0.1666107177734375, 292.36297607421875, 95.15611267089844, 479.05999755859375], [387.53448486328125, 372.7150573730469, 451.3768310546875, 412.3829040527344], [86.87561798095703, 358.85626220703125, 205.63909912109375, 425.15936279296875], [469.57635498046875, 413.2991943359375, 518.9349975585938, 464.3636474609375], [35.517669677734375, 358.3660888671875, 76.59228515625, 387.2703857421875], [35.21161651611328, 358.46014404296875, 87.2568130493164, 405.17047119140625], [0.18601036071777344, 408.421875, 28.77157974243164, 443.06787109375]], 'scores': [0.9595423340797424, 0.8953056931495667, 0.6373372673988342, 0.5815532803535461, 0.5747745633125305, 0.4054854214191437, 0.3670084476470947, 0.31100940704345703], 'class_indices': [0.0, 0.0, 63.0, 62.0, 56.0, 0.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=332220;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=600016;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 209.0ms\n",
      "Speed: 2.2ms preprocess, 209.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[57.00657653808594, 160.3409423828125, 483.82177734375, 479.55401611328125], [0.29422760009765625, 292.1553039550781, 94.8675765991211, 478.6762390136719], [470.14617919921875, 413.32244873046875, 518.8109741210938, 464.45257568359375], [86.75405883789062, 359.10675048828125, 205.39968872070312, 425.59576416015625], [387.4814147949219, 372.4071044921875, 451.4822082519531, 412.124755859375], [35.014549255371094, 358.20068359375, 86.52605438232422, 405.8394775390625], [214.1029052734375, 438.0807189941406, 282.4992370605469, 479.7168884277344], [387.64837646484375, 372.5530090332031, 451.43890380859375, 411.7908020019531], [35.26039123535156, 358.28704833984375, 77.20576477050781, 388.51904296875]], 'scores': [0.9552572965621948, 0.8914047479629517, 0.6308268308639526, 0.6098997592926025, 0.546306312084198, 0.46914219856262207, 0.432504802942276, 0.3590849041938782, 0.25270187854766846], 'class_indices': [0.0, 0.0, 56.0, 62.0, 63.0, 0.0, 27.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=133542;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=917687;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 214.3ms\n",
      "Speed: 1.8ms preprocess, 214.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.898101806640625, 160.36920166015625, 484.6112365722656, 479.58251953125], [0.15682220458984375, 292.22216796875, 95.57919311523438, 479.13897705078125], [388.1015625, 372.55072021484375, 451.2081298828125, 411.41302490234375], [86.90359497070312, 358.8280029296875, 205.51760864257812, 425.5350341796875], [469.43133544921875, 413.2485046386719, 519.0459594726562, 464.4490051269531], [35.7021484375, 357.84423828125, 86.68045043945312, 404.660888671875], [36.41911315917969, 357.9526672363281, 77.09648132324219, 387.9065856933594], [214.09243774414062, 437.1226501464844, 282.8177795410156, 479.7145080566406]], 'scores': [0.9549264907836914, 0.8602097630500793, 0.5898956656455994, 0.5601591467857361, 0.5091794729232788, 0.3719298243522644, 0.2789987027645111, 0.27314475178718567], 'class_indices': [0.0, 0.0, 63.0, 62.0, 56.0, 0.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:03] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:03]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=60104;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=424782;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 208.8ms\n",
      "Speed: 2.6ms preprocess, 208.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[57.847412109375, 160.19476318359375, 484.496826171875, 479.5721435546875], [0.3130035400390625, 292.1341857910156, 95.62699890136719, 478.8492126464844], [87.134765625, 358.87689208984375, 205.81805419921875, 425.6531982421875], [469.931396484375, 413.169189453125, 518.815673828125, 465.01220703125], [388.05279541015625, 372.61993408203125, 451.25640869140625, 411.77435302734375], [213.5728759765625, 438.26678466796875, 282.14434814453125, 479.72271728515625], [35.10063171386719, 357.5936279296875, 87.19171142578125, 404.334228515625], [35.26036071777344, 357.54315185546875, 77.57318115234375, 387.55816650390625]], 'scores': [0.9553793668746948, 0.8957921266555786, 0.6058616638183594, 0.5751427412033081, 0.5545894503593445, 0.4832035005092621, 0.4645124077796936, 0.32462868094444275], 'class_indices': [0.0, 0.0, 62.0, 56.0, 63.0, 27.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=549744;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=282845;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 207.4ms\n",
      "Speed: 2.9ms preprocess, 207.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.287445068359375, 160.061767578125, 484.5547180175781, 479.58966064453125], [0.303466796875, 292.809326171875, 95.94011688232422, 478.93695068359375], [387.99822998046875, 372.8121337890625, 451.15655517578125, 411.58270263671875], [469.818603515625, 413.47894287109375, 518.850341796875, 464.61871337890625], [86.7318115234375, 358.51800537109375, 205.8857421875, 425.40582275390625], [36.23119354248047, 357.86834716796875, 86.10478973388672, 405.95611572265625], [214.26727294921875, 437.4615478515625, 282.9959716796875, 479.720947265625], [36.760658264160156, 357.9287109375, 77.1390609741211, 388.719970703125]], 'scores': [0.9578426480293274, 0.9154118299484253, 0.6677610874176025, 0.634437620639801, 0.6062512993812561, 0.499496728181839, 0.4531819522380829, 0.257138192653656], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 0.0, 27.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=147336;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=180570;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 1 tv, 2 laptops, 211.3ms\n",
      "Speed: 1.8ms preprocess, 211.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.54896545410156, 160.33941650390625, 484.347900390625, 479.5281982421875], [0.1463165283203125, 294.8994140625, 95.26820373535156, 479.452880859375], [388.0726013183594, 372.7696533203125, 451.3699035644531, 411.27978515625], [469.939208984375, 413.0281677246094, 518.9075927734375, 464.5877990722656], [86.50428771972656, 359.06085205078125, 205.5254364013672, 425.40618896484375], [86.42616271972656, 358.6954345703125, 205.46018981933594, 425.7010498046875], [34.28021240234375, 358.19354248046875, 86.37429809570312, 407.20025634765625], [34.430503845214844, 358.1096496582031, 77.24750518798828, 389.8077087402344]], 'scores': [0.9532040953636169, 0.796816885471344, 0.6178900599479675, 0.609161376953125, 0.4724978506565094, 0.4701996445655823, 0.3813076317310333, 0.3025197684764862], 'class_indices': [0.0, 0.0, 63.0, 56.0, 63.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=880968;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=962633;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 211.8ms\n",
      "Speed: 4.8ms preprocess, 211.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[57.19416809082031, 160.2545166015625, 485.28240966796875, 479.58026123046875], [86.45890808105469, 358.734619140625, 205.00926208496094, 425.61474609375], [388.7683410644531, 372.7039794921875, 451.3833923339844, 409.46282958984375], [0.15131378173828125, 294.24578857421875, 94.41190338134766, 479.4949951171875], [470.04644775390625, 413.4344482421875, 518.9297485351562, 464.32440185546875], [214.68505859375, 436.79949951171875, 283.55096435546875, 479.73077392578125], [35.27119445800781, 358.8505859375, 77.2144775390625, 390.14306640625], [34.900474548339844, 358.7119140625, 85.38516235351562, 405.7294921875]], 'scores': [0.9539406895637512, 0.6922711133956909, 0.682375967502594, 0.6697799563407898, 0.584380030632019, 0.44825029373168945, 0.3449159264564514, 0.2907516062259674], 'class_indices': [0.0, 62.0, 63.0, 0.0, 56.0, 27.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:04] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:04]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=84114;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=73467;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 2 laptops, 216.7ms\n",
      "Speed: 2.4ms preprocess, 216.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.68408203125, 160.3128662109375, 485.1595458984375, 479.55169677734375], [0.12953948974609375, 294.2217102050781, 95.54837799072266, 479.7909240722656], [389.05682373046875, 372.73358154296875, 451.35467529296875, 409.43865966796875], [214.68399047851562, 436.1019287109375, 284.2509460449219, 479.747314453125], [470.13262939453125, 413.6474304199219, 518.7865600585938, 464.3485412597656], [86.61105346679688, 358.906982421875, 205.75497436523438, 425.457275390625], [86.56101989746094, 358.56903076171875, 205.7254180908203, 425.73089599609375], [35.63268280029297, 358.69659423828125, 77.10966491699219, 389.9942626953125], [35.22880554199219, 358.5566101074219, 89.21710205078125, 406.7325744628906]], 'scores': [0.9573682546615601, 0.8315707445144653, 0.6326485872268677, 0.5985633730888367, 0.5706264972686768, 0.5198619365692139, 0.47167596220970154, 0.3142954707145691, 0.3074638247489929], 'class_indices': [0.0, 0.0, 63.0, 27.0, 56.0, 63.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=389753;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=162735;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 214.5ms\n",
      "Speed: 2.6ms preprocess, 214.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.97102355957031, 160.37783813476562, 485.59515380859375, 479.5797424316406], [0.3161773681640625, 294.38140869140625, 95.82330322265625, 479.29412841796875], [86.509033203125, 358.55938720703125, 205.411865234375, 425.30096435546875], [36.6876220703125, 357.8122253417969, 75.72418212890625, 388.2387390136719], [470.04443359375, 413.3736267089844, 518.769287109375, 464.2734680175781], [389.1824035644531, 372.53521728515625, 451.3627624511719, 409.30548095703125], [213.93704223632812, 436.09326171875, 284.0032653808594, 479.764892578125], [389.0989685058594, 372.5263671875, 451.3124694824219, 409.069580078125]], 'scores': [0.955346941947937, 0.9232646226882935, 0.6287347078323364, 0.6008985042572021, 0.5735039710998535, 0.5371291041374207, 0.4389587640762329, 0.3946143686771393], 'class_indices': [0.0, 0.0, 62.0, 0.0, 56.0, 63.0, 27.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=516327;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=20085;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 207.1ms\n",
      "Speed: 2.1ms preprocess, 207.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.7972412109375, 160.5265350341797, 485.8990478515625, 479.59637451171875], [0.1914215087890625, 294.51708984375, 95.44448852539062, 479.787109375], [86.82156372070312, 359.0081481933594, 205.56405639648438, 425.2340393066406], [470.28399658203125, 413.47320556640625, 519.1180419921875, 464.32464599609375], [213.984375, 436.352783203125, 284.7919921875, 479.75390625], [389.36224365234375, 372.8228759765625, 451.3106689453125, 408.2197265625], [36.847412109375, 357.7908020019531, 73.96284484863281, 387.9546813964844], [36.25408172607422, 357.54388427734375, 84.8072509765625, 404.65179443359375]], 'scores': [0.9555739164352417, 0.9053253531455994, 0.660651683807373, 0.6549199819564819, 0.6014035940170288, 0.5865510702133179, 0.5169342160224915, 0.27117758989334106], 'class_indices': [0.0, 0.0, 62.0, 56.0, 27.0, 63.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=12851;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=991267;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 1 toothbrush, 213.4ms\n",
      "Speed: 2.3ms preprocess, 213.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.620452880859375, 160.42202758789062, 486.0624694824219, 479.5346984863281], [0.2254486083984375, 294.5673522949219, 95.74119567871094, 478.7501525878906], [86.6513671875, 358.6029968261719, 205.54867553710938, 424.9183044433594], [470.2391357421875, 413.314697265625, 519.010986328125, 464.76513671875], [389.61370849609375, 372.60968017578125, 451.29766845703125, 407.97344970703125], [217.48095703125, 319.38079833984375, 277.8272705078125, 342.38470458984375], [35.1285400390625, 358.4538879394531, 72.4129638671875, 385.9656066894531], [34.790435791015625, 358.26641845703125, 83.45217895507812, 405.195556640625], [213.14349365234375, 435.2342529296875, 284.0452575683594, 479.67919921875], [389.49591064453125, 372.6593933105469, 451.35296630859375, 408.0426330566406]], 'scores': [0.9531756043434143, 0.8498793244361877, 0.7238073348999023, 0.5852243304252625, 0.5055434703826904, 0.47420549392700195, 0.4308604896068573, 0.3309294581413269, 0.3244776427745819, 0.2584734559059143], 'class_indices': [0.0, 0.0, 62.0, 56.0, 63.0, 79.0, 0.0, 0.0, 27.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=848126;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=728073;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 2 chairs, 2 tvs, 1 laptop, 1 toothbrush, 212.5ms\n",
      "Speed: 6.0ms preprocess, 212.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.651336669921875, 160.56495666503906, 485.6478576660156, 479.5487060546875], [0.31121826171875, 294.3704833984375, 94.73188781738281, 478.8408203125], [86.73165893554688, 358.52496337890625, 205.47598266601562, 425.57403564453125], [214.08438110351562, 435.83331298828125, 284.5367126464844, 479.76776123046875], [470.18499755859375, 413.37738037109375, 518.7101440429688, 464.36956787109375], [389.38861083984375, 372.78033447265625, 451.53631591796875, 407.86236572265625], [389.27337646484375, 372.53363037109375, 451.38116455078125, 408.2508544921875], [34.1925048828125, 357.4195556640625, 83.35682678222656, 405.7197265625], [217.58773803710938, 319.2515869140625, 278.0121765136719, 342.413330078125], [34.69197082519531, 357.5576171875, 72.23280334472656, 386.1480712890625], [470.1820068359375, 413.28948974609375, 544.2913208007812, 479.73443603515625]], 'scores': [0.9543986916542053, 0.9067183136940002, 0.6214655637741089, 0.6074987649917603, 0.5440468192100525, 0.4620581865310669, 0.4619583785533905, 0.4392974376678467, 0.3198327422142029, 0.2713031470775604, 0.25347471237182617], 'class_indices': [0.0, 0.0, 62.0, 27.0, 56.0, 62.0, 63.0, 0.0, 79.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:05] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:05]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=209773;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=682484;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 211.0ms\n",
      "Speed: 4.3ms preprocess, 211.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.97358703613281, 160.46099853515625, 485.13946533203125, 479.57843017578125], [0.312164306640625, 294.66229248046875, 94.76671600341797, 479.35687255859375], [86.94644165039062, 358.56964111328125, 205.56817626953125, 425.74127197265625], [388.94927978515625, 373.014404296875, 451.25421142578125, 409.5980224609375], [34.935523986816406, 358.20013427734375, 84.11113739013672, 405.411376953125], [470.115234375, 413.3840026855469, 518.737060546875, 464.4126281738281], [35.318885803222656, 358.357177734375, 72.1358413696289, 385.15576171875], [388.9057922363281, 372.9951171875, 451.2669982910156, 409.41424560546875]], 'scores': [0.9586337208747864, 0.9232192039489746, 0.7175549268722534, 0.5158990621566772, 0.4979707896709442, 0.4169510304927826, 0.28922298550605774, 0.2757014334201813], 'class_indices': [0.0, 0.0, 62.0, 63.0, 0.0, 56.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=519582;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=579636;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 209.4ms\n",
      "Speed: 2.7ms preprocess, 209.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.69566345214844, 160.35116577148438, 484.7005615234375, 479.5492858886719], [0.31744384765625, 293.890625, 94.27635192871094, 479.33447265625], [388.537353515625, 372.42608642578125, 451.36676025390625, 409.84332275390625], [470.05181884765625, 413.25421142578125, 518.8693237304688, 464.5723876953125], [86.55123901367188, 359.0880432128906, 205.47280883789062, 426.0671081542969], [34.28448486328125, 357.4417724609375, 72.60359191894531, 386.2906494140625], [33.83191680908203, 357.60015869140625, 84.87973022460938, 406.07110595703125], [214.22357177734375, 437.16839599609375, 283.8340148925781, 479.7359619140625]], 'scores': [0.9596981406211853, 0.9168567061424255, 0.6762489676475525, 0.5921751856803894, 0.581610381603241, 0.3949938714504242, 0.38989710807800293, 0.37550050020217896], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 0.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=967411;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=324423;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 202.4ms\n",
      "Speed: 2.5ms preprocess, 202.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[57.28599548339844, 160.42263793945312, 484.0264892578125, 479.6427307128906], [0.333831787109375, 292.7928161621094, 95.62535095214844, 479.0945739746094], [387.55999755859375, 372.6249694824219, 451.14923095703125, 410.7366027832031], [86.61941528320312, 358.63128662109375, 205.47222900390625, 425.85577392578125], [470.10748291015625, 413.4129333496094, 519.0313110351562, 464.2095031738281], [31.220739364624023, 357.9813232421875, 85.0888442993164, 405.81494140625], [214.01229858398438, 437.9769287109375, 282.8765563964844, 479.7738037109375]], 'scores': [0.9570402503013611, 0.91804039478302, 0.6581270098686218, 0.5855875015258789, 0.5775406956672668, 0.49301308393478394, 0.3519798517227173], 'class_indices': [0.0, 0.0, 63.0, 62.0, 56.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=888111;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=541945;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 211.8ms\n",
      "Speed: 5.7ms preprocess, 211.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.43357849121094, 160.20608520507812, 483.9495849609375, 479.6575622558594], [0.36000823974609375, 291.662353515625, 94.09051513671875, 479.324951171875], [86.74160766601562, 358.9271240234375, 205.36712646484375, 426.50457763671875], [469.900634765625, 413.60198974609375, 518.6185302734375, 464.46173095703125], [386.48455810546875, 372.87017822265625, 451.42291259765625, 412.64227294921875], [31.3183536529541, 358.4683837890625, 84.0446548461914, 404.4383544921875], [386.34521484375, 372.6383056640625, 451.3607177734375, 413.01513671875], [31.844552993774414, 358.43414306640625, 73.8989486694336, 386.04852294921875]], 'scores': [0.9517862200737, 0.9144414663314819, 0.64203280210495, 0.5810711979866028, 0.4956287145614624, 0.4008784294128418, 0.3549283444881439, 0.2868456542491913], 'class_indices': [0.0, 0.0, 62.0, 56.0, 63.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:06] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:06]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=452151;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=510249;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 2 laptops, 212.5ms\n",
      "Speed: 1.9ms preprocess, 212.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.98846435546875, 161.2639923095703, 483.26727294921875, 479.62713623046875], [0.218017578125, 291.1480407714844, 96.42430114746094, 478.6274108886719], [469.8463134765625, 413.500732421875, 518.5263671875, 464.06951904296875], [30.97772979736328, 358.6402893066406, 85.81742858886719, 403.7522277832031], [385.73297119140625, 372.7370910644531, 451.42547607421875, 411.4308166503906], [86.94133758544922, 358.973388671875, 204.9326171875, 425.50244140625], [86.93672180175781, 359.1741638183594, 205.3748016357422, 425.8900451660156], [385.5113525390625, 372.7218017578125, 451.427734375, 411.4776611328125], [31.24188995361328, 359.04730224609375, 72.47602844238281, 384.18438720703125]], 'scores': [0.9538776278495789, 0.8659242987632751, 0.6774083971977234, 0.555479884147644, 0.5132765769958496, 0.5085774660110474, 0.44973835349082947, 0.3143727779388428, 0.27382680773735046], 'class_indices': [0.0, 0.0, 56.0, 0.0, 63.0, 63.0, 62.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=385304;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=43430;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 4 persons, 1 chair, 1 tv, 1 laptop, 216.0ms\n",
      "Speed: 1.9ms preprocess, 216.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[57.81666564941406, 161.630126953125, 483.3773193359375, 479.6431884765625], [0.1316375732421875, 290.258056640625, 96.74067687988281, 479.1787109375], [87.0273666381836, 358.77886962890625, 205.26449584960938, 425.48553466796875], [469.56341552734375, 413.3927307128906, 518.739990234375, 464.4210510253906], [384.8763427734375, 372.86468505859375, 451.288330078125, 412.06988525390625], [33.26042938232422, 358.07342529296875, 74.66211700439453, 386.33685302734375], [33.3316650390625, 357.921142578125, 86.43120574951172, 404.0987548828125]], 'scores': [0.9515083432197571, 0.900754988193512, 0.6161067485809326, 0.5875024795532227, 0.5645585060119629, 0.3515823483467102, 0.31633761525154114], 'class_indices': [0.0, 0.0, 62.0, 56.0, 63.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=77837;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=249681;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 213.8ms\n",
      "Speed: 2.6ms preprocess, 213.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[58.06736755371094, 161.88499450683594, 482.850341796875, 479.6064453125], [0.3302459716796875, 291.04486083984375, 95.68623352050781, 479.27764892578125], [86.92428588867188, 358.6451721191406, 205.3353271484375, 425.9723205566406], [468.83349609375, 413.6227111816406, 518.533447265625, 464.4546813964844], [382.91497802734375, 373.16436767578125, 451.35211181640625, 412.39215087890625], [33.30487060546875, 358.42608642578125, 87.07914733886719, 406.12847900390625], [33.94178771972656, 358.34234619140625, 74.81370544433594, 386.36846923828125], [382.7005615234375, 372.925048828125, 451.4381103515625, 412.56024169921875]], 'scores': [0.9531070590019226, 0.918083906173706, 0.604458212852478, 0.5811775326728821, 0.49416422843933105, 0.4334217607975006, 0.31607624888420105, 0.2762509882450104], 'class_indices': [0.0, 0.0, 62.0, 56.0, 63.0, 0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=179173;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=102513;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 cell phone, 1 toothbrush, 214.2ms\n",
      "Speed: 2.1ms preprocess, 214.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.4864501953125, 163.072021484375, 480.62408447265625, 479.5166015625], [0.32990264892578125, 291.10546875, 96.43017578125, 478.8555908203125], [87.11555480957031, 359.540283203125, 201.94163513183594, 425.6502685546875], [466.34716796875, 413.37005615234375, 518.201171875, 464.05389404296875], [376.62255859375, 372.8394775390625, 451.394287109375, 413.209228515625], [34.10956573486328, 358.0368957519531, 86.6825942993164, 403.7485046386719], [34.50335693359375, 357.89910888671875, 75.07600402832031, 387.27020263671875], [259.1832275390625, 257.0726318359375, 297.47314453125, 311.797119140625], [213.243408203125, 312.9898681640625, 243.086669921875, 328.03631591796875]], 'scores': [0.9536218643188477, 0.9189661741256714, 0.6503119468688965, 0.6400195360183716, 0.5545140504837036, 0.4511513113975525, 0.3266763687133789, 0.30153635144233704, 0.2751021385192871], 'class_indices': [0.0, 0.0, 62.0, 56.0, 62.0, 0.0, 0.0, 67.0, 79.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=454105;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=876714;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 214.4ms\n",
      "Speed: 4.2ms preprocess, 214.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[52.603729248046875, 163.39407348632812, 473.6093444824219, 479.4184265136719], [0.24993896484375, 291.51251220703125, 97.04010772705078, 478.66656494140625], [86.8881607055664, 358.98284912109375, 205.55221557617188, 425.35211181640625], [463.36688232421875, 413.45245361328125, 518.6749267578125, 464.0162353515625], [33.8057861328125, 358.0771484375, 75.22907257080078, 386.434326171875], [86.85406494140625, 358.7673034667969, 205.59866333007812, 425.7537536621094], [372.33978271484375, 373.0347595214844, 450.94427490234375, 419.3163146972656], [33.385032653808594, 358.1861267089844, 87.52942657470703, 404.2684020996094]], 'scores': [0.9456937313079834, 0.8754072785377502, 0.5777701735496521, 0.47505733370780945, 0.4271954596042633, 0.418948769569397, 0.4093872606754303, 0.26702171564102173], 'class_indices': [0.0, 0.0, 63.0, 56.0, 0.0, 62.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:07] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:07]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=685989;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=655576;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 2 tvs, 212.9ms\n",
      "Speed: 2.4ms preprocess, 212.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[59.09672546386719, 163.03941345214844, 467.22113037109375, 479.43017578125], [0.274871826171875, 290.77008056640625, 96.8203125, 478.14544677734375], [86.90274047851562, 358.7563781738281, 205.53115844726562, 426.1003723144531], [34.61156463623047, 357.9963684082031, 86.62393951416016, 405.8572692871094], [370.2071533203125, 372.7869873046875, 451.0433349609375, 434.345947265625], [35.22266387939453, 358.1016845703125, 75.71548461914062, 385.573974609375]], 'scores': [0.9477603435516357, 0.7361757755279541, 0.5308830738067627, 0.49147364497184753, 0.331631064414978, 0.26379695534706116], 'class_indices': [0.0, 0.0, 62.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=422994;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=722694;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 1 tv, 1 laptop, 207.1ms\n",
      "Speed: 2.3ms preprocess, 207.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.71661376953125, 160.59521484375, 468.07769775390625, 479.47222900390625], [0.217926025390625, 291.564697265625, 95.03804779052734, 478.8878173828125], [461.765869140625, 413.28125, 518.877197265625, 464.08099365234375], [86.86168670654297, 358.83807373046875, 205.5823974609375, 425.58721923828125], [35.049644470214844, 358.35211181640625, 86.31254577636719, 405.26544189453125], [406.4996337890625, 373.186279296875, 450.898681640625, 432.90869140625]], 'scores': [0.9487818479537964, 0.8809170126914978, 0.6058849096298218, 0.5515578985214233, 0.46019279956817627, 0.32439693808555603], 'class_indices': [0.0, 0.0, 56.0, 63.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=263240;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=65222;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 214.1ms\n",
      "Speed: 1.7ms preprocess, 214.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.78082275390625, 162.17337036132812, 467.91400146484375, 479.4024353027344], [0.31951904296875, 291.8758544921875, 95.9722671508789, 478.757568359375], [462.03564453125, 413.59991455078125, 518.5849609375, 464.00390625], [86.882568359375, 358.85723876953125, 205.7349853515625, 425.48321533203125], [86.90361785888672, 359.0992431640625, 205.734619140625, 425.2198486328125], [368.22442626953125, 372.51519775390625, 451.39971923828125, 436.05743408203125], [34.12528991699219, 358.06658935546875, 86.4124755859375, 404.78619384765625], [34.560516357421875, 358.248291015625, 74.83990478515625, 384.6614990234375]], 'scores': [0.9432213306427002, 0.9079346656799316, 0.503131628036499, 0.44813141226768494, 0.44656798243522644, 0.40938684344291687, 0.3151496648788452, 0.26397833228111267], 'class_indices': [0.0, 0.0, 56.0, 62.0, 63.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=254917;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=462554;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 2 chairs, 2 tvs, 1 laptop, 208.3ms\n",
      "Speed: 2.1ms preprocess, 208.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.41389465332031, 162.03140258789062, 468.32318115234375, 479.4292297363281], [0.32975006103515625, 291.7206726074219, 95.76673889160156, 478.9410705566406], [87.03421020507812, 359.0664978027344, 205.53646850585938, 425.2021179199219], [87.0071792602539, 358.81085205078125, 205.52725219726562, 425.50726318359375], [462.28790283203125, 413.38360595703125, 519.0360107421875, 463.76898193359375], [31.502473831176758, 358.38232421875, 86.03349304199219, 405.43994140625], [368.75396728515625, 372.53302001953125, 451.12823486328125, 436.73614501953125], [461.5657958984375, 413.08099365234375, 537.0736694335938, 479.73223876953125], [31.736305236816406, 358.8843688964844, 73.32105255126953, 384.1438903808594]], 'scores': [0.9457557797431946, 0.9120981693267822, 0.46741390228271484, 0.4389244318008423, 0.42225056886672974, 0.41543179750442505, 0.410519003868103, 0.2643934190273285, 0.2535809278488159], 'class_indices': [0.0, 0.0, 63.0, 62.0, 56.0, 0.0, 62.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:08] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=879560;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=759836;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 1 tv, 1 laptop, 211.3ms\n",
      "Speed: 3.0ms preprocess, 211.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.48075866699219, 162.3916015625, 470.19464111328125, 479.41204833984375], [0.22151947021484375, 291.8672790527344, 96.46717834472656, 479.3498229980469], [87.03292846679688, 358.9073486328125, 205.20571899414062, 425.7864990234375], [370.462158203125, 372.571044921875, 451.116943359375, 434.3681640625], [31.415616989135742, 358.3788757324219, 84.72738647460938, 403.1657409667969], [462.35955810546875, 413.33441162109375, 518.9711303710938, 463.592529296875]], 'scores': [0.9463099241256714, 0.9013724327087402, 0.6493899822235107, 0.4425579607486725, 0.3836404085159302, 0.36516764760017395], 'class_indices': [0.0, 0.0, 63.0, 62.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=473742;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=367070;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 209.6ms\n",
      "Speed: 3.1ms preprocess, 209.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[57.56758117675781, 163.04876708984375, 471.02618408203125, 479.447509765625], [0.3112640380859375, 292.0419921875, 96.89232635498047, 478.69671630859375], [86.87164306640625, 358.75146484375, 205.26083374023438, 426.5450439453125], [30.809099197387695, 358.3899230957031, 84.24021911621094, 404.1632385253906], [86.80245971679688, 358.6695556640625, 205.72161865234375, 426.929443359375], [462.353271484375, 413.5308837890625, 519.08984375, 464.0125732421875], [371.7197265625, 373.090576171875, 451.1881103515625, 433.48175048828125], [30.677141189575195, 358.66119384765625, 72.08103942871094, 383.84954833984375]], 'scores': [0.9457622170448303, 0.90926194190979, 0.5939780473709106, 0.5245542526245117, 0.4746171832084656, 0.41799309849739075, 0.3617740273475647, 0.2857280969619751], 'class_indices': [0.0, 0.0, 63.0, 0.0, 62.0, 56.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=365871;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=193385;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 211.8ms\n",
      "Speed: 2.4ms preprocess, 211.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.797943115234375, 163.2021942138672, 470.3934020996094, 479.44732666015625], [0.33367919921875, 291.42266845703125, 96.93086242675781, 479.12274169921875], [87.11854553222656, 358.68109130859375, 205.57176208496094, 426.2779541015625], [462.38873291015625, 413.14398193359375, 518.7538452148438, 463.67913818359375], [32.035316467285156, 358.8560485839844, 85.85623168945312, 404.7522277832031], [32.16295623779297, 359.1871337890625, 73.70744323730469, 386.56011962890625], [370.78717041015625, 372.26531982421875, 451.18365478515625, 434.72100830078125], [201.75363159179688, 432.23760986328125, 268.0448913574219, 479.72900390625]], 'scores': [0.9474983811378479, 0.9234668016433716, 0.6120983958244324, 0.44255656003952026, 0.40871721506118774, 0.3600875735282898, 0.3504292070865631, 0.3090638220310211], 'class_indices': [0.0, 0.0, 62.0, 56.0, 0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=647466;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=446599;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 217.4ms\n",
      "Speed: 2.1ms preprocess, 217.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[57.5714111328125, 163.25274658203125, 469.69952392578125, 479.466552734375], [0.3294219970703125, 291.8515625, 96.39570617675781, 479.2176513671875], [86.92671966552734, 358.7364501953125, 205.42074584960938, 427.3038330078125], [462.24163818359375, 413.34100341796875, 518.8095092773438, 464.27972412109375], [370.3848571777344, 372.63104248046875, 451.2568664550781, 435.95513916015625], [35.16767883300781, 358.93035888671875, 73.90290832519531, 386.76971435546875], [34.63368225097656, 358.6072998046875, 84.6092300415039, 403.270751953125]], 'scores': [0.9477673172950745, 0.9202554821968079, 0.5900490283966064, 0.4933294653892517, 0.48774176836013794, 0.314669668674469, 0.2840518653392792], 'class_indices': [0.0, 0.0, 62.0, 56.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=696104;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=698856;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 211.9ms\n",
      "Speed: 2.8ms preprocess, 211.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.52833557128906, 162.4716033935547, 468.60546875, 479.48980712890625], [0.33010101318359375, 291.85858154296875, 97.13249969482422, 479.32354736328125], [36.62333679199219, 358.21356201171875, 72.3532943725586, 385.73748779296875], [461.89239501953125, 413.0751953125, 519.1007690429688, 463.9810791015625], [86.79886627197266, 358.81268310546875, 205.46292114257812, 426.2718505859375], [370.1641845703125, 372.59124755859375, 451.12762451171875, 436.18927001953125], [200.4722900390625, 433.399658203125, 266.5675048828125, 479.76922607421875], [86.83056640625, 359.1578674316406, 205.39071655273438, 426.0961608886719]], 'scores': [0.9451650381088257, 0.9244115948677063, 0.5324360132217407, 0.4999096691608429, 0.4955070912837982, 0.40660837292671204, 0.38929688930511475, 0.3763321042060852], 'class_indices': [0.0, 0.0, 0.0, 56.0, 62.0, 62.0, 27.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:09] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:09]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=644520;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=16402;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 209.3ms\n",
      "Speed: 4.1ms preprocess, 209.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.18830871582031, 163.4297637939453, 469.15301513671875, 479.46820068359375], [0.33331298828125, 291.5926208496094, 95.40432739257812, 479.1019592285156], [462.0252685546875, 413.3520202636719, 518.785888671875, 464.5346374511719], [87.0633544921875, 358.70654296875, 205.36557006835938, 427.073486328125], [369.9959716796875, 372.5870361328125, 451.02813720703125, 436.44073486328125], [87.08514404296875, 358.92864990234375, 205.35516357421875, 426.70623779296875], [34.634490966796875, 358.7478942871094, 70.96624755859375, 383.8649597167969], [34.424278259277344, 358.181640625, 85.17790985107422, 404.0133056640625], [201.199462890625, 434.0166015625, 266.7207336425781, 479.77166748046875]], 'scores': [0.94791179895401, 0.9230188727378845, 0.5438670516014099, 0.533161997795105, 0.5231662392616272, 0.447068452835083, 0.41998109221458435, 0.3249361217021942, 0.2559487223625183], 'class_indices': [0.0, 0.0, 56.0, 62.0, 62.0, 63.0, 0.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=215425;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=353249;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 209.5ms\n",
      "Speed: 2.4ms preprocess, 209.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.49147033691406, 163.14471435546875, 469.0618896484375, 479.45831298828125], [0.22856903076171875, 291.3930358886719, 96.14939880371094, 479.0923767089844], [461.9964599609375, 413.290771484375, 518.71826171875, 464.23370361328125], [87.05193328857422, 358.8337707519531, 205.51318359375, 426.4873352050781], [370.37347412109375, 372.49951171875, 451.2984619140625, 437.0989990234375], [87.01889038085938, 359.17535400390625, 205.4727783203125, 426.37750244140625], [33.42401885986328, 358.5830078125, 69.75814056396484, 383.70794677734375], [33.19001770019531, 357.9757995605469, 85.16192626953125, 404.3825378417969]], 'scores': [0.9459642171859741, 0.8940882682800293, 0.5115028619766235, 0.5078819394111633, 0.4378453493118286, 0.42199209332466125, 0.364662230014801, 0.35037991404533386], 'class_indices': [0.0, 0.0, 56.0, 62.0, 62.0, 63.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=997633;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=673545;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 212.8ms\n",
      "Speed: 3.1ms preprocess, 212.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.98426818847656, 163.53976440429688, 469.3260498046875, 479.4481506347656], [0.35321044921875, 291.3829345703125, 96.1275863647461, 479.439697265625], [462.364013671875, 413.473388671875, 518.9572143554688, 464.5452880859375], [86.91059112548828, 358.7655944824219, 205.5439453125, 426.5097351074219], [370.1951599121094, 372.58245849609375, 451.1767272949219, 436.24774169921875], [33.10009765625, 359.1949462890625, 69.19923400878906, 384.1851806640625], [32.82460021972656, 358.79632568359375, 85.92012786865234, 404.19842529296875]], 'scores': [0.9456632733345032, 0.9205756783485413, 0.5829617977142334, 0.5655093789100647, 0.5046321153640747, 0.46895772218704224, 0.35273972153663635], 'class_indices': [0.0, 0.0, 56.0, 62.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=904850;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=663231;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 208.1ms\n",
      "Speed: 2.1ms preprocess, 208.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.83247375488281, 163.12594604492188, 468.45465087890625, 479.3059997558594], [0.3225860595703125, 291.278564453125, 97.11503601074219, 479.31640625], [462.02313232421875, 413.33746337890625, 519.1289672851562, 464.31939697265625], [86.84178924560547, 359.05548095703125, 204.99981689453125, 426.14813232421875], [86.89114379882812, 358.87823486328125, 205.41622924804688, 426.5743408203125], [33.214385986328125, 359.2254943847656, 69.21705627441406, 383.3886413574219], [32.8968505859375, 358.5885009765625, 84.52446746826172, 402.98291015625], [370.3904724121094, 372.82489013671875, 451.1212463378906, 436.42132568359375], [201.10122680664062, 432.92266845703125, 266.9347229003906, 479.76165771484375]], 'scores': [0.942718505859375, 0.9272552132606506, 0.5759425759315491, 0.506770670413971, 0.47577208280563354, 0.46170878410339355, 0.40188977122306824, 0.37684503197669983, 0.353386789560318], 'class_indices': [0.0, 0.0, 56.0, 63.0, 62.0, 0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:10] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:10]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=666385;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=418860;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 210.0ms\n",
      "Speed: 2.6ms preprocess, 210.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[57.247833251953125, 163.96212768554688, 469.5566101074219, 479.4576110839844], [0.1501312255859375, 291.3665466308594, 96.59024047851562, 479.5392150878906], [87.01467895507812, 359.14410400390625, 205.41961669921875, 426.67437744140625], [462.2646484375, 413.584228515625, 519.109375, 464.5509033203125], [200.55880737304688, 433.67401123046875, 267.226318359375, 480.0], [86.9918212890625, 358.83648681640625, 205.34423828125, 427.04656982421875], [370.8457946777344, 372.60595703125, 451.1181945800781, 436.68463134765625], [31.934722900390625, 358.67730712890625, 85.98841094970703, 404.52728271484375], [32.147705078125, 359.3360595703125, 69.61631774902344, 384.3570556640625]], 'scores': [0.9456616640090942, 0.8654879927635193, 0.5048869848251343, 0.4957772195339203, 0.4751175045967102, 0.4454798400402069, 0.42628636956214905, 0.4046526551246643, 0.33016055822372437], 'class_indices': [0.0, 0.0, 63.0, 56.0, 27.0, 62.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=35410;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=828723;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 206.0ms\n",
      "Speed: 2.5ms preprocess, 206.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.864654541015625, 163.54959106445312, 469.6766052246094, 479.4560241699219], [0.23030853271484375, 291.2530212402344, 96.34523010253906, 478.5711975097656], [462.16265869140625, 413.34912109375, 518.9418334960938, 464.078369140625], [86.70315551757812, 359.24102783203125, 205.1226806640625, 426.07904052734375], [372.71221923828125, 372.4117431640625, 451.02178955078125, 436.3231201171875], [86.71636962890625, 359.1056823730469, 205.3807373046875, 426.5967102050781], [31.50950813293457, 359.6346435546875, 68.78277587890625, 383.6817626953125], [31.475549697875977, 358.98638916015625, 84.38323211669922, 403.42498779296875]], 'scores': [0.948852002620697, 0.8703688383102417, 0.6518980264663696, 0.522310197353363, 0.4889465868473053, 0.4673882722854614, 0.4579722583293915, 0.33920273184776306], 'class_indices': [0.0, 0.0, 56.0, 63.0, 62.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=48457;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=713565;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 209.0ms\n",
      "Speed: 2.5ms preprocess, 209.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[57.0308837890625, 163.3685302734375, 468.85882568359375, 479.477783203125], [0.20702362060546875, 291.5748291015625, 97.15282440185547, 479.4686279296875], [87.06277465820312, 359.30609130859375, 205.472900390625, 426.55706787109375], [200.23907470703125, 433.4845275878906, 266.59185791015625, 479.7500915527344], [30.937828063964844, 359.05615234375, 85.40005493164062, 403.8790283203125], [461.85064697265625, 413.43609619140625, 519.0963745117188, 464.17169189453125], [30.927146911621094, 359.7272033691406, 67.88032531738281, 382.8786926269531], [372.16278076171875, 372.6127014160156, 451.26068115234375, 436.8417053222656]], 'scores': [0.9481666684150696, 0.8977599740028381, 0.6988812685012817, 0.5652570128440857, 0.4568982422351837, 0.4286726713180542, 0.425381064414978, 0.36206385493278503], 'class_indices': [0.0, 0.0, 62.0, 27.0, 0.0, 56.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=573726;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=653875;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 2 chairs, 2 tvs, 1 laptop, 216.1ms\n",
      "Speed: 2.4ms preprocess, 216.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.23480224609375, 162.93678283691406, 468.7005615234375, 479.3551025390625], [0.17687225341796875, 291.32208251953125, 96.3205337524414, 479.61810302734375], [199.38482666015625, 432.9609680175781, 266.3180847167969, 479.7862854003906], [86.66861724853516, 359.18084716796875, 205.14395141601562, 426.06207275390625], [30.554359436035156, 359.1805419921875, 86.4468994140625, 404.4356689453125], [86.6283950805664, 359.318115234375, 204.92410278320312, 425.612548828125], [462.05322265625, 412.99652099609375, 518.9248657226562, 464.36846923828125], [372.39337158203125, 372.6025390625, 451.2694091796875, 436.4322509765625], [30.804426193237305, 359.7901306152344, 67.21037292480469, 383.6922912597656], [461.72918701171875, 412.80078125, 541.8099975585938, 479.7412109375]], 'scores': [0.9427302479743958, 0.9087421894073486, 0.7385144233703613, 0.5365943908691406, 0.47326958179473877, 0.4144926369190216, 0.3494221270084381, 0.32662245631217957, 0.26887747645378113, 0.2571021020412445], 'class_indices': [0.0, 0.0, 27.0, 62.0, 0.0, 63.0, 56.0, 62.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=315951;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=697241;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 215.9ms\n",
      "Speed: 1.9ms preprocess, 215.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.49015808105469, 164.5728759765625, 469.57781982421875, 479.46673583984375], [0.19519805908203125, 291.9222412109375, 97.5102310180664, 479.3275146484375], [86.90414428710938, 359.6318054199219, 206.4068603515625, 426.8856506347656], [30.540925979614258, 360.16497802734375, 66.67880249023438, 381.83367919921875], [199.474853515625, 433.1436767578125, 266.3815002441406, 479.7689208984375], [462.1319580078125, 413.5824890136719, 518.9032592773438, 464.4392395019531], [373.0162353515625, 372.915283203125, 451.317626953125, 436.12176513671875]], 'scores': [0.9489685893058777, 0.896950364112854, 0.5716533660888672, 0.489128977060318, 0.4497602880001068, 0.4169538617134094, 0.37630927562713623], 'class_indices': [0.0, 0.0, 62.0, 0.0, 27.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:11] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:11]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=194166;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=423363;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 207.3ms\n",
      "Speed: 2.4ms preprocess, 207.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.703369140625, 164.24371337890625, 470.4588623046875, 479.42340087890625], [0.3444671630859375, 291.8914794921875, 95.80914306640625, 479.111083984375], [87.13627624511719, 359.19268798828125, 206.0424041748047, 426.78424072265625], [199.67431640625, 432.6180419921875, 267.190185546875, 479.7425537109375], [29.78844451904297, 360.40496826171875, 66.37284851074219, 382.36016845703125], [372.9894714355469, 372.513671875, 451.1485900878906, 435.449462890625], [462.239013671875, 413.40679931640625, 518.892578125, 464.21600341796875], [29.975982666015625, 359.7181396484375, 85.27642822265625, 404.02197265625]], 'scores': [0.946815550327301, 0.9172409772872925, 0.596177875995636, 0.5863813161849976, 0.5203303098678589, 0.3940119445323944, 0.39216041564941406, 0.30199530720710754], 'class_indices': [0.0, 0.0, 62.0, 27.0, 0.0, 62.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=31028;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=806893;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 215.7ms\n",
      "Speed: 2.5ms preprocess, 215.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.58628845214844, 162.92514038085938, 470.390625, 479.3046569824219], [0.23317718505859375, 292.273193359375, 95.9908218383789, 479.5799560546875], [159.3953857421875, 290.735107421875, 219.8905029296875, 384.9095458984375], [87.29742431640625, 359.50146484375, 194.02117919921875, 426.15802001953125], [29.38379669189453, 360.46710205078125, 65.01473999023438, 382.00823974609375], [200.477294921875, 432.7828369140625, 267.41107177734375, 479.7264404296875], [373.25115966796875, 372.73992919921875, 451.16864013671875, 435.31549072265625], [462.4022216796875, 413.56524658203125, 518.6505737304688, 464.29510498046875]], 'scores': [0.9494282603263855, 0.9197208285331726, 0.8354390859603882, 0.5936629176139832, 0.5640148520469666, 0.5420388579368591, 0.3724769055843353, 0.3437839150428772], 'class_indices': [0.0, 0.0, 0.0, 62.0, 0.0, 27.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=852247;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=591344;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 5 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 208.2ms\n",
      "Speed: 2.1ms preprocess, 208.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.14959716796875, 163.8103485107422, 469.91558837890625, 479.34796142578125], [0.243316650390625, 292.31927490234375, 96.05301666259766, 479.53009033203125], [152.6681365966797, 289.6890869140625, 219.5611114501953, 385.05194091796875], [29.801620483398438, 360.63775634765625, 64.60884094238281, 381.37835693359375], [87.2349853515625, 359.68353271484375, 189.10592651367188, 425.22869873046875], [372.872802734375, 372.60235595703125, 451.21875, 435.61041259765625], [462.03076171875, 413.2950439453125, 518.7559814453125, 463.9580078125], [87.05050659179688, 359.77301025390625, 188.96221923828125, 424.99407958984375], [199.5743408203125, 433.09405517578125, 266.7406311035156, 479.7154541015625], [29.827423095703125, 359.908935546875, 85.44085693359375, 403.529541015625]], 'scores': [0.9495269060134888, 0.9216680526733398, 0.8527741432189941, 0.623225748538971, 0.473599910736084, 0.42677250504493713, 0.4097781479358673, 0.3752880394458771, 0.37149885296821594, 0.2952991724014282], 'class_indices': [0.0, 0.0, 0.0, 0.0, 62.0, 62.0, 56.0, 63.0, 27.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=853389;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=189092;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 5 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 206.6ms\n",
      "Speed: 2.5ms preprocess, 206.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.803466796875, 163.6269989013672, 470.30316162109375, 479.47479248046875], [0.3235931396484375, 292.2159118652344, 95.61223602294922, 478.9407043457031], [150.13427734375, 291.59539794921875, 218.5550537109375, 386.10943603515625], [198.67434692382812, 432.69097900390625, 266.1952209472656, 479.77032470703125], [29.484670639038086, 360.57720947265625, 64.84436798095703, 381.53948974609375], [87.12191772460938, 359.9477844238281, 186.31564331054688, 426.7469177246094], [462.3687744140625, 413.5153503417969, 518.92529296875, 464.5592346191406], [372.65997314453125, 372.82952880859375, 451.14544677734375, 435.81292724609375], [29.677536010742188, 359.93646240234375, 85.90538024902344, 404.20831298828125]], 'scores': [0.9489160776138306, 0.9163514375686646, 0.8549032211303711, 0.6298006176948547, 0.565514326095581, 0.5563480257987976, 0.4928750991821289, 0.3514440953731537, 0.3057505786418915], 'class_indices': [0.0, 0.0, 0.0, 27.0, 0.0, 63.0, 56.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:12] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:12]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=387674;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=197993;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 210.1ms\n",
      "Speed: 2.4ms preprocess, 210.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.9393310546875, 164.22740173339844, 469.99969482421875, 479.45458984375], [0.25156402587890625, 292.1082763671875, 95.97530364990234, 479.529541015625], [152.3538818359375, 290.6388244628906, 219.54364013671875, 385.5818786621094], [29.912778854370117, 360.6072998046875, 64.4537353515625, 381.30059814453125], [462.2545166015625, 413.438232421875, 518.8348999023438, 464.234130859375], [199.9344482421875, 432.9091796875, 266.6143798828125, 479.7318115234375], [372.641845703125, 372.71820068359375, 451.2376708984375, 436.29449462890625], [86.96575164794922, 359.93359375, 186.96380615234375, 425.4881591796875], [87.10446166992188, 359.86004638671875, 187.0625, 425.27911376953125]], 'scores': [0.9505397081375122, 0.9260841012001038, 0.8601866960525513, 0.6866801977157593, 0.5278805494308472, 0.5101538300514221, 0.49252423644065857, 0.4396181106567383, 0.3890547454357147], 'class_indices': [0.0, 0.0, 0.0, 0.0, 56.0, 27.0, 62.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=730927;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=360126;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 5 persons, 1 tie, 1 chair, 2 tvs, 211.4ms\n",
      "Speed: 2.1ms preprocess, 211.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.5802001953125, 163.666015625, 470.1517333984375, 479.47698974609375], [0.257537841796875, 291.5379638671875, 96.54659271240234, 479.5047607421875], [159.65377807617188, 289.965576171875, 219.67236328125, 385.3189697265625], [87.13968658447266, 359.6791687011719, 191.75411987304688, 426.4335021972656], [29.797239303588867, 360.4417724609375, 64.52273559570312, 381.3177490234375], [372.471435546875, 372.6418762207031, 451.0760498046875, 436.3825378417969], [461.8714599609375, 413.50360107421875, 518.7044677734375, 464.37677001953125], [29.682401657104492, 359.8802490234375, 86.40394592285156, 404.973876953125], [199.13748168945312, 432.0094909667969, 266.2479553222656, 479.7488098144531]], 'scores': [0.9491915106773376, 0.9238757491111755, 0.851173996925354, 0.49788621068000793, 0.49149438738822937, 0.4854448735713959, 0.4567060172557831, 0.3713214695453644, 0.31475770473480225], 'class_indices': [0.0, 0.0, 0.0, 62.0, 0.0, 62.0, 56.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=155707;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=77159;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 211.1ms\n",
      "Speed: 2.6ms preprocess, 211.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.12506103515625, 163.65060424804688, 469.34405517578125, 479.3169250488281], [0.18811798095703125, 291.9632568359375, 96.8582763671875, 479.5179443359375], [169.38235473632812, 288.1282958984375, 220.74002075195312, 385.0404052734375], [29.61241340637207, 360.5177001953125, 64.53892517089844, 381.1236572265625], [461.8031005859375, 413.3109436035156, 518.9722900390625, 464.4123840332031], [86.9829330444336, 359.57794189453125, 197.82870483398438, 426.21075439453125], [372.2601318359375, 372.63787841796875, 451.2589111328125, 436.511962890625], [199.46572875976562, 432.6578369140625, 266.77032470703125, 479.7913818359375]], 'scores': [0.9496789574623108, 0.9302318096160889, 0.8565401434898376, 0.6709741950035095, 0.5785630345344543, 0.5602366924285889, 0.4527539908885956, 0.3162127137184143], 'class_indices': [0.0, 0.0, 0.0, 0.0, 56.0, 62.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=885685;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=940573;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 211.3ms\n",
      "Speed: 1.9ms preprocess, 211.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.02117919921875, 163.7889404296875, 469.29022216796875, 479.32635498046875], [0.14447021484375, 291.17901611328125, 96.82244873046875, 479.42596435546875], [184.8951416015625, 290.8837890625, 220.83026123046875, 384.196533203125], [29.58119010925293, 360.40679931640625, 64.69064331054688, 381.22711181640625], [86.95012664794922, 359.4034423828125, 204.33740234375, 426.3380126953125], [462.16033935546875, 413.52081298828125, 518.9325561523438, 464.35992431640625], [372.4874267578125, 372.60943603515625, 451.019287109375, 436.5914306640625], [200.185546875, 432.8352966308594, 266.160888671875, 479.7605285644531]], 'scores': [0.9486342668533325, 0.8591177463531494, 0.7460334897041321, 0.5870723724365234, 0.5425721406936646, 0.47227346897125244, 0.43539178371429443, 0.4066725969314575], 'class_indices': [0.0, 0.0, 0.0, 0.0, 62.0, 56.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=581698;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=752295;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 tvs, 211.7ms\n",
      "Speed: 2.4ms preprocess, 211.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.1971435546875, 164.0635223388672, 469.91375732421875, 479.45452880859375], [0.237091064453125, 292.46746826171875, 96.63079833984375, 479.48858642578125], [29.882232666015625, 360.6210632324219, 64.74092864990234, 381.8355407714844], [86.80586242675781, 359.543212890625, 207.0106964111328, 426.27001953125], [462.04864501953125, 413.38116455078125, 519.1334838867188, 464.47344970703125], [372.6748962402344, 372.61785888671875, 451.2688293457031, 436.56036376953125]], 'scores': [0.9488073587417603, 0.9200731515884399, 0.5875672698020935, 0.5519749522209167, 0.44302213191986084, 0.3918362855911255], 'class_indices': [0.0, 0.0, 0.0, 62.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:13] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:13]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=721645;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=923156;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 215.2ms\n",
      "Speed: 3.2ms preprocess, 215.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.025634765625, 163.337158203125, 469.56646728515625, 479.470947265625], [0.1876220703125, 292.37060546875, 96.65798950195312, 479.81671142578125], [87.14291381835938, 358.83984375, 205.372314453125, 426.28765869140625], [462.44390869140625, 413.4967041015625, 518.8363647460938, 464.1019287109375], [29.52552604675293, 360.50775146484375, 64.91592407226562, 381.69921875], [372.9405517578125, 372.4409484863281, 451.0648193359375, 436.0419006347656], [29.29065704345703, 359.70458984375, 84.35235595703125, 402.80316162109375]], 'scores': [0.9457917213439941, 0.9235097765922546, 0.6506573557853699, 0.5742413401603699, 0.5595377683639526, 0.5051329731941223, 0.28618210554122925], 'class_indices': [0.0, 0.0, 62.0, 56.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=991970;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=46890;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 tvs, 1 laptop, 210.7ms\n",
      "Speed: 2.9ms preprocess, 210.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.69465637207031, 163.4651641845703, 469.87420654296875, 479.51165771484375], [0.35073089599609375, 292.13079833984375, 96.85334014892578, 478.50689697265625], [30.195314407348633, 360.432373046875, 64.47857666015625, 380.89337158203125], [461.8692626953125, 413.60882568359375, 519.1141967773438, 464.49505615234375], [373.2315673828125, 372.7637939453125, 451.2593994140625, 437.3636474609375], [87.03501892089844, 358.93829345703125, 205.36985778808594, 425.67633056640625], [87.0138931274414, 358.72869873046875, 205.37319946289062, 426.03533935546875]], 'scores': [0.94769686460495, 0.8180186748504639, 0.7278454899787903, 0.552512526512146, 0.5225726366043091, 0.475592702627182, 0.4459395110607147], 'class_indices': [0.0, 0.0, 0.0, 56.0, 62.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=306240;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=131204;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 1 laptop, 213.3ms\n",
      "Speed: 2.1ms preprocess, 213.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.01611328125, 163.2098388671875, 469.46990966796875, 479.493896484375], [0.35186004638671875, 292.504638671875, 96.8877182006836, 479.1134033203125], [29.560522079467773, 360.11907958984375, 64.36968231201172, 381.19573974609375], [86.86737060546875, 359.02447509765625, 205.38229370117188, 425.54742431640625], [372.82818603515625, 372.41693115234375, 451.22650146484375, 436.89300537109375], [462.04876708984375, 413.38629150390625, 518.568115234375, 464.10870361328125]], 'scores': [0.9508794546127319, 0.9196717143058777, 0.6886997222900391, 0.583342432975769, 0.46510985493659973, 0.4069637060165405], 'class_indices': [0.0, 0.0, 0.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=987246;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=743696;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 tvs, 1 laptop, 212.7ms\n",
      "Speed: 2.2ms preprocess, 212.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.80792236328125, 163.8638916015625, 468.85791015625, 479.3360595703125], [0.23595428466796875, 292.30767822265625, 96.63098907470703, 479.48089599609375], [29.95146942138672, 360.40643310546875, 64.59101104736328, 381.92572021484375], [462.0860595703125, 413.43072509765625, 518.6148071289062, 464.08734130859375], [373.0865478515625, 372.61016845703125, 451.1009521484375, 436.87322998046875], [87.15076446533203, 359.11175537109375, 205.05682373046875, 426.207763671875], [87.02658081054688, 358.8924560546875, 205.42376708984375, 426.5191650390625]], 'scores': [0.9502596259117126, 0.9201422333717346, 0.700476348400116, 0.5717194080352783, 0.46306318044662476, 0.4598039388656616, 0.43898412585258484], 'class_indices': [0.0, 0.0, 0.0, 56.0, 62.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:14] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:14]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=522337;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=448733;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 207.6ms\n",
      "Speed: 2.5ms preprocess, 207.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.49322509765625, 163.92724609375, 468.38482666015625, 479.48699951171875], [0.2357025146484375, 292.84368896484375, 96.85799407958984, 479.21795654296875], [28.993186950683594, 360.4974060058594, 63.78412628173828, 381.2401428222656], [461.9671630859375, 413.44097900390625, 518.739501953125, 464.06402587890625], [372.8607177734375, 372.73675537109375, 451.1395263671875, 437.61822509765625], [87.04557800292969, 358.87786865234375, 205.63670349121094, 425.63311767578125], [200.89541625976562, 432.92041015625, 266.8636169433594, 479.7415771484375]], 'scores': [0.9527267813682556, 0.9041635394096375, 0.7136709690093994, 0.5741187334060669, 0.562200665473938, 0.5141192078590393, 0.4194554090499878], 'class_indices': [0.0, 0.0, 0.0, 56.0, 62.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=590976;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=45930;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 212.0ms\n",
      "Speed: 2.3ms preprocess, 212.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.59782409667969, 163.81141662597656, 468.69866943359375, 479.458984375], [0.30084991455078125, 292.591796875, 96.72688293457031, 479.147216796875], [86.94121551513672, 359.20196533203125, 205.3896484375, 425.73760986328125], [199.16943359375, 433.8723449707031, 266.05828857421875, 479.7590637207031], [29.049711227416992, 360.59320068359375, 64.4019775390625, 382.232177734375], [372.52984619140625, 372.58905029296875, 451.13201904296875, 438.23480224609375], [462.16339111328125, 413.2982177734375, 518.9490356445312, 464.52069091796875], [28.906692504882812, 359.8932800292969, 85.1591796875, 404.5397644042969]], 'scores': [0.9503068923950195, 0.918800413608551, 0.6133270263671875, 0.5728674530982971, 0.5607887506484985, 0.5031663775444031, 0.42628800868988037, 0.252298504114151], 'class_indices': [0.0, 0.0, 62.0, 27.0, 0.0, 62.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=828970;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=797549;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 207.9ms\n",
      "Speed: 3.5ms preprocess, 207.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.82073974609375, 162.8797607421875, 468.329833984375, 479.316650390625], [0.1660003662109375, 292.6031494140625, 96.30315399169922, 479.3970947265625], [29.22547721862793, 360.40545654296875, 64.49491882324219, 381.76959228515625], [86.93463134765625, 358.937744140625, 205.32278442382812, 426.2257080078125], [372.735107421875, 372.4853820800781, 451.2254638671875, 436.7859191894531], [461.88079833984375, 413.50054931640625, 518.9028930664062, 464.1646728515625], [200.06832885742188, 434.056396484375, 266.50762939453125, 479.76336669921875]], 'scores': [0.9498674273490906, 0.9051649570465088, 0.6672415733337402, 0.5797373652458191, 0.5270431637763977, 0.5258175134658813, 0.37685108184814453], 'class_indices': [0.0, 0.0, 0.0, 62.0, 62.0, 56.0, 27.0]}]\n",
      "Video frames: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=373743;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=795091;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 1 tie, 2 chairs, 1 tv, 1 laptop, 212.1ms\n",
      "Speed: 2.7ms preprocess, 212.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.45994567871094, 163.67208862304688, 469.1524658203125, 479.4537658691406], [0.17200469970703125, 292.135009765625, 96.34559631347656, 478.48553466796875], [29.502601623535156, 360.5100402832031, 63.83726501464844, 381.6943054199219], [86.86445617675781, 359.11187744140625, 205.4702911376953, 425.09613037109375], [372.8175048828125, 372.58489990234375, 451.110595703125, 437.07818603515625], [462.64447021484375, 413.24639892578125, 518.9751586914062, 464.508056640625], [198.9757080078125, 434.1542663574219, 265.590087890625, 479.7576599121094], [461.6168212890625, 412.8956604003906, 540.6015625, 479.6867980957031]], 'scores': [0.9479343891143799, 0.8676460981369019, 0.6884909272193909, 0.5709400177001953, 0.4959160387516022, 0.40467309951782227, 0.2936645448207855, 0.2672044038772583], 'class_indices': [0.0, 0.0, 0.0, 63.0, 62.0, 56.0, 27.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:15] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:15]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=412265;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=695058;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 1 tv, 1 laptop, 213.0ms\n",
      "Speed: 5.3ms preprocess, 213.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.74432373046875, 163.38294982910156, 468.868408203125, 479.30224609375], [0.15239715576171875, 292.566162109375, 96.57323455810547, 458.228759765625], [28.976783752441406, 360.4638671875, 63.59521484375, 381.0572509765625], [87.119384765625, 359.0885009765625, 205.14797973632812, 425.7548828125], [373.1068115234375, 372.620361328125, 451.207763671875, 437.0079345703125], [0.2760772705078125, 447.28363037109375, 78.84210205078125, 479.7314453125], [200.01690673828125, 433.09259033203125, 267.09454345703125, 479.72540283203125], [462.18914794921875, 413.45367431640625, 518.7155151367188, 464.31072998046875]], 'scores': [0.9474978446960449, 0.7329825758934021, 0.711522102355957, 0.6162941455841064, 0.3829030692577362, 0.3572733402252197, 0.30775079131126404, 0.27889713644981384], 'class_indices': [0.0, 0.0, 0.0, 63.0, 62.0, 56.0, 27.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=296776;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=408457;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 217.8ms\n",
      "Speed: 2.5ms preprocess, 217.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.81231689453125, 162.640869140625, 468.3509521484375, 479.3280029296875], [0.2369232177734375, 292.5440673828125, 95.68180847167969, 479.17864990234375], [28.77433204650879, 360.2952575683594, 63.24309539794922, 381.0624694824219], [86.96553039550781, 359.3848876953125, 205.5045623779297, 426.2286376953125], [372.3433837890625, 372.3402099609375, 451.1258544921875, 437.385986328125], [462.47607421875, 413.1221923828125, 519.0679931640625, 464.1160888671875], [28.58357048034668, 359.50714111328125, 84.9134521484375, 403.34234619140625]], 'scores': [0.9478329420089722, 0.9026750326156616, 0.6907346844673157, 0.5643165111541748, 0.5027730464935303, 0.44743412733078003, 0.2578217387199402], 'class_indices': [0.0, 0.0, 0.0, 62.0, 62.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=439716;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=195524;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 216.7ms\n",
      "Speed: 2.9ms preprocess, 216.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.50502014160156, 161.5716552734375, 468.542236328125, 479.4749755859375], [0.27291107177734375, 292.7177734375, 96.60104370117188, 479.579345703125], [29.31920623779297, 360.5887451171875, 63.72187805175781, 380.7716064453125], [370.91473388671875, 372.8664245605469, 451.19622802734375, 437.8028259277344], [86.86564636230469, 359.5501708984375, 206.3347625732422, 426.6038818359375], [461.7532958984375, 413.42877197265625, 518.7421875, 464.60247802734375], [86.75252532958984, 359.701904296875, 205.79214477539062, 426.09820556640625], [28.836807250976562, 359.7878723144531, 86.09722137451172, 405.1303405761719]], 'scores': [0.9497966766357422, 0.9211412668228149, 0.6664035320281982, 0.6011225581169128, 0.5886566042900085, 0.44598662853240967, 0.3695903420448303, 0.2968120872974396], 'class_indices': [0.0, 0.0, 0.0, 62.0, 62.0, 56.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=60871;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=234771;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 209.7ms\n",
      "Speed: 2.1ms preprocess, 209.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.493896484375, 158.87857055664062, 468.29449462890625, 479.4579772949219], [0.20685577392578125, 292.00689697265625, 96.9401626586914, 479.281982421875], [29.196701049804688, 360.3897399902344, 64.14139556884766, 382.0055236816406], [462.33837890625, 413.15130615234375, 518.9461669921875, 464.62615966796875], [369.6373291015625, 372.58905029296875, 451.3331298828125, 438.39398193359375], [87.15884399414062, 358.92742919921875, 205.85601806640625, 426.51849365234375], [201.08746337890625, 433.3778381347656, 266.87286376953125, 479.7519836425781]], 'scores': [0.9526459574699402, 0.9051215052604675, 0.6828850507736206, 0.5070178508758545, 0.5048900842666626, 0.5037407279014587, 0.421673446893692], 'class_indices': [0.0, 0.0, 0.0, 56.0, 62.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=290333;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=66133;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 tvs, 208.2ms\n",
      "Speed: 2.1ms preprocess, 208.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.77742004394531, 157.7401123046875, 468.12152099609375, 479.43853759765625], [0.33404541015625, 291.91168212890625, 96.51683044433594, 479.26885986328125], [29.171531677246094, 360.46356201171875, 64.00946044921875, 381.96026611328125], [87.03213500976562, 358.6366882324219, 204.86798095703125, 426.6054992675781], [369.41204833984375, 372.8995056152344, 450.7353515625, 438.4299621582031], [462.1773681640625, 413.1625671386719, 518.993896484375, 464.3423156738281]], 'scores': [0.9502488970756531, 0.9229422211647034, 0.706588625907898, 0.5194427967071533, 0.4896619915962219, 0.3327498435974121], 'class_indices': [0.0, 0.0, 0.0, 62.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:16] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:16]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=482766;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=638535;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 213.7ms\n",
      "Speed: 2.5ms preprocess, 213.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.84246826171875, 157.56590270996094, 468.2081298828125, 479.479736328125], [0.354522705078125, 291.97271728515625, 96.90792083740234, 479.48724365234375], [29.14659309387207, 360.3346862792969, 63.53076171875, 381.7636413574219], [201.36294555664062, 435.029296875, 267.8636779785156, 479.9661865234375], [369.72991943359375, 372.5679931640625, 451.24176025390625, 438.7012939453125], [462.02825927734375, 413.289306640625, 519.2206420898438, 464.461669921875], [87.16035461425781, 359.040771484375, 201.41734313964844, 424.9488525390625]], 'scores': [0.950639545917511, 0.9119306206703186, 0.7180716395378113, 0.5291495323181152, 0.5183857083320618, 0.5162558555603027, 0.4790283739566803], 'class_indices': [0.0, 0.0, 0.0, 27.0, 62.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=249179;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=85332;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 205.2ms\n",
      "Speed: 2.4ms preprocess, 205.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.76177978515625, 157.19064331054688, 467.9434814453125, 479.4706115722656], [0.2686309814453125, 292.12646484375, 96.22775268554688, 479.32574462890625], [28.653146743774414, 360.557861328125, 63.26580047607422, 381.6982421875], [461.70745849609375, 413.5607604980469, 518.9253540039062, 464.4104919433594], [369.0152587890625, 372.60491943359375, 451.2515869140625, 438.035888671875], [200.89508056640625, 435.64581298828125, 267.27520751953125, 479.75347900390625], [87.31748962402344, 358.95111083984375, 200.1710968017578, 426.5745849609375]], 'scores': [0.9495394229888916, 0.9099541306495667, 0.7294288873672485, 0.5982605814933777, 0.5817289352416992, 0.4744628071784973, 0.4298385977745056], 'class_indices': [0.0, 0.0, 0.0, 56.0, 62.0, 27.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=274236;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=272811;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 211.5ms\n",
      "Speed: 4.8ms preprocess, 211.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.76673889160156, 156.76031494140625, 468.3760986328125, 479.48681640625], [0.23677825927734375, 291.95306396484375, 96.62159729003906, 479.51824951171875], [29.125032424926758, 360.4338684082031, 63.85807800292969, 381.4168395996094], [369.325439453125, 372.60009765625, 450.97564697265625, 438.72607421875], [87.1799087524414, 358.94805908203125, 201.61520385742188, 426.09429931640625], [462.0826416015625, 413.17657470703125, 519.3262329101562, 464.13543701171875], [201.38763427734375, 434.6190490722656, 267.19683837890625, 479.7422790527344]], 'scores': [0.9508959054946899, 0.92125004529953, 0.6511050462722778, 0.5463526844978333, 0.5293809771537781, 0.4974537193775177, 0.2767500877380371], 'class_indices': [0.0, 0.0, 0.0, 62.0, 62.0, 56.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=909119;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=51685;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 213.5ms\n",
      "Speed: 1.9ms preprocess, 213.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.297271728515625, 156.832275390625, 468.5413513183594, 479.44873046875], [0.271392822265625, 292.3454895019531, 96.14195251464844, 479.4903869628906], [28.8002986907959, 360.39404296875, 63.647239685058594, 380.4617919921875], [369.7342529296875, 372.5177001953125, 451.1309814453125, 437.89404296875], [461.7066650390625, 413.3526611328125, 519.0714111328125, 464.37884521484375], [201.86068725585938, 434.40301513671875, 267.85687255859375, 480.0], [86.94559478759766, 358.68585205078125, 196.31649780273438, 426.599853515625]], 'scores': [0.9533007144927979, 0.909540057182312, 0.6819191575050354, 0.5197786688804626, 0.4413518011569977, 0.32322633266448975, 0.3195616602897644], 'class_indices': [0.0, 0.0, 0.0, 62.0, 56.0, 27.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:17] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=313994;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=984579;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 218.8ms\n",
      "Speed: 2.4ms preprocess, 218.8ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.294036865234375, 157.12063598632812, 468.3484802246094, 479.4775695800781], [0.2642669677734375, 291.85882568359375, 96.60551452636719, 479.49005126953125], [86.7323226928711, 358.49822998046875, 196.83535766601562, 426.49835205078125], [369.5794677734375, 372.58203125, 451.43756103515625, 438.51171875], [28.96478271484375, 360.3743896484375, 63.971473693847656, 380.745361328125], [202.077880859375, 435.7823791503906, 268.470947265625, 479.8974914550781], [461.95648193359375, 413.26580810546875, 518.6630249023438, 464.38641357421875]], 'scores': [0.9533905386924744, 0.9169402718544006, 0.5796560049057007, 0.5487882494926453, 0.5283840298652649, 0.5131199955940247, 0.44881975650787354], 'class_indices': [0.0, 0.0, 62.0, 62.0, 0.0, 27.0, 56.0]}]\n",
      "Video frames: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=374059;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=795517;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 216.6ms\n",
      "Speed: 2.5ms preprocess, 216.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.95936584472656, 157.61607360839844, 468.202392578125, 479.4180908203125], [0.23607635498046875, 291.8803405761719, 96.04138946533203, 479.5872497558594], [29.148075103759766, 360.59124755859375, 63.89925003051758, 380.87799072265625], [369.5856018066406, 372.7628173828125, 451.0805358886719, 438.1993408203125], [87.25625610351562, 358.95709228515625, 197.84573364257812, 426.30462646484375], [201.9384765625, 436.4735107421875, 267.861328125, 479.7398681640625], [462.26690673828125, 413.4683532714844, 518.8994750976562, 464.5790710449219]], 'scores': [0.9507257342338562, 0.9143542051315308, 0.6474255919456482, 0.5401569604873657, 0.47895103693008423, 0.35070300102233887, 0.27053216099739075], 'class_indices': [0.0, 0.0, 0.0, 62.0, 62.0, 27.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=94337;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=764113;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 2 chairs, 2 tvs, 1 laptop, 212.9ms\n",
      "Speed: 2.3ms preprocess, 212.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.81201171875, 157.7086181640625, 468.0169677734375, 479.4364013671875], [31.77251434326172, 357.91265869140625, 65.74850463867188, 381.36529541015625], [0.26750946044921875, 291.31793212890625, 96.91561126708984, 477.23187255859375], [86.75250244140625, 359.2987976074219, 196.9022216796875, 425.9089660644531], [369.5473937988281, 373.0042724609375, 451.1980285644531, 439.76904296875], [201.25302124023438, 434.88671875, 267.17498779296875, 479.8768310546875], [0.34466552734375, 447.143310546875, 79.85115051269531, 479.7257080078125], [86.97115325927734, 358.895263671875, 198.99642944335938, 426.23980712890625], [461.7601318359375, 413.39556884765625, 518.9527587890625, 464.61895751953125]], 'scores': [0.951614260673523, 0.7414283752441406, 0.6992573142051697, 0.5632812976837158, 0.490892231464386, 0.4138655364513397, 0.37782806158065796, 0.3564245402812958, 0.3054998219013214], 'class_indices': [0.0, 0.0, 0.0, 63.0, 62.0, 27.0, 56.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=542280;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=326762;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 214.5ms\n",
      "Speed: 2.3ms preprocess, 214.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.66796875, 157.792724609375, 467.9239501953125, 479.4483642578125], [0.3199615478515625, 292.14813232421875, 96.94557189941406, 479.16375732421875], [32.028106689453125, 357.958251953125, 66.94154357910156, 382.50946044921875], [369.4396667480469, 372.43072509765625, 451.1507873535156, 439.59893798828125], [200.909912109375, 435.102783203125, 266.7161865234375, 479.7591552734375], [87.269287109375, 359.11297607421875, 194.74649047851562, 426.805419921875], [87.05050659179688, 359.14227294921875, 192.70684814453125, 426.50396728515625], [462.07440185546875, 413.343017578125, 519.0205078125, 464.4793701171875]], 'scores': [0.9505879878997803, 0.9084700345993042, 0.7253590226173401, 0.5235171914100647, 0.4581204652786255, 0.4565301835536957, 0.41587314009666443, 0.3874675929546356], 'class_indices': [0.0, 0.0, 0.0, 62.0, 27.0, 62.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=953647;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=168145;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 212.0ms\n",
      "Speed: 2.6ms preprocess, 212.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.36846923828125, 158.83447265625, 468.030517578125, 479.43414306640625], [0.33792877197265625, 291.74627685546875, 96.74089813232422, 479.35186767578125], [31.63152503967285, 358.31011962890625, 65.34107208251953, 382.1724853515625], [86.82550048828125, 359.04736328125, 191.57769775390625, 425.655029296875], [370.739990234375, 372.384521484375, 451.3568115234375, 440.600830078125], [201.45382690429688, 435.92742919921875, 267.8096008300781, 479.72784423828125], [462.091796875, 413.2667236328125, 518.96630859375, 464.47119140625], [87.07269287109375, 358.7352600097656, 192.85833740234375, 426.1109924316406]], 'scores': [0.9501623511314392, 0.9102293252944946, 0.6441856622695923, 0.5764504671096802, 0.5042566061019897, 0.4660457372665405, 0.39747971296310425, 0.3822312653064728], 'class_indices': [0.0, 0.0, 0.0, 63.0, 62.0, 27.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:18] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:18]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=700167;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=293343;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 chair, 2 tvs, 1 laptop, 214.0ms\n",
      "Speed: 2.2ms preprocess, 214.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.44236755371094, 159.766845703125, 467.808349609375, 479.4393310546875], [0.34050750732421875, 291.84075927734375, 95.94032287597656, 479.30096435546875], [30.467628479003906, 358.831787109375, 64.85237884521484, 382.35546875], [371.43212890625, 372.99774169921875, 450.94854736328125, 440.13482666015625], [86.96048736572266, 359.22216796875, 192.85842895507812, 426.19891357421875], [462.31951904296875, 413.42071533203125, 519.1394653320312, 464.55621337890625], [87.17693328857422, 359.0655517578125, 194.110595703125, 426.61669921875]], 'scores': [0.9507706761360168, 0.9143500924110413, 0.6620290875434875, 0.5285590887069702, 0.4984009265899658, 0.4592882990837097, 0.44739052653312683], 'class_indices': [0.0, 0.0, 0.0, 62.0, 63.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=933744;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=756557;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 tv, 1 laptop, 204.6ms\n",
      "Speed: 2.8ms preprocess, 204.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.92388916015625, 160.6912384033203, 467.2838134765625, 479.40863037109375], [0.18244171142578125, 292.74664306640625, 95.87342071533203, 479.10137939453125], [30.380033493041992, 359.34722900390625, 65.17169189453125, 381.93756103515625], [198.28097534179688, 434.5491943359375, 265.8070373535156, 479.7822265625], [86.81320190429688, 359.23333740234375, 191.39407348632812, 425.47174072265625], [371.9245910644531, 372.826904296875, 451.1327209472656, 440.9697265625], [0.0847015380859375, 292.19061279296875, 38.597801208496094, 372.10980224609375]], 'scores': [0.9521645903587341, 0.81108158826828, 0.7028895020484924, 0.7027769088745117, 0.5389196872711182, 0.5046864748001099, 0.25242605805397034], 'class_indices': [0.0, 0.0, 0.0, 27.0, 63.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=468474;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=200655;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 211.7ms\n",
      "Speed: 3.0ms preprocess, 211.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.06256103515625, 162.19366455078125, 468.2386474609375, 479.4427490234375], [0.196685791015625, 291.9627685546875, 96.84912872314453, 479.6519775390625], [30.178434371948242, 359.35113525390625, 64.38667297363281, 381.15093994140625], [462.1168212890625, 413.36785888671875, 518.39013671875, 463.89288330078125], [87.20698547363281, 359.0240783691406, 187.6864776611328, 425.0802307128906], [198.841552734375, 434.45941162109375, 265.61676025390625, 479.75543212890625], [372.98309326171875, 372.717529296875, 451.09478759765625, 439.351318359375]], 'scores': [0.9541180729866028, 0.9107588529586792, 0.7462150454521179, 0.6759971380233765, 0.5998143553733826, 0.5815677046775818, 0.4964817464351654], 'class_indices': [0.0, 0.0, 0.0, 56.0, 62.0, 27.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=51382;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=434499;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 210.4ms\n",
      "Speed: 2.1ms preprocess, 210.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.01832580566406, 162.92068481445312, 468.22705078125, 479.4164733886719], [0.216278076171875, 291.672607421875, 96.54206848144531, 479.31787109375], [29.964523315429688, 359.5183410644531, 64.65071868896484, 381.9432678222656], [198.24093627929688, 434.66448974609375, 265.55816650390625, 479.75970458984375], [86.81670379638672, 359.5587463378906, 182.8592529296875, 425.4402160644531], [373.0877380371094, 372.89068603515625, 451.0179138183594, 439.56451416015625], [461.73553466796875, 413.31976318359375, 518.7359008789062, 464.45623779296875]], 'scores': [0.9571179747581482, 0.9079608917236328, 0.7239329814910889, 0.6913533210754395, 0.46558722853660583, 0.44908779859542847, 0.2886417508125305], 'class_indices': [0.0, 0.0, 0.0, 27.0, 62.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[27/06/2024 15:15:19] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[27/06/2024 15:15:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=506979;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=526708;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 210.1ms\n",
      "Speed: 2.3ms preprocess, 210.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.10040283203125, 164.5121307373047, 468.10247802734375, 479.46282958984375], [0.20046234130859375, 292.03424072265625, 96.50173950195312, 479.7044677734375], [197.0484619140625, 435.21044921875, 264.19219970703125, 479.73681640625], [30.003427505493164, 359.527099609375, 64.35074615478516, 380.85296630859375], [86.89617919921875, 359.90313720703125, 172.97781372070312, 424.8079833984375], [461.911865234375, 413.21527099609375, 518.7579345703125, 464.28912353515625], [373.14068603515625, 372.75994873046875, 451.34674072265625, 438.6396484375], [162.63198852539062, 356.0662536621094, 212.71737670898438, 384.2033996582031]], 'scores': [0.9570444226264954, 0.9133488535881042, 0.7946283221244812, 0.6765456199645996, 0.46534666419029236, 0.4576301574707031, 0.4406800866127014, 0.3700979948043823], 'class_indices': [0.0, 0.0, 27.0, 0.0, 63.0, 56.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=90699;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=666866;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 209.9ms\n",
      "Speed: 2.5ms preprocess, 209.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.57151794433594, 163.60362243652344, 468.5872802734375, 479.4151611328125], [0.28943634033203125, 291.531005859375, 95.95317077636719, 477.829345703125], [29.30821990966797, 359.71563720703125, 64.21698760986328, 381.68048095703125], [196.1923828125, 434.4682922363281, 264.37249755859375, 479.7651062011719], [86.88937377929688, 359.4427795410156, 166.92681884765625, 425.0654602050781], [155.9171905517578, 353.5992431640625, 210.79762268066406, 384.4798583984375], [373.44281005859375, 372.5305480957031, 451.28277587890625, 438.9996643066406], [461.8702392578125, 413.4006042480469, 518.86572265625, 464.3374328613281]], 'scores': [0.9560243487358093, 0.7611561417579651, 0.751218855381012, 0.5968824625015259, 0.5829893350601196, 0.524429202079773, 0.48935645818710327, 0.4704016149044037], 'class_indices': [0.0, 0.0, 0.0, 27.0, 63.0, 0.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Used `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">render_boxes</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` sink, but predictions that were provided do    <a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sinks.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         not match the expected format of object detection prediction that could <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         be accepted by `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">supervision.Detection.from_inference</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Used `\u001b[1;35mrender_boxes\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m` sink, but predictions that were provided do    \u001b]8;id=61910;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py\u001b\\\u001b[2msinks.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=936528;file:///home/utilisateur/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/sinks.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                      \u001b[0m         not match the expected format of object detection prediction that could \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                      \u001b[0m         be accepted by `\u001b[1;35msupervision.Detection.from_inference\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 71\u001b[0m\n\u001b[1;32m     64\u001b[0m my_model \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../shifumi_trained.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m InferencePipeline\u001b[38;5;241m.\u001b[39minit_with_custom_logic(\n\u001b[1;32m     66\u001b[0m     on_video_frame\u001b[38;5;241m=\u001b[39mmy_model\u001b[38;5;241m.\u001b[39minfer,\n\u001b[1;32m     67\u001b[0m     video_reference\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,  \u001b[38;5;66;03m# Ensure this is the correct device ID for your webcam\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     on_prediction\u001b[38;5;241m=\u001b[39mrender_boxes,  \u001b[38;5;66;03m# Function to run after each prediction\u001b[39;00m\n\u001b[1;32m     69\u001b[0m )\n\u001b[0;32m---> 71\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/inference_pipeline.py:747\u001b[0m, in \u001b[0;36mInferencePipeline.start\u001b[0;34m(self, use_main_thread)\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_pipeline_start()\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_main_thread:\n\u001b[0;32m--> 747\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_inference_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatching_thread \u001b[38;5;241m=\u001b[39m Thread(target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_inference_results)\n",
      "File \u001b[0;32m~/Documents/Projets/ShiFuMi_computerVision/venv/lib/python3.10/site-packages/inference/core/interfaces/stream/inference_pipeline.py:838\u001b[0m, in \u001b[0;36mInferencePipeline._dispatch_inference_results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dispatch_inference_results\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m         inference_results: Optional[\n\u001b[1;32m    837\u001b[0m             Tuple[List[AnyPrediction], List[VideoFrame]]\n\u001b[0;32m--> 838\u001b[0m         ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictions_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inference_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictions_queue\u001b[38;5;241m.\u001b[39mtask_done()\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 219.2ms\n",
      "Speed: 1.6ms preprocess, 219.2ms inference, 266.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.67021179199219, 163.77523803710938, 467.99493408203125, 479.4228210449219], [0.34021759033203125, 292.2184143066406, 95.75505065917969, 479.4447326660156], [29.963178634643555, 359.6097412109375, 64.23823547363281, 381.620849609375], [197.688720703125, 434.5080261230469, 264.69256591796875, 479.7569885253906], [151.6286163330078, 354.4468994140625, 213.4404754638672, 384.93951416015625], [373.32916259765625, 372.5877380371094, 451.42608642578125, 439.5577087402344], [462.0816650390625, 413.37701416015625, 519.1405029296875, 464.7132568359375], [87.00738525390625, 359.662109375, 164.22940063476562, 425.318359375]], 'scores': [0.9543023705482483, 0.9147205948829651, 0.7893399596214294, 0.7428930401802063, 0.5185942649841309, 0.5014598369598389, 0.454854279756546, 0.4143734276294708], 'class_indices': [0.0, 0.0, 0.0, 27.0, 0.0, 62.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 2 chairs, 1 tv, 1 laptop, 210.3ms\n",
      "Speed: 2.4ms preprocess, 210.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.91261291503906, 163.37208557128906, 467.755615234375, 479.42138671875], [29.849340438842773, 359.5361022949219, 64.23841094970703, 381.4522399902344], [0.26848602294921875, 291.5819091796875, 96.22425842285156, 478.4788818359375], [198.70480346679688, 436.6563720703125, 263.6928405761719, 479.780029296875], [373.14501953125, 372.6368103027344, 450.9830322265625, 439.0635681152344], [462.01751708984375, 413.4605712890625, 518.8311157226562, 464.1363525390625], [86.63664245605469, 359.6837158203125, 162.86459350585938, 425.2877197265625], [149.56033325195312, 353.4714660644531, 214.51083374023438, 385.6462707519531], [0.316375732421875, 446.71490478515625, 77.61520385742188, 479.72857666015625]], 'scores': [0.9571893215179443, 0.7540670037269592, 0.7434493899345398, 0.5855918526649475, 0.5333777666091919, 0.5074619650840759, 0.4824845790863037, 0.442972868680954, 0.2579915523529053], 'class_indices': [0.0, 0.0, 0.0, 27.0, 62.0, 56.0, 63.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 197.4ms\n",
      "Speed: 2.2ms preprocess, 197.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.7427978515625, 164.06797790527344, 467.744140625, 479.4405517578125], [0.1777191162109375, 292.51580810546875, 97.07257843017578, 479.6513671875], [198.70138549804688, 435.12811279296875, 265.4448547363281, 479.7606201171875], [30.81806755065918, 359.41290283203125, 64.87158203125, 381.9244384765625], [150.66651916503906, 354.1217041015625, 214.57154846191406, 385.553955078125], [462.11016845703125, 413.61370849609375, 519.1195678710938, 464.28814697265625], [373.24249267578125, 373.0235595703125, 451.15338134765625, 439.826904296875], [87.1805419921875, 359.8409729003906, 163.78726196289062, 424.8658142089844]], 'scores': [0.9576839208602905, 0.889204740524292, 0.7908204197883606, 0.6949138641357422, 0.5352460741996765, 0.5194582343101501, 0.5059850215911865, 0.4402673840522766], 'class_indices': [0.0, 0.0, 27.0, 0.0, 0.0, 56.0, 62.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 197.1ms\n",
      "Speed: 2.1ms preprocess, 197.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.59617614746094, 163.7496337890625, 467.767333984375, 479.4132080078125], [0.30680084228515625, 292.01263427734375, 95.91172790527344, 479.1666259765625], [30.588333129882812, 358.6834716796875, 65.55400085449219, 382.05035400390625], [198.56033325195312, 435.61883544921875, 264.90325927734375, 479.7467041015625], [153.247802734375, 354.93060302734375, 212.173828125, 385.05584716796875], [373.34222412109375, 372.92718505859375, 451.38140869140625, 440.44915771484375], [461.77191162109375, 413.40899658203125, 518.6696166992188, 464.61456298828125], [87.1265869140625, 359.45684814453125, 165.15420532226562, 425.36883544921875], [87.19868469238281, 359.58355712890625, 165.18487548828125, 425.96673583984375]], 'scores': [0.9557614922523499, 0.9221120476722717, 0.6968607902526855, 0.609006941318512, 0.5411885976791382, 0.45539963245391846, 0.4490988552570343, 0.3365069031715393, 0.3127160966396332], 'class_indices': [0.0, 0.0, 0.0, 27.0, 0.0, 62.0, 56.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 197.6ms\n",
      "Speed: 2.2ms preprocess, 197.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.4593505859375, 164.325927734375, 467.6392822265625, 479.42425537109375], [0.18259429931640625, 292.155029296875, 96.95147705078125, 479.6474609375], [198.46261596679688, 434.0623779296875, 265.3288879394531, 479.7615966796875], [30.483596801757812, 359.16375732421875, 65.26204681396484, 381.8189697265625], [373.15069580078125, 372.85528564453125, 451.05010986328125, 440.0147705078125], [155.31683349609375, 355.4511413574219, 210.94647216796875, 384.9247741699219], [86.95912170410156, 359.9585266113281, 166.548828125, 425.1529235839844], [461.59197998046875, 413.485107421875, 519.0298461914062, 464.9449462890625], [86.73358154296875, 360.13909912109375, 166.91629028320312, 424.97991943359375]], 'scores': [0.9551337361335754, 0.9009963274002075, 0.6859342455863953, 0.6714404821395874, 0.4688509404659271, 0.4679349362850189, 0.4635748863220215, 0.4614267945289612, 0.28586816787719727], 'class_indices': [0.0, 0.0, 27.0, 0.0, 62.0, 0.0, 62.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 2 chairs, 2 tvs, 1 laptop, 195.8ms\n",
      "Speed: 1.6ms preprocess, 195.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.86991882324219, 164.17222595214844, 467.45037841796875, 479.4437255859375], [0.225067138671875, 292.918701171875, 96.28645324707031, 478.8466796875], [30.868379592895508, 359.411865234375, 65.97152709960938, 381.72821044921875], [199.38522338867188, 434.5626525878906, 265.3716125488281, 479.7657775878906], [373.1912536621094, 372.5313720703125, 451.6447448730469, 441.139892578125], [157.19381713867188, 355.24163818359375, 214.22549438476562, 384.999755859375], [86.9237060546875, 359.6407470703125, 168.73023986816406, 425.103271484375], [86.96818542480469, 359.7376403808594, 168.4336700439453, 425.4917907714844], [462.13116455078125, 413.3343200683594, 518.793212890625, 464.6791687011719], [461.229736328125, 413.06182861328125, 537.910400390625, 479.75048828125]], 'scores': [0.9559474587440491, 0.8420078754425049, 0.7169296145439148, 0.671470582485199, 0.5498771667480469, 0.512039840221405, 0.42866262793540955, 0.42552173137664795, 0.3186783194541931, 0.2916334271430969], 'class_indices': [0.0, 0.0, 0.0, 27.0, 62.0, 0.0, 63.0, 62.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 207.5ms\n",
      "Speed: 2.1ms preprocess, 207.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.97589111328125, 164.12042236328125, 467.6678466796875, 479.43438720703125], [0.196014404296875, 292.937744140625, 95.25116729736328, 478.2794189453125], [31.272354125976562, 359.25091552734375, 66.04536437988281, 382.14154052734375], [158.9062042236328, 356.005859375, 214.35826110839844, 384.842041015625], [199.50357055664062, 435.1654052734375, 265.6075439453125, 479.72589111328125], [372.8487548828125, 372.44580078125, 451.28857421875, 441.3040771484375], [87.04098510742188, 359.8618469238281, 169.95309448242188, 425.5779113769531], [86.99539947509766, 359.74652099609375, 170.11105346679688, 425.11907958984375], [461.87823486328125, 413.3641357421875, 518.8760986328125, 464.54290771484375]], 'scores': [0.9548078179359436, 0.8316785097122192, 0.6498004198074341, 0.6417855620384216, 0.6339342594146729, 0.4652310907840729, 0.3940575122833252, 0.3520401418209076, 0.3189865052700043], 'class_indices': [0.0, 0.0, 0.0, 0.0, 27.0, 62.0, 62.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 227.0ms\n",
      "Speed: 1.8ms preprocess, 227.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.08155822753906, 164.19232177734375, 467.4500732421875, 479.4495849609375], [0.30603790283203125, 292.6767578125, 96.62539672851562, 478.7008056640625], [31.26556968688965, 359.5413818359375, 65.88939666748047, 382.538818359375], [161.20240783691406, 355.73309326171875, 214.00547790527344, 384.90447998046875], [372.96337890625, 372.8272705078125, 451.0496826171875, 441.4605712890625], [198.17404174804688, 435.12200927734375, 264.4936218261719, 479.74981689453125], [461.3922119140625, 413.426513671875, 519.0374755859375, 464.558349609375], [86.69254302978516, 359.774169921875, 171.95797729492188, 425.4564208984375], [86.65530395507812, 359.6234130859375, 172.10867309570312, 425.1201171875]], 'scores': [0.9585577249526978, 0.911666989326477, 0.6828071475028992, 0.5791200995445251, 0.5002975463867188, 0.4376256465911865, 0.39853647351264954, 0.3951592445373535, 0.3893490135669708], 'class_indices': [0.0, 0.0, 0.0, 0.0, 62.0, 27.0, 56.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 215.8ms\n",
      "Speed: 2.2ms preprocess, 215.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.97645568847656, 163.7128143310547, 467.36328125, 479.40875244140625], [199.36795043945312, 435.11572265625, 264.9849548339844, 479.7755126953125], [0.14339447021484375, 290.8792724609375, 96.023193359375, 462.005859375], [162.3304443359375, 354.8160095214844, 214.9779052734375, 384.4861755371094], [30.926780700683594, 359.52410888671875, 65.67852783203125, 381.33465576171875], [373.41632080078125, 372.8546142578125, 450.7059326171875, 440.5504150390625], [86.9675064086914, 359.64013671875, 171.81192016601562, 425.5662841796875], [461.71453857421875, 413.2200012207031, 518.866943359375, 464.0798034667969]], 'scores': [0.9522494077682495, 0.7490744590759277, 0.7058552503585815, 0.6990879774093628, 0.6981250643730164, 0.5128653645515442, 0.44292593002319336, 0.3593074679374695], 'class_indices': [0.0, 27.0, 0.0, 0.0, 0.0, 62.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 206.7ms\n",
      "Speed: 2.3ms preprocess, 206.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.07331848144531, 163.8179931640625, 467.15545654296875, 479.4395751953125], [0.31717681884765625, 291.6401672363281, 96.37523651123047, 479.3535461425781], [31.251632690429688, 359.70721435546875, 66.14691925048828, 382.75604248046875], [199.68353271484375, 434.565673828125, 266.4900817871094, 479.76690673828125], [373.52044677734375, 372.91265869140625, 451.16461181640625, 440.25579833984375], [162.36390686035156, 353.7483825683594, 216.02760314941406, 384.8833312988281], [86.53900146484375, 359.79473876953125, 172.01556396484375, 425.01446533203125], [86.77989959716797, 359.7659912109375, 171.60943603515625, 425.2410888671875], [461.94671630859375, 413.4884033203125, 518.6710815429688, 464.442626953125]], 'scores': [0.9565310478210449, 0.9206972718238831, 0.7091875076293945, 0.6651461720466614, 0.5472313761711121, 0.49139097332954407, 0.3751870095729828, 0.3454127013683319, 0.25952479243278503], 'class_indices': [0.0, 0.0, 0.0, 27.0, 62.0, 0.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 215.5ms\n",
      "Speed: 1.8ms preprocess, 215.5ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.579193115234375, 163.58914184570312, 467.6030578613281, 479.4307556152344], [0.220062255859375, 291.49737548828125, 96.76522064208984, 479.48553466796875], [161.22711181640625, 354.3939208984375, 216.11651611328125, 384.53485107421875], [31.38092613220215, 359.5085754394531, 66.8929672241211, 382.0926208496094], [198.58013916015625, 434.36602783203125, 265.309814453125, 479.76544189453125], [373.47369384765625, 372.82122802734375, 451.12762451171875, 440.66876220703125], [86.67642211914062, 359.68145751953125, 172.13088989257812, 425.27325439453125], [86.8656997680664, 359.7112731933594, 172.06417846679688, 425.5596008300781], [461.8292236328125, 413.59698486328125, 518.8909912109375, 464.55218505859375], [31.520780563354492, 358.86737060546875, 86.83004760742188, 404.77459716796875]], 'scores': [0.9538275599479675, 0.9130886197090149, 0.7105104327201843, 0.6410253643989563, 0.5714914798736572, 0.4986584484577179, 0.43796059489250183, 0.39082589745521545, 0.33227992057800293, 0.2525661885738373], 'class_indices': [0.0, 0.0, 0.0, 0.0, 27.0, 62.0, 63.0, 62.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 231.9ms\n",
      "Speed: 1.9ms preprocess, 231.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.97120666503906, 163.7880859375, 467.5185546875, 479.47113037109375], [0.27530670166015625, 290.91998291015625, 96.02749633789062, 477.66033935546875], [30.405990600585938, 360.024169921875, 65.02195739746094, 381.95782470703125], [198.41021728515625, 434.8143615722656, 264.9517822265625, 479.7505798339844], [161.65518188476562, 354.87896728515625, 216.17855834960938, 384.82550048828125], [372.96978759765625, 372.9407653808594, 451.03411865234375, 440.8019104003906], [461.92645263671875, 413.44097900390625, 519.0191650390625, 464.45556640625], [86.71707153320312, 359.96673583984375, 172.29656982421875, 425.08941650390625]], 'scores': [0.9548963904380798, 0.7858585715293884, 0.7284652590751648, 0.6196603178977966, 0.5869101881980896, 0.5851171612739563, 0.5057229995727539, 0.454164057970047], 'class_indices': [0.0, 0.0, 0.0, 27.0, 0.0, 62.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 222.5ms\n",
      "Speed: 1.8ms preprocess, 222.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.539794921875, 163.5394287109375, 467.70703125, 479.4365234375], [0.22097015380859375, 291.6183166503906, 95.16122436523438, 479.5252990722656], [198.13320922851562, 433.877197265625, 265.2366943359375, 479.7659912109375], [30.161401748657227, 360.14495849609375, 65.2250747680664, 381.98419189453125], [461.69329833984375, 413.35809326171875, 518.6566162109375, 464.29473876953125], [86.69589233398438, 359.646484375, 174.97024536132812, 425.125732421875], [373.368896484375, 372.7822570800781, 450.9190673828125, 440.5796813964844], [164.0480194091797, 353.42645263671875, 218.8839874267578, 384.55474853515625]], 'scores': [0.953489363193512, 0.9019643068313599, 0.6977543830871582, 0.6589933633804321, 0.5154095888137817, 0.4880213141441345, 0.48198002576828003, 0.3564084768295288], 'class_indices': [0.0, 0.0, 27.0, 0.0, 56.0, 63.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 197.5ms\n",
      "Speed: 2.1ms preprocess, 197.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.84423828125, 163.7393341064453, 467.773681640625, 479.44342041015625], [0.317413330078125, 292.33734130859375, 95.79920196533203, 479.13800048828125], [29.836210250854492, 360.359619140625, 64.40945434570312, 381.509521484375], [198.889892578125, 434.56585693359375, 265.834228515625, 479.75958251953125], [373.6419372558594, 372.7802734375, 451.1029357910156, 440.3680419921875], [461.7635498046875, 413.7919921875, 518.6749877929688, 464.153564453125], [86.79167175292969, 359.59564208984375, 179.35069274902344, 426.17510986328125], [86.9647216796875, 359.451904296875, 180.23806762695312, 425.91162109375], [168.62179565429688, 352.83447265625, 219.45553588867188, 384.340087890625]], 'scores': [0.9529173970222473, 0.917896032333374, 0.6924754977226257, 0.6411845684051514, 0.5596773624420166, 0.43650591373443604, 0.39122307300567627, 0.35811319947242737, 0.31582745909690857], 'class_indices': [0.0, 0.0, 0.0, 27.0, 62.0, 56.0, 63.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 2 chairs, 2 tvs, 1 laptop, 193.5ms\n",
      "Speed: 2.2ms preprocess, 193.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.88121032714844, 163.39599609375, 467.8037109375, 479.42840576171875], [0.23769378662109375, 291.90460205078125, 96.61372375488281, 479.5684814453125], [29.51485824584961, 360.02569580078125, 63.90690231323242, 381.42144775390625], [373.22808837890625, 372.70660400390625, 451.05572509765625, 440.02227783203125], [461.77801513671875, 413.149169921875, 519.0318603515625, 464.5426025390625], [198.75271606445312, 434.92822265625, 265.5298767089844, 479.7552490234375], [86.72161102294922, 359.7117614746094, 180.06597900390625, 425.0379333496094], [86.92002868652344, 359.5838928222656, 180.83973693847656, 425.1175231933594], [461.2012939453125, 412.782958984375, 538.67138671875, 479.7540283203125]], 'scores': [0.9502286314964294, 0.9260360598564148, 0.6704487800598145, 0.5762196779251099, 0.4856902062892914, 0.443744033575058, 0.4181506037712097, 0.40253132581710815, 0.2804400622844696], 'class_indices': [0.0, 0.0, 0.0, 62.0, 56.0, 27.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 2 tvs, 1 laptop, 193.4ms\n",
      "Speed: 1.8ms preprocess, 193.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.99745178222656, 163.0880126953125, 467.2406005859375, 479.39630126953125], [0.21477508544921875, 291.90185546875, 96.88240051269531, 479.74749755859375], [29.62645149230957, 360.41668701171875, 64.04522705078125, 381.74224853515625], [372.8274230957031, 372.9015197753906, 451.0849304199219, 441.0064392089844], [86.74227905273438, 359.76275634765625, 181.0328369140625, 424.69354248046875], [86.98530578613281, 359.640869140625, 181.67710876464844, 424.7491455078125], [198.63912963867188, 435.0377197265625, 265.4683532714844, 479.7509765625]], 'scores': [0.9493913054466248, 0.9188104271888733, 0.6986933946609497, 0.5363849997520447, 0.45660945773124695, 0.36604049801826477, 0.32739564776420593], 'class_indices': [0.0, 0.0, 0.0, 62.0, 63.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 196.5ms\n",
      "Speed: 1.6ms preprocess, 196.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.52667236328125, 163.3583984375, 467.694580078125, 479.4129638671875], [0.1974945068359375, 292.76239013671875, 96.98054504394531, 478.546142578125], [29.429420471191406, 360.3988037109375, 64.24154663085938, 380.9388427734375], [200.23892211914062, 434.671630859375, 266.2574768066406, 479.7421875], [87.0953369140625, 359.5892639160156, 180.66180419921875, 425.0086364746094], [373.12213134765625, 372.71014404296875, 451.03717041015625, 441.46539306640625], [86.9773178100586, 359.67926025390625, 179.82089233398438, 425.05474853515625], [462.12823486328125, 413.3160400390625, 518.8898315429688, 464.4881591796875]], 'scores': [0.9516561031341553, 0.8773248195648193, 0.7053535580635071, 0.6529955267906189, 0.4828665554523468, 0.4798519015312195, 0.3386100232601166, 0.317844033241272], 'class_indices': [0.0, 0.0, 0.0, 27.0, 62.0, 62.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 195.7ms\n",
      "Speed: 2.0ms preprocess, 195.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.22312927246094, 163.44332885742188, 467.414794921875, 479.4316711425781], [0.24623870849609375, 291.992431640625, 96.21205139160156, 479.3775634765625], [29.526565551757812, 360.28472900390625, 63.95770263671875, 380.52471923828125], [198.43667602539062, 435.144287109375, 266.1131286621094, 479.7491455078125], [462.0318603515625, 413.3190002441406, 518.93896484375, 464.4992980957031], [373.0783386230469, 372.73883056640625, 451.1789855957031, 440.74102783203125], [87.0573501586914, 359.7059326171875, 177.76101684570312, 424.7967529296875], [87.18924713134766, 359.6898193359375, 178.52206420898438, 424.8660888671875], [171.31732177734375, 347.98675537109375, 219.41082763671875, 384.48321533203125]], 'scores': [0.9527806043624878, 0.9127011299133301, 0.6516788601875305, 0.5235918760299683, 0.5167636275291443, 0.5102841854095459, 0.4459119439125061, 0.42604219913482666, 0.28074681758880615], 'class_indices': [0.0, 0.0, 0.0, 27.0, 56.0, 62.0, 63.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 202.1ms\n",
      "Speed: 1.9ms preprocess, 202.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.60057067871094, 163.1983184814453, 467.37451171875, 479.43865966796875], [0.22695159912109375, 292.123291015625, 96.48014831542969, 479.2947998046875], [29.181255340576172, 360.4295654296875, 63.62380599975586, 380.9049072265625], [87.11985778808594, 359.7193908691406, 178.09251403808594, 425.2730407714844], [373.6258544921875, 372.71685791015625, 451.257080078125, 441.24700927734375], [461.8294677734375, 413.37860107421875, 518.770751953125, 464.46673583984375], [199.58319091796875, 435.2901916503906, 266.18072509765625, 479.7496032714844], [169.44503784179688, 347.6123046875, 219.59194946289062, 384.57861328125]], 'scores': [0.9500434398651123, 0.9036381244659424, 0.7363950610160828, 0.46153128147125244, 0.4403867721557617, 0.40306034684181213, 0.3891986906528473, 0.387318879365921], 'class_indices': [0.0, 0.0, 0.0, 62.0, 62.0, 56.0, 27.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 195.5ms\n",
      "Speed: 1.9ms preprocess, 195.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.86723327636719, 163.62985229492188, 467.46636962890625, 479.4136047363281], [0.2255096435546875, 291.8901062011719, 96.42245483398438, 478.6608581542969], [29.130905151367188, 360.40557861328125, 63.24937438964844, 380.07391357421875], [373.427490234375, 372.741943359375, 451.0128173828125, 441.347412109375], [199.65948486328125, 436.0247802734375, 266.18084716796875, 479.750732421875], [86.71731567382812, 360.0520324707031, 171.29959106445312, 425.2045593261719], [163.8512725830078, 348.806396484375, 218.0661163330078, 384.52899169921875], [461.908447265625, 413.3663024902344, 518.7357177734375, 464.7375183105469]], 'scores': [0.9490326642990112, 0.8386268019676208, 0.7291351556777954, 0.5204549431800842, 0.5172356367111206, 0.5015208721160889, 0.4760628342628479, 0.33729517459869385], 'class_indices': [0.0, 0.0, 0.0, 62.0, 27.0, 63.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 193.9ms\n",
      "Speed: 1.7ms preprocess, 193.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.82080078125, 162.671875, 467.25872802734375, 479.3360595703125], [0.19873046875, 291.93682861328125, 96.69470977783203, 479.22845458984375], [29.479530334472656, 360.2484130859375, 63.37506866455078, 380.980712890625], [373.3082580566406, 372.77294921875, 450.9556579589844, 440.6942138671875], [87.02291870117188, 359.783447265625, 166.66690063476562, 424.894775390625], [461.98516845703125, 413.35894775390625, 518.9725952148438, 464.46429443359375], [156.45004272460938, 348.5367431640625, 219.92483520507812, 384.740478515625], [199.91983032226562, 435.35064697265625, 266.2608947753906, 479.71331787109375]], 'scores': [0.9473662376403809, 0.8928273916244507, 0.7073012590408325, 0.5473896265029907, 0.513221263885498, 0.44684210419654846, 0.4273356795310974, 0.3156786859035492], 'class_indices': [0.0, 0.0, 0.0, 62.0, 63.0, 56.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 193.2ms\n",
      "Speed: 2.1ms preprocess, 193.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.022796630859375, 163.3184356689453, 467.6139831542969, 479.42559814453125], [0.15875244140625, 291.91741943359375, 96.42876434326172, 479.58575439453125], [29.21258544921875, 360.5233154296875, 63.50678253173828, 380.626708984375], [151.86309814453125, 349.4517517089844, 220.26409912109375, 385.1405944824219], [373.1819152832031, 372.9281005859375, 451.0127258300781, 441.210693359375], [199.56875610351562, 434.94561767578125, 266.0657043457031, 479.74676513671875], [86.87738037109375, 359.95025634765625, 165.07778930664062, 425.28070068359375], [86.76150512695312, 360.01190185546875, 164.87969970703125, 425.18316650390625], [462.2557373046875, 413.53326416015625, 519.2005615234375, 464.76556396484375]], 'scores': [0.9504028558731079, 0.9131636619567871, 0.6928614377975464, 0.5015416145324707, 0.4985339939594269, 0.4804731607437134, 0.40465354919433594, 0.38081830739974976, 0.34389689564704895], 'class_indices': [0.0, 0.0, 0.0, 0.0, 62.0, 27.0, 62.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 196.8ms\n",
      "Speed: 1.9ms preprocess, 196.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.76490783691406, 163.7322998046875, 467.6197509765625, 479.4478759765625], [0.20323944091796875, 291.8651123046875, 96.16542053222656, 478.97998046875], [29.655441284179688, 360.51739501953125, 63.240814208984375, 380.03411865234375], [373.505859375, 372.94866943359375, 451.047119140625, 440.56939697265625], [150.63087463378906, 350.4669189453125, 220.0583038330078, 385.4617919921875], [199.48638916015625, 435.0426025390625, 266.00762939453125, 479.7430419921875], [86.93141174316406, 359.83673095703125, 164.7210693359375, 425.60711669921875], [461.67999267578125, 413.1695556640625, 518.8794555664062, 464.3856201171875], [86.81134033203125, 359.8783264160156, 164.62301635742188, 425.4645690917969]], 'scores': [0.9497413039207458, 0.8899470567703247, 0.7130559086799622, 0.5714020729064941, 0.5442625880241394, 0.48989102244377136, 0.42928990721702576, 0.3806071877479553, 0.30381670594215393], 'class_indices': [0.0, 0.0, 0.0, 62.0, 0.0, 27.0, 62.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 199.7ms\n",
      "Speed: 2.1ms preprocess, 199.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.21064758300781, 163.56283569335938, 467.77825927734375, 479.4002990722656], [0.25420379638671875, 292.1541748046875, 96.22399139404297, 479.5250244140625], [29.4537410736084, 360.35784912109375, 63.64452362060547, 380.57757568359375], [373.74090576171875, 372.5323791503906, 451.4638671875, 439.9057312011719], [199.06130981445312, 435.484375, 265.6625671386719, 479.74627685546875], [86.9727783203125, 359.7557678222656, 166.27174377441406, 425.0877380371094], [152.25564575195312, 349.94708251953125, 221.5628662109375, 384.99407958984375], [87.04412841796875, 359.662353515625, 165.992919921875, 424.98956298828125], [462.05035400390625, 413.3754577636719, 518.9578857421875, 464.5312194824219]], 'scores': [0.9499121904373169, 0.9141359925270081, 0.7360800504684448, 0.5198760032653809, 0.47039851546287537, 0.42879387736320496, 0.40889042615890503, 0.3561820089817047, 0.3035922646522522], 'class_indices': [0.0, 0.0, 0.0, 62.0, 27.0, 62.0, 0.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 193.2ms\n",
      "Speed: 2.0ms preprocess, 193.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.874176025390625, 163.531005859375, 467.9761047363281, 479.45977783203125], [0.17668914794921875, 292.0420837402344, 96.72731018066406, 479.9302673339844], [28.951549530029297, 360.46282958984375, 63.7421989440918, 381.44525146484375], [373.7320556640625, 372.8084411621094, 451.2291259765625, 439.7417907714844], [155.51600646972656, 350.3585510253906, 221.38389587402344, 384.6340026855469], [87.25067138671875, 359.8261413574219, 167.17160034179688, 425.0054626464844], [461.91741943359375, 413.57257080078125, 518.9174194335938, 464.63641357421875], [198.8759765625, 435.04449462890625, 265.82464599609375, 479.7486572265625]], 'scores': [0.9526667594909668, 0.9166142344474792, 0.7233619689941406, 0.5444144606590271, 0.5120206475257874, 0.4812980890274048, 0.32620686292648315, 0.2805466949939728], 'class_indices': [0.0, 0.0, 0.0, 62.0, 0.0, 63.0, 56.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 193.6ms\n",
      "Speed: 1.8ms preprocess, 193.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.14118957519531, 163.47772216796875, 468.18389892578125, 479.43499755859375], [0.23822784423828125, 291.5302734375, 95.93765258789062, 479.49951171875], [29.16552734375, 360.3641357421875, 64.02831268310547, 381.2890625], [198.19815063476562, 434.92474365234375, 265.9158630371094, 479.745361328125], [158.27328491210938, 348.9872741699219, 220.90866088867188, 384.4713439941406], [374.1007995605469, 372.8341064453125, 451.0587463378906, 440.0106201171875], [462.2705078125, 413.518310546875, 518.5118408203125, 464.1800537109375], [86.80142211914062, 359.83441162109375, 168.68572998046875, 424.72247314453125]], 'scores': [0.952160656452179, 0.9186080098152161, 0.6564253568649292, 0.5789622068405151, 0.5203242301940918, 0.511702299118042, 0.4461645483970642, 0.4355711340904236], 'class_indices': [0.0, 0.0, 0.0, 27.0, 0.0, 62.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 194.1ms\n",
      "Speed: 1.6ms preprocess, 194.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.18061828613281, 163.66616821289062, 467.85723876953125, 479.4372863769531], [0.3294677734375, 291.431640625, 96.76266479492188, 479.10986328125], [29.034276962280273, 360.5634765625, 63.50360870361328, 381.7786865234375], [159.6078338623047, 347.63232421875, 221.3805389404297, 384.5205078125], [86.67233276367188, 359.64361572265625, 169.59243774414062, 425.18988037109375], [373.9620056152344, 372.9461669921875, 451.1662292480469, 439.70654296875], [461.4927978515625, 413.536376953125, 519.034912109375, 464.7098388671875], [199.2305908203125, 435.6842041015625, 265.86907958984375, 479.74053955078125]], 'scores': [0.9525865912437439, 0.9200258255004883, 0.6655480861663818, 0.5590322613716125, 0.4836219847202301, 0.4623613953590393, 0.4154476821422577, 0.36269867420196533], 'class_indices': [0.0, 0.0, 0.0, 0.0, 63.0, 62.0, 56.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 1 tv, 1 laptop, 195.7ms\n",
      "Speed: 1.7ms preprocess, 195.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.54026794433594, 163.42926025390625, 468.195068359375, 479.45379638671875], [0.22081756591796875, 291.82012939453125, 96.64511108398438, 479.62042236328125], [29.114059448242188, 360.24334716796875, 63.480201721191406, 380.93621826171875], [374.058349609375, 372.54876708984375, 451.362548828125, 440.32159423828125], [160.4835205078125, 347.21966552734375, 221.55865478515625, 384.46343994140625], [461.7637939453125, 413.2857360839844, 518.687744140625, 464.1990661621094], [86.717529296875, 359.578857421875, 170.28457641601562, 425.05560302734375]], 'scores': [0.9539265632629395, 0.9257310032844543, 0.7293121218681335, 0.608230471611023, 0.57342529296875, 0.5723356008529663, 0.508903980255127], 'class_indices': [0.0, 0.0, 0.0, 62.0, 0.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 194.3ms\n",
      "Speed: 2.0ms preprocess, 194.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.8907470703125, 162.82652282714844, 468.17901611328125, 479.456787109375], [0.3031158447265625, 291.6510009765625, 96.12615203857422, 478.9947509765625], [29.04253387451172, 360.37249755859375, 64.01605224609375, 381.51226806640625], [161.49298095703125, 347.1744689941406, 222.07992553710938, 384.2762756347656], [200.18572998046875, 435.24859619140625, 266.0744934082031, 479.74713134765625], [461.87109375, 413.3953857421875, 518.6368408203125, 464.312255859375], [374.41583251953125, 372.6328125, 451.1226806640625, 440.37451171875], [87.09119415283203, 359.7708740234375, 170.51220703125, 425.31683349609375], [87.068603515625, 359.80096435546875, 170.51470947265625, 425.63226318359375]], 'scores': [0.9521603584289551, 0.9181426167488098, 0.6949605345726013, 0.5930691957473755, 0.5097732543945312, 0.4986281096935272, 0.4774418771266937, 0.45736488699913025, 0.4069652855396271], 'class_indices': [0.0, 0.0, 0.0, 0.0, 27.0, 56.0, 62.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 192.9ms\n",
      "Speed: 1.7ms preprocess, 192.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.75750732421875, 163.4482421875, 468.1441650390625, 479.43267822265625], [0.24582672119140625, 291.87713623046875, 96.50205993652344, 479.56622314453125], [29.076194763183594, 360.4263916015625, 64.03779602050781, 380.9322509765625], [374.52520751953125, 372.9530029296875, 451.16961669921875, 439.75634765625], [162.12045288085938, 347.0298156738281, 222.32394409179688, 384.4296569824219], [461.72021484375, 413.6708984375, 518.7305908203125, 464.5010986328125], [86.42650604248047, 359.7843017578125, 171.39501953125, 425.2208251953125], [86.50045013427734, 359.82232666015625, 171.54086303710938, 425.34698486328125], [201.0286865234375, 434.36834716796875, 266.00677490234375, 479.7669677734375]], 'scores': [0.9518336653709412, 0.922760009765625, 0.6049688458442688, 0.5748823881149292, 0.5703740119934082, 0.45624884963035583, 0.45058295130729675, 0.44316667318344116, 0.4409509301185608], 'class_indices': [0.0, 0.0, 0.0, 62.0, 0.0, 56.0, 63.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 197.2ms\n",
      "Speed: 2.2ms preprocess, 197.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.85453796386719, 163.58135986328125, 467.71527099609375, 479.47332763671875], [0.17618560791015625, 291.8890380859375, 96.53826141357422, 479.484375], [29.58307647705078, 360.49468994140625, 63.958580017089844, 381.354248046875], [162.27490234375, 347.6041259765625, 222.27349853515625, 384.41400146484375], [373.8852844238281, 372.90533447265625, 451.1000671386719, 439.66522216796875], [462.12493896484375, 413.3964538574219, 519.2869262695312, 464.4471130371094], [86.75718688964844, 359.92529296875, 171.15794372558594, 424.8646240234375], [86.66921997070312, 359.9154052734375, 171.01458740234375, 424.852294921875]], 'scores': [0.9521117210388184, 0.8930083513259888, 0.6539979577064514, 0.6214600205421448, 0.5763545632362366, 0.4719851315021515, 0.46910223364830017, 0.3277357518672943], 'class_indices': [0.0, 0.0, 0.0, 0.0, 62.0, 56.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 197.2ms\n",
      "Speed: 2.1ms preprocess, 197.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.146392822265625, 163.2344512939453, 467.9877014160156, 479.44757080078125], [0.25460052490234375, 292.5963134765625, 96.75532531738281, 479.3753662109375], [29.433059692382812, 360.4344482421875, 63.30116271972656, 380.8057861328125], [163.28114318847656, 347.20416259765625, 222.1186981201172, 384.53326416015625], [374.2716064453125, 372.9989929199219, 451.100830078125, 440.7507019042969], [86.91200256347656, 359.9190673828125, 171.7474822998047, 425.3431396484375], [461.8828125, 413.63336181640625, 518.8851318359375, 464.32403564453125]], 'scores': [0.9523630738258362, 0.924161434173584, 0.74652498960495, 0.54372638463974, 0.5356416702270508, 0.4756202697753906, 0.4003261923789978], 'class_indices': [0.0, 0.0, 0.0, 0.0, 62.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 198.6ms\n",
      "Speed: 2.1ms preprocess, 198.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.79454040527344, 163.624267578125, 467.933837890625, 479.4307861328125], [0.292205810546875, 292.14599609375, 96.46665954589844, 479.55340576171875], [28.981918334960938, 360.3544616699219, 63.56780242919922, 381.1595764160156], [374.4176940917969, 373.0459289550781, 451.1956481933594, 439.4762268066406], [462.135986328125, 413.4557189941406, 518.91455078125, 464.6006774902344], [165.43600463867188, 347.28631591796875, 222.26077270507812, 384.48394775390625], [86.78752899169922, 359.70294189453125, 175.35394287109375, 425.44635009765625], [200.7879638671875, 434.474609375, 266.3421936035156, 479.74468994140625], [86.57781982421875, 359.71087646484375, 174.86581420898438, 425.33990478515625]], 'scores': [0.9528875350952148, 0.9342679381370544, 0.6562219858169556, 0.6009092926979065, 0.5268629193305969, 0.5155734419822693, 0.49752429127693176, 0.43574070930480957, 0.32859736680984497], 'class_indices': [0.0, 0.0, 0.0, 62.0, 56.0, 0.0, 62.0, 27.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 tv, 1 laptop, 194.3ms\n",
      "Speed: 1.5ms preprocess, 194.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.75555419921875, 163.5362548828125, 467.90484619140625, 479.3895263671875], [0.26003265380859375, 292.285888671875, 96.5396957397461, 479.315185546875], [200.17236328125, 434.4930114746094, 266.54144287109375, 479.7499084472656], [29.147565841674805, 360.4396057128906, 64.16093444824219, 381.7745666503906], [167.0572509765625, 347.2684326171875, 221.7379150390625, 384.326904296875], [86.65512084960938, 359.7164001464844, 175.43414306640625, 424.7155456542969], [374.02008056640625, 372.1255798339844, 451.20623779296875, 438.8214416503906]], 'scores': [0.9518834352493286, 0.9252949953079224, 0.7225056290626526, 0.6256373524665833, 0.5901582837104797, 0.5182459354400635, 0.5102329254150391], 'class_indices': [0.0, 0.0, 27.0, 0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 195.3ms\n",
      "Speed: 1.8ms preprocess, 195.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.421966552734375, 163.52090454101562, 468.1055603027344, 479.4543762207031], [0.281463623046875, 292.4503173828125, 97.01423645019531, 479.556884765625], [29.07341766357422, 360.6378173828125, 63.70252990722656, 381.451416015625], [374.28369140625, 372.880615234375, 450.9627685546875, 440.95806884765625], [86.8936767578125, 359.826904296875, 176.023681640625, 425.52978515625], [166.83306884765625, 347.79638671875, 222.307861328125, 384.55322265625], [462.25140380859375, 413.5517578125, 519.1588745117188, 464.868408203125], [200.47659301757812, 434.93450927734375, 266.9587707519531, 479.7374267578125], [86.67446899414062, 359.9226989746094, 175.30514526367188, 425.4331359863281]], 'scores': [0.9518221020698547, 0.9298465847969055, 0.7208818793296814, 0.5298623442649841, 0.5004892349243164, 0.4912753999233246, 0.4695976972579956, 0.3903244435787201, 0.36550599336624146], 'class_indices': [0.0, 0.0, 0.0, 62.0, 62.0, 0.0, 56.0, 27.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 198.5ms\n",
      "Speed: 2.0ms preprocess, 198.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.21784973144531, 163.6094970703125, 468.00274658203125, 479.44573974609375], [0.25672149658203125, 291.8194580078125, 96.72250366210938, 479.5091552734375], [28.867067337036133, 360.49639892578125, 63.41462707519531, 380.75006103515625], [374.4176025390625, 372.64404296875, 451.0084228515625, 440.31005859375], [166.3561553955078, 347.64825439453125, 222.2963409423828, 384.45526123046875], [86.7330322265625, 359.6829833984375, 175.55563354492188, 425.17498779296875], [462.0482177734375, 413.58575439453125, 519.1881103515625, 464.24456787109375]], 'scores': [0.9528331756591797, 0.9115185141563416, 0.7509355545043945, 0.5661487579345703, 0.5148471593856812, 0.5036301016807556, 0.48827433586120605], 'class_indices': [0.0, 0.0, 0.0, 62.0, 0.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 193.8ms\n",
      "Speed: 1.7ms preprocess, 193.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.93370056152344, 163.53573608398438, 467.9195556640625, 479.4416809082031], [0.2130584716796875, 291.6798400878906, 96.13489532470703, 478.7966003417969], [29.332477569580078, 360.33221435546875, 63.347530364990234, 380.68243408203125], [201.3922119140625, 434.98162841796875, 267.3720703125, 479.71954345703125], [166.75778198242188, 347.6348876953125, 221.70767211914062, 384.30657958984375], [86.69903564453125, 359.77740478515625, 175.36190795898438, 425.91680908203125], [86.900390625, 359.687744140625, 175.86993408203125, 425.6678466796875], [374.57470703125, 372.63555908203125, 451.2677001953125, 439.44573974609375], [462.07977294921875, 413.2259826660156, 518.9796752929688, 464.3600769042969]], 'scores': [0.950842022895813, 0.8650860786437988, 0.7352924346923828, 0.7134873867034912, 0.503279983997345, 0.42886748909950256, 0.4143049120903015, 0.3889593183994293, 0.350166380405426], 'class_indices': [0.0, 0.0, 0.0, 27.0, 0.0, 63.0, 62.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 192.5ms\n",
      "Speed: 1.5ms preprocess, 192.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[62.4283447265625, 162.21185302734375, 467.87554931640625, 479.47314453125], [0.3056640625, 291.90576171875, 96.32113647460938, 479.4776611328125], [28.884403228759766, 360.6407470703125, 63.879940032958984, 381.6767578125], [373.7830810546875, 372.8645935058594, 451.13861083984375, 440.4924621582031], [461.9788818359375, 413.70257568359375, 518.8648681640625, 463.90606689453125], [163.5346221923828, 347.74713134765625, 220.3809356689453, 383.10382080078125], [86.922119140625, 359.844482421875, 173.3773193359375, 426.732421875], [86.75690460205078, 359.82464599609375, 173.00494384765625, 426.74798583984375]], 'scores': [0.9548705220222473, 0.9200415015220642, 0.6674302220344543, 0.5887116193771362, 0.5659782290458679, 0.5115788578987122, 0.448909193277359, 0.304678738117218], 'class_indices': [0.0, 0.0, 0.0, 62.0, 56.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 1 tv, 1 laptop, 193.9ms\n",
      "Speed: 1.8ms preprocess, 193.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[57.52671813964844, 161.06341552734375, 467.510986328125, 479.44366455078125], [0.2210693359375, 291.3716125488281, 96.56930541992188, 478.7182312011719], [29.317155838012695, 360.39312744140625, 64.02119445800781, 381.32794189453125], [163.31167602539062, 348.3857421875, 219.48126220703125, 382.5421142578125], [86.71508026123047, 359.5937194824219, 174.17681884765625, 424.7638244628906], [372.58746337890625, 372.674560546875, 451.02508544921875, 441.6439208984375], [462.04205322265625, 413.2555236816406, 519.0043334960938, 463.7080383300781]], 'scores': [0.9509844183921814, 0.8701560497283936, 0.726698100566864, 0.5375881195068359, 0.523786187171936, 0.4996008574962616, 0.36791256070137024], 'class_indices': [0.0, 0.0, 0.0, 0.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 1 tv, 1 laptop, 198.2ms\n",
      "Speed: 1.6ms preprocess, 198.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[58.08172607421875, 161.11607360839844, 469.98187255859375, 479.47900390625], [0.23198699951171875, 291.99188232421875, 95.60226440429688, 479.5706787109375], [29.12403106689453, 360.61883544921875, 63.41859436035156, 380.52178955078125], [462.3157958984375, 413.5135803222656, 519.027099609375, 464.0418395996094], [165.1475830078125, 349.58441162109375, 221.18911743164062, 383.01605224609375], [86.52491760253906, 359.9277038574219, 176.66957092285156, 426.2432556152344], [374.72955322265625, 372.591796875, 451.26385498046875, 436.6680908203125]], 'scores': [0.9502360820770264, 0.9263488054275513, 0.7251224517822266, 0.560017466545105, 0.5107842683792114, 0.4859284460544586, 0.40345460176467896], 'class_indices': [0.0, 0.0, 0.0, 56.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 195.9ms\n",
      "Speed: 1.7ms preprocess, 195.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[62.13972473144531, 160.8096923828125, 472.17010498046875, 479.46240234375], [0.2249908447265625, 292.07879638671875, 96.53227233886719, 479.58514404296875], [29.076324462890625, 360.47991943359375, 63.63123321533203, 381.44757080078125], [462.89599609375, 413.4313049316406, 518.7315673828125, 464.0411682128906], [377.95135498046875, 373.16961669921875, 450.86614990234375, 425.75616455078125], [86.92840576171875, 359.6565856933594, 179.44744873046875, 427.1214294433594], [86.75096130371094, 359.6084289550781, 179.0134735107422, 427.1581115722656], [164.5369873046875, 350.51116943359375, 225.05035400390625, 382.71868896484375]], 'scores': [0.9519362449645996, 0.9231879115104675, 0.6811203956604004, 0.5105755925178528, 0.4647056460380554, 0.44252097606658936, 0.3105344772338867, 0.2937611937522888], 'class_indices': [0.0, 0.0, 0.0, 56.0, 62.0, 62.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 196.3ms\n",
      "Speed: 1.3ms preprocess, 196.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[61.4525146484375, 159.90692138671875, 472.5970458984375, 479.31256103515625], [0.24349212646484375, 291.09771728515625, 97.3522720336914, 478.71337890625], [28.84821319580078, 360.4207763671875, 63.943077087402344, 380.55181884765625], [86.86287689208984, 359.552001953125, 178.61813354492188, 427.2415771484375], [379.1384582519531, 372.948974609375, 451.1143493652344, 425.58514404296875], [463.01812744140625, 413.49615478515625, 518.5642700195312, 463.98345947265625], [86.84097290039062, 359.4259033203125, 178.6539306640625, 427.2696533203125], [162.44581604003906, 350.5494689941406, 226.4175262451172, 382.4288024902344]], 'scores': [0.9537341594696045, 0.8144599795341492, 0.6325768828392029, 0.48751896619796753, 0.4862710237503052, 0.37849587202072144, 0.32108816504478455, 0.3117820918560028], 'class_indices': [0.0, 0.0, 0.0, 62.0, 62.0, 56.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 193.8ms\n",
      "Speed: 1.8ms preprocess, 193.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[61.63316345214844, 160.14407348632812, 473.2371826171875, 479.4521789550781], [0.2521209716796875, 292.70574951171875, 96.94113159179688, 479.24749755859375], [29.316232681274414, 360.4100341796875, 63.507713317871094, 381.5811767578125], [462.970947265625, 413.1991882324219, 518.68798828125, 463.9665832519531], [86.97482299804688, 359.62481689453125, 177.47036743164062, 427.06854248046875], [379.6578369140625, 372.9410400390625, 451.1455078125, 424.00439453125], [159.2845916748047, 351.0199279785156, 225.4058074951172, 382.4757995605469], [86.80558013916016, 359.60791015625, 176.76321411132812, 427.052734375]], 'scores': [0.9535747766494751, 0.9170491695404053, 0.6703683733940125, 0.5382025837898254, 0.4834057092666626, 0.46991243958473206, 0.4255276918411255, 0.374226838350296], 'class_indices': [0.0, 0.0, 0.0, 56.0, 62.0, 62.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 194.1ms\n",
      "Speed: 1.6ms preprocess, 194.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[60.800994873046875, 160.76165771484375, 470.0203552246094, 479.2960205078125], [0.28270721435546875, 292.4144287109375, 95.96308135986328, 479.39544677734375], [29.186738967895508, 360.7447204589844, 63.64482116699219, 380.8133850097656], [377.3929443359375, 373.36065673828125, 451.085693359375, 435.04949951171875], [86.94776916503906, 359.8238525390625, 175.69187927246094, 427.7001953125], [461.94970703125, 413.8299560546875, 518.7564697265625, 464.17852783203125], [157.52963256835938, 351.9397277832031, 224.06216430664062, 383.5456237792969]], 'scores': [0.9516202807426453, 0.9199896454811096, 0.7034911513328552, 0.4807691276073456, 0.45383337140083313, 0.4265483021736145, 0.31291669607162476], 'class_indices': [0.0, 0.0, 0.0, 62.0, 62.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 204.9ms\n",
      "Speed: 1.7ms preprocess, 204.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.28422546386719, 161.50543212890625, 468.15203857421875, 479.4764404296875], [0.211212158203125, 292.1329650878906, 95.93811798095703, 479.7687072753906], [29.48223876953125, 359.7164611816406, 63.6942138671875, 380.9513244628906], [86.75377655029297, 359.525146484375, 176.762451171875, 425.37457275390625], [373.59765625, 372.3616943359375, 451.0968017578125, 439.2308349609375], [462.206298828125, 413.1226806640625, 518.752197265625, 464.0301513671875], [158.24368286132812, 352.2897033691406, 220.97262573242188, 383.0109558105469]], 'scores': [0.9517754316329956, 0.9198837876319885, 0.7007611989974976, 0.49740052223205566, 0.4810810387134552, 0.35612642765045166, 0.3034542500972748], 'class_indices': [0.0, 0.0, 0.0, 62.0, 62.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 2 chairs, 1 tv, 1 laptop, 212.6ms\n",
      "Speed: 1.8ms preprocess, 212.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.10577392578125, 162.14242553710938, 466.5347900390625, 479.4363098144531], [0.354888916015625, 292.2021179199219, 96.35205841064453, 479.2763977050781], [29.754056930541992, 359.35626220703125, 64.1572265625, 381.81243896484375], [86.45797729492188, 360.12078857421875, 174.79974365234375, 425.54986572265625], [372.0675048828125, 373.1399230957031, 450.93670654296875, 444.2116394042969], [462.07867431640625, 413.6049499511719, 519.0311889648438, 464.7897033691406], [157.85232543945312, 353.79547119140625, 218.21774291992188, 384.1463623046875], [460.7958984375, 413.28271484375, 544.058349609375, 479.6727294921875]], 'scores': [0.9517549276351929, 0.9292327761650085, 0.7263145446777344, 0.6777794361114502, 0.5068066716194153, 0.3676307201385498, 0.2630421221256256, 0.25945964455604553], 'class_indices': [0.0, 0.0, 0.0, 63.0, 62.0, 56.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 1 tv, 1 laptop, 212.2ms\n",
      "Speed: 1.8ms preprocess, 212.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.57313537597656, 162.19393920898438, 466.8790283203125, 479.4712829589844], [0.20171356201171875, 292.13458251953125, 96.96554565429688, 479.48699951171875], [29.148893356323242, 359.4389343261719, 63.34088134765625, 381.8794250488281], [86.64672088623047, 359.9669189453125, 173.35723876953125, 424.9554443359375], [462.05487060546875, 413.5536193847656, 518.8261108398438, 464.1645202636719], [157.7091064453125, 352.97137451171875, 218.96832275390625, 384.31585693359375], [371.59771728515625, 373.0030822753906, 451.16094970703125, 443.8512268066406]], 'scores': [0.95337975025177, 0.9069855809211731, 0.7689709663391113, 0.59063321352005, 0.4932762682437897, 0.48525965213775635, 0.47915998101234436], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 215.5ms\n",
      "Speed: 2.1ms preprocess, 215.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.69378662109375, 161.7926483154297, 466.495849609375, 479.49713134765625], [0.27120208740234375, 292.13128662109375, 96.10094451904297, 479.52734375], [29.48370361328125, 359.52166748046875, 63.61212158203125, 381.83270263671875], [196.47711181640625, 432.72149658203125, 263.89013671875, 479.72650146484375], [371.9140625, 372.740234375, 451.001220703125, 444.9244384765625], [461.34466552734375, 413.342529296875, 518.6964721679688, 464.2608642578125], [86.83704376220703, 359.65643310546875, 173.4776611328125, 425.05474853515625], [86.71414184570312, 359.6322021484375, 173.49606323242188, 425.4058837890625], [157.98190307617188, 350.8240966796875, 218.67416381835938, 384.178466796875]], 'scores': [0.9497612118721008, 0.9156192541122437, 0.6966820359230042, 0.5285438895225525, 0.49405696988105774, 0.48635080456733704, 0.41963866353034973, 0.37031880021095276, 0.29640039801597595], 'class_indices': [0.0, 0.0, 0.0, 27.0, 62.0, 56.0, 62.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 196.9ms\n",
      "Speed: 2.5ms preprocess, 196.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.88789367675781, 162.411865234375, 466.20745849609375, 479.4561767578125], [0.184783935546875, 292.3218994140625, 95.77830505371094, 479.86151123046875], [29.9825439453125, 359.754150390625, 65.23028564453125, 381.52978515625], [371.39080810546875, 372.84124755859375, 451.30047607421875, 444.40399169921875], [461.449951171875, 413.33349609375, 518.7485961914062, 464.6195068359375], [86.90623474121094, 359.8454284667969, 173.55894470214844, 425.8613586425781], [86.78104400634766, 359.82940673828125, 173.50186157226562, 425.82830810546875], [158.10592651367188, 351.8199768066406, 218.99490356445312, 384.3645324707031]], 'scores': [0.9479072093963623, 0.9181711077690125, 0.7062188982963562, 0.527778148651123, 0.4728270471096039, 0.45838114619255066, 0.4323028326034546, 0.32533809542655945], 'class_indices': [0.0, 0.0, 0.0, 62.0, 56.0, 62.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 193.3ms\n",
      "Speed: 2.2ms preprocess, 193.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.12602233886719, 161.80357360839844, 466.04620361328125, 479.4359130859375], [0.3283843994140625, 291.47906494140625, 96.62995910644531, 479.26800537109375], [30.491165161132812, 359.390625, 65.39862060546875, 381.72076416015625], [86.68072509765625, 359.66888427734375, 172.92861938476562, 424.76800537109375], [371.3118896484375, 372.5428466796875, 451.5264892578125, 444.601806640625], [461.39825439453125, 413.0621337890625, 519.0633544921875, 464.2032470703125], [86.81033325195312, 359.7281799316406, 173.20028686523438, 424.7726135253906], [158.08206176757812, 352.1058349609375, 218.72048950195312, 384.1585693359375]], 'scores': [0.9484331011772156, 0.9257676005363464, 0.6787263751029968, 0.4532240629196167, 0.4398556649684906, 0.4359130263328552, 0.4155580997467041, 0.37272605299949646], 'class_indices': [0.0, 0.0, 0.0, 63.0, 62.0, 56.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 196.3ms\n",
      "Speed: 1.9ms preprocess, 196.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.84190368652344, 162.1875762939453, 465.6436767578125, 479.43914794921875], [0.22682952880859375, 292.3798522949219, 95.98783111572266, 479.3954772949219], [30.605735778808594, 360.16571044921875, 66.31253051757812, 382.77618408203125], [370.87744140625, 372.901611328125, 451.0831298828125, 444.9774169921875], [461.84375, 413.68194580078125, 519.0728149414062, 464.43853759765625], [157.54806518554688, 352.464599609375, 218.96835327148438, 384.55419921875], [86.7652816772461, 359.807373046875, 173.07260131835938, 425.1817626953125], [86.64715576171875, 359.7792053222656, 172.73236083984375, 424.9759216308594], [195.73846435546875, 434.42596435546875, 264.1095275878906, 479.75384521484375]], 'scores': [0.9507874250411987, 0.9103910326957703, 0.6980475187301636, 0.6061248779296875, 0.48949727416038513, 0.47934016585350037, 0.4475201666355133, 0.34445205330848694, 0.26275092363357544], 'class_indices': [0.0, 0.0, 0.0, 62.0, 56.0, 0.0, 62.0, 63.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 chair, 2 tvs, 204.7ms\n",
      "Speed: 1.8ms preprocess, 204.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.88362121582031, 162.4794921875, 465.11505126953125, 479.4259033203125], [0.3266448974609375, 292.2522277832031, 96.30279541015625, 479.2041320800781], [370.86566162109375, 372.9988098144531, 451.05767822265625, 444.8218688964844], [30.84844207763672, 360.10528564453125, 65.99365997314453, 382.48541259765625], [86.67947387695312, 359.89874267578125, 173.08953857421875, 424.67462158203125], [461.55499267578125, 413.6142883300781, 519.2719116210938, 464.4562072753906], [157.72030639648438, 351.90655517578125, 218.9444580078125, 384.6898193359375], [31.10816192626953, 359.6007080078125, 84.06185913085938, 403.25518798828125]], 'scores': [0.9496411681175232, 0.9191550612449646, 0.6193953156471252, 0.550345778465271, 0.45470908284187317, 0.3601404130458832, 0.3411791920661926, 0.2827436923980713], 'class_indices': [0.0, 0.0, 62.0, 0.0, 62.0, 56.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 2 chairs, 2 tvs, 212.5ms\n",
      "Speed: 1.8ms preprocess, 212.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.159454345703125, 160.6752471923828, 465.9667663574219, 479.52618408203125], [0.1918182373046875, 291.7962951660156, 94.89350128173828, 479.7801208496094], [370.77099609375, 373.00634765625, 450.90753173828125, 444.587158203125], [31.598350524902344, 359.9886474609375, 66.99317169189453, 382.616943359375], [86.9402847290039, 359.69244384765625, 173.96310424804688, 425.48138427734375], [156.51295471191406, 352.73809814453125, 218.65171813964844, 384.49346923828125], [461.8668212890625, 413.44061279296875, 518.864501953125, 464.32525634765625], [31.641372680664062, 359.3619689941406, 84.35533905029297, 403.6187438964844], [460.07470703125, 413.11029052734375, 538.1555786132812, 479.74951171875]], 'scores': [0.9471883773803711, 0.9116421341896057, 0.6294065117835999, 0.5549524426460266, 0.47057482600212097, 0.3849983215332031, 0.33417001366615295, 0.30161699652671814, 0.25841498374938965], 'class_indices': [0.0, 0.0, 62.0, 0.0, 62.0, 0.0, 56.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 chair, 1 tv, 1 laptop, 207.2ms\n",
      "Speed: 1.7ms preprocess, 207.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.71537780761719, 161.89561462402344, 464.93951416015625, 479.4630126953125], [0.35907745361328125, 292.402587890625, 95.92289733886719, 479.4959716796875], [370.43365478515625, 373.1387939453125, 451.09326171875, 445.3211669921875], [31.836936950683594, 360.09234619140625, 67.8453140258789, 383.45562744140625], [86.61367797851562, 359.9521789550781, 174.66900634765625, 424.8952941894531], [154.9031524658203, 354.9460754394531, 217.8675994873047, 385.1236877441406], [462.00762939453125, 413.5302734375, 518.7772216796875, 464.4473876953125], [32.010902404785156, 359.538330078125, 83.95918273925781, 403.37933349609375]], 'scores': [0.9494661092758179, 0.9212431311607361, 0.6129021644592285, 0.551970362663269, 0.48772892355918884, 0.43831002712249756, 0.36920779943466187, 0.2969115972518921], 'class_indices': [0.0, 0.0, 62.0, 0.0, 63.0, 0.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 chair, 1 tv, 1 laptop, 201.3ms\n",
      "Speed: 1.7ms preprocess, 201.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.02415466308594, 159.81997680664062, 465.6846923828125, 479.5415344238281], [0.18927764892578125, 291.911865234375, 95.37650299072266, 479.54327392578125], [369.6395263671875, 372.5760498046875, 451.04345703125, 444.803955078125], [33.244300842285156, 359.4321594238281, 84.41968536376953, 404.6742248535156], [155.3153839111328, 354.4208984375, 220.4354705810547, 384.8988037109375], [86.58807373046875, 359.60540771484375, 173.56991577148438, 424.76324462890625], [33.370201110839844, 359.9070739746094, 69.20738983154297, 384.1127624511719], [459.6385498046875, 413.0772705078125, 537.7010498046875, 479.7198486328125]], 'scores': [0.9493585228919983, 0.9121760129928589, 0.5581468343734741, 0.4505540430545807, 0.4178280234336853, 0.3820275366306305, 0.3592653274536133, 0.2596932351589203], 'class_indices': [0.0, 0.0, 62.0, 0.0, 0.0, 63.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 chair, 2 tvs, 1 laptop, 214.6ms\n",
      "Speed: 1.6ms preprocess, 214.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.08918762207031, 160.046142578125, 465.73028564453125, 479.4483642578125], [0.3156280517578125, 292.4300537109375, 94.54795837402344, 479.26123046875], [369.7840270996094, 373.1964111328125, 451.0699157714844, 444.6917724609375], [33.40843200683594, 360.17449951171875, 69.89427947998047, 384.321533203125], [33.14264678955078, 359.9973449707031, 82.95536804199219, 403.8534851074219], [462.0374755859375, 413.82452392578125, 519.0015869140625, 464.64971923828125], [86.96356964111328, 360.07647705078125, 177.989501953125, 426.13525390625], [86.80522155761719, 360.1318359375, 177.6706085205078, 425.957275390625], [158.28277587890625, 356.3603515625, 222.128173828125, 384.88134765625]], 'scores': [0.9497115015983582, 0.91162109375, 0.5457985401153564, 0.4598919749259949, 0.39051488041877747, 0.35602715611457825, 0.32330623269081116, 0.2844693958759308, 0.2818250358104706], 'class_indices': [0.0, 0.0, 62.0, 0.0, 0.0, 56.0, 62.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 204.1ms\n",
      "Speed: 2.2ms preprocess, 204.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.16349792480469, 158.32284545898438, 466.53350830078125, 479.4216613769531], [0.34148406982421875, 291.71282958984375, 95.35370635986328, 479.454345703125], [369.6402587890625, 372.6153564453125, 451.2880859375, 443.901611328125], [33.054664611816406, 359.25921630859375, 73.65678405761719, 385.93365478515625], [86.31844329833984, 359.6824951171875, 182.55416870117188, 426.0914306640625], [461.42755126953125, 413.235595703125, 519.256103515625, 464.328857421875], [259.0235595703125, 431.89990234375, 288.414306640625, 479.847412109375], [86.65869903564453, 359.60601806640625, 189.1739501953125, 425.88983154296875], [32.890235900878906, 359.20355224609375, 83.08618927001953, 401.87005615234375]], 'scores': [0.9497613906860352, 0.9223455190658569, 0.5371601581573486, 0.48030227422714233, 0.3814139664173126, 0.36848804354667664, 0.3681398928165436, 0.339925616979599, 0.2951197028160095], 'class_indices': [0.0, 0.0, 62.0, 0.0, 63.0, 56.0, 27.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 tie, 1 chair, 2 tvs, 211.6ms\n",
      "Speed: 1.8ms preprocess, 211.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.11529541015625, 157.96144104003906, 466.63848876953125, 479.436767578125], [0.34767913818359375, 292.02850341796875, 95.82051086425781, 479.45025634765625], [369.3443603515625, 372.84295654296875, 450.95599365234375, 443.82208251953125], [259.3529052734375, 431.6723327636719, 289.6494140625, 479.7975158691406], [32.496185302734375, 359.51666259765625, 73.72642517089844, 385.42767333984375], [462.12933349609375, 413.47930908203125, 518.8258666992188, 464.29833984375], [87.05667114257812, 359.67987060546875, 187.81500244140625, 424.974853515625], [32.41687774658203, 359.45538330078125, 85.0204086303711, 403.84881591796875], [165.61175537109375, 355.7712097167969, 224.24371337890625, 384.2624816894531]], 'scores': [0.9491447806358337, 0.9240275025367737, 0.5522706508636475, 0.47895076870918274, 0.44845205545425415, 0.4266871213912964, 0.4044118821620941, 0.31837132573127747, 0.2502897083759308], 'class_indices': [0.0, 0.0, 62.0, 27.0, 0.0, 56.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 2 tvs, 1 laptop, 210.5ms\n",
      "Speed: 2.0ms preprocess, 210.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.80377197265625, 157.42355346679688, 466.70440673828125, 479.4615783691406], [0.385284423828125, 292.1170654296875, 96.14814758300781, 479.5240478515625], [369.06622314453125, 373.0794372558594, 451.10467529296875, 443.5491027832031], [461.45758056640625, 413.4930725097656, 518.798095703125, 464.6485900878906], [86.45030212402344, 359.5701904296875, 184.6457061767578, 425.279296875], [30.899198532104492, 359.35552978515625, 84.8087158203125, 404.11993408203125], [86.71542358398438, 359.474365234375, 184.28427124023438, 425.0408935546875]], 'scores': [0.9499204754829407, 0.9215933680534363, 0.5518845915794373, 0.5345779061317444, 0.4549192786216736, 0.45329076051712036, 0.35040098428726196], 'class_indices': [0.0, 0.0, 62.0, 56.0, 63.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 chair, 2 tvs, 1 laptop, 196.5ms\n",
      "Speed: 2.4ms preprocess, 196.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.8814697265625, 156.6738739013672, 466.6080322265625, 479.49468994140625], [0.17010498046875, 292.16461181640625, 95.43167114257812, 478.55267333984375], [174.43218994140625, 314.0832824707031, 224.586181640625, 384.5133972167969], [461.621337890625, 413.5643310546875, 518.7135009765625, 464.6220703125], [369.1495361328125, 372.89898681640625, 451.199951171875, 443.70074462890625], [86.62191009521484, 359.62164306640625, 188.76980590820312, 425.65838623046875], [30.05560302734375, 360.0819091796875, 72.08141326904297, 384.9659423828125], [30.38541603088379, 359.8890380859375, 85.52286529541016, 405.3060302734375], [86.33941650390625, 359.549072265625, 187.73110961914062, 425.4937744140625]], 'scores': [0.9504253268241882, 0.8679127097129822, 0.5422810316085815, 0.5110898613929749, 0.5109226107597351, 0.388349324464798, 0.31512516736984253, 0.30706432461738586, 0.25956782698631287], 'class_indices': [0.0, 0.0, 0.0, 56.0, 62.0, 62.0, 0.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 chair, 2 tvs, 1 laptop, 192.6ms\n",
      "Speed: 1.5ms preprocess, 192.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.00212097167969, 157.31546020507812, 466.77252197265625, 479.4712219238281], [0.3370361328125, 292.1575622558594, 96.16642761230469, 478.7134704589844], [180.14486694335938, 307.420166015625, 224.27615356445312, 384.29730224609375], [369.54486083984375, 372.93695068359375, 451.0682373046875, 443.60955810546875], [86.69000244140625, 359.63201904296875, 192.622802734375, 425.81268310546875], [461.62310791015625, 413.4566650390625, 519.1967163085938, 464.79144287109375], [86.93447875976562, 359.44842529296875, 194.60775756835938, 426.22003173828125], [31.41203498840332, 359.99853515625, 83.60688781738281, 403.7225341796875], [31.365447998046875, 360.0653076171875, 72.559326171875, 385.544677734375]], 'scores': [0.9509517550468445, 0.905695915222168, 0.5977150797843933, 0.546195924282074, 0.48191264271736145, 0.41490811109542847, 0.38870322704315186, 0.36182862520217896, 0.3529144823551178], 'class_indices': [0.0, 0.0, 0.0, 62.0, 63.0, 56.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 196.4ms\n",
      "Speed: 1.6ms preprocess, 196.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[52.97572326660156, 157.2130889892578, 467.072265625, 479.49664306640625], [0.22259521484375, 291.52581787109375, 95.8025131225586, 468.82684326171875], [369.1373291015625, 372.739990234375, 451.0887451171875, 443.1824951171875], [461.62432861328125, 413.4352722167969, 518.8203735351562, 464.2699279785156], [31.038583755493164, 359.6158447265625, 73.58557891845703, 385.7667236328125], [87.01936340332031, 359.2027587890625, 200.13563537597656, 425.0382080078125], [86.86429595947266, 359.32098388671875, 199.73153686523438, 424.711669921875], [31.269559860229492, 359.385498046875, 84.75154113769531, 404.826171875]], 'scores': [0.950223445892334, 0.7344489693641663, 0.5155313014984131, 0.49523866176605225, 0.4589010179042816, 0.4162508249282837, 0.37621909379959106, 0.29902514815330505], 'class_indices': [0.0, 0.0, 62.0, 56.0, 0.0, 62.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 196.3ms\n",
      "Speed: 1.8ms preprocess, 196.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.40608215332031, 157.19598388671875, 466.78216552734375, 479.43450927734375], [0.32477569580078125, 291.7823486328125, 97.16859436035156, 478.9952392578125], [369.604248046875, 372.83489990234375, 450.955078125, 443.67523193359375], [30.728927612304688, 358.94091796875, 85.4517822265625, 405.2974853515625], [86.80783081054688, 359.32061767578125, 204.8609619140625, 424.31005859375], [461.85601806640625, 413.4876708984375, 518.9172973632812, 464.40625], [86.95870971679688, 359.1658630371094, 205.45614624023438, 424.6355895996094], [30.92510986328125, 359.2835693359375, 73.07757568359375, 385.4742431640625]], 'scores': [0.949840784072876, 0.9131563901901245, 0.5500286817550659, 0.5484704971313477, 0.46101415157318115, 0.42853373289108276, 0.4045703709125519, 0.25058725476264954], 'class_indices': [0.0, 0.0, 62.0, 0.0, 63.0, 56.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 1 laptop, 196.1ms\n",
      "Speed: 2.3ms preprocess, 196.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.75312805175781, 157.40789794921875, 467.07867431640625, 479.49359130859375], [0.33307647705078125, 292.07708740234375, 96.40027618408203, 479.19207763671875], [86.90292358398438, 358.88275146484375, 205.85177612304688, 425.24578857421875], [369.50067138671875, 372.99951171875, 450.99786376953125, 442.57421875], [31.35967445373535, 359.58984375, 74.1280746459961, 386.9041748046875], [461.7308349609375, 413.62933349609375, 518.7920532226562, 464.5721435546875]], 'scores': [0.9502421021461487, 0.9265871047973633, 0.601416826248169, 0.5774939656257629, 0.4995618164539337, 0.4966645836830139], 'class_indices': [0.0, 0.0, 63.0, 62.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 198.8ms\n",
      "Speed: 1.7ms preprocess, 198.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.26806640625, 157.45889282226562, 466.9658203125, 479.4581604003906], [86.65206909179688, 358.88787841796875, 205.1063232421875, 424.54827880859375], [0.2915496826171875, 291.8658447265625, 96.64230346679688, 478.776611328125], [369.8341064453125, 372.6539306640625, 451.0811767578125, 442.279296875], [461.9078369140625, 413.6096496582031, 519.1263427734375, 464.3680114746094], [31.93677520751953, 358.89739990234375, 84.63041687011719, 404.19329833984375], [261.2658386230469, 431.6541748046875, 291.1922912597656, 479.6878662109375]], 'scores': [0.9490997791290283, 0.7676510810852051, 0.7490692734718323, 0.5380696058273315, 0.5092118978500366, 0.3940562605857849, 0.2718808650970459], 'class_indices': [0.0, 63.0, 0.0, 62.0, 56.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 2 tvs, 1 laptop, 198.9ms\n",
      "Speed: 2.0ms preprocess, 198.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.48834228515625, 157.05628967285156, 467.22998046875, 479.4720458984375], [0.32989501953125, 291.8316650390625, 96.72364044189453, 479.439453125], [369.7427978515625, 372.7612609863281, 451.18603515625, 442.7912902832031], [86.88916015625, 358.85321044921875, 205.12298583984375, 424.65869140625], [33.14196014404297, 359.01690673828125, 74.38524627685547, 386.38458251953125], [461.73443603515625, 413.329345703125, 518.758544921875, 464.3199462890625], [86.94178009033203, 358.81201171875, 205.4974365234375, 424.6429443359375]], 'scores': [0.952403724193573, 0.9237028956413269, 0.5506668090820312, 0.4542635977268219, 0.44564464688301086, 0.4307093918323517, 0.35483598709106445], 'class_indices': [0.0, 0.0, 62.0, 63.0, 0.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 1 tv, 1 laptop, 196.3ms\n",
      "Speed: 1.8ms preprocess, 196.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.84248352050781, 157.4193115234375, 467.49822998046875, 479.47259521484375], [0.34232330322265625, 292.01251220703125, 95.7110366821289, 479.42852783203125], [86.89178466796875, 358.8291320800781, 205.3175048828125, 425.1473693847656], [369.82489013671875, 372.9088134765625, 451.09039306640625, 441.61468505859375], [462.337890625, 413.43341064453125, 519.0198974609375, 464.46466064453125], [32.024169921875, 358.99505615234375, 84.85578155517578, 404.77044677734375], [32.33903503417969, 359.31494140625, 74.87773132324219, 387.77490234375]], 'scores': [0.9530593156814575, 0.9147031307220459, 0.6300631761550903, 0.5370975732803345, 0.46620669960975647, 0.46581193804740906, 0.2609352469444275], 'class_indices': [0.0, 0.0, 63.0, 62.0, 56.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 chair, 1 tv, 1 laptop, 195.6ms\n",
      "Speed: 1.9ms preprocess, 195.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.40165710449219, 157.39404296875, 467.18829345703125, 479.44122314453125], [0.3439788818359375, 291.87384033203125, 96.5936508178711, 479.45208740234375], [86.85549926757812, 358.7125244140625, 205.14956665039062, 424.9158935546875], [461.7503662109375, 413.22161865234375, 518.78564453125, 464.05804443359375], [369.5374755859375, 372.766845703125, 451.25, 442.46783447265625], [34.08934783935547, 358.8695068359375, 74.4460678100586, 386.35009765625], [201.8079833984375, 432.7587890625, 268.75433349609375, 479.75665283203125]], 'scores': [0.9513339996337891, 0.9143424034118652, 0.6372358202934265, 0.5647974014282227, 0.5229557156562805, 0.45019766688346863, 0.38098379969596863], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 1 laptop, 192.3ms\n",
      "Speed: 2.2ms preprocess, 192.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.70375061035156, 157.83050537109375, 467.023193359375, 479.4576416015625], [0.32269287109375, 291.5656433105469, 96.66090393066406, 479.4689025878906], [369.32110595703125, 372.9410400390625, 451.10296630859375, 442.48968505859375], [86.94609832763672, 358.754638671875, 205.85498046875, 425.2392578125], [461.46832275390625, 413.58514404296875, 519.06640625, 464.47747802734375], [34.296974182128906, 358.9608154296875, 74.6993179321289, 386.331787109375]], 'scores': [0.9497074484825134, 0.9018567800521851, 0.5522751212120056, 0.5223214030265808, 0.46367645263671875, 0.4185580015182495], 'class_indices': [0.0, 0.0, 62.0, 63.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 201.2ms\n",
      "Speed: 1.9ms preprocess, 201.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.673675537109375, 157.82394409179688, 467.1383361816406, 479.4523010253906], [0.235931396484375, 291.48297119140625, 95.92115783691406, 479.39251708984375], [369.4613037109375, 372.9788818359375, 451.11566162109375, 443.3350830078125], [86.73849487304688, 359.36956787109375, 205.24185180664062, 425.85260009765625], [461.83160400390625, 413.48406982421875, 518.6386108398438, 464.56365966796875], [86.63041687011719, 359.133544921875, 204.89451599121094, 425.513427734375], [31.713417053222656, 359.2489929199219, 74.83988189697266, 386.8845520019531], [31.6151123046875, 359.06982421875, 85.46876525878906, 403.88909912109375], [261.239501953125, 432.7076721191406, 290.364501953125, 479.7532653808594]], 'scores': [0.9481613039970398, 0.813290536403656, 0.513853132724762, 0.5051079392433167, 0.4375094175338745, 0.41607701778411865, 0.39623138308525085, 0.35029831528663635, 0.30892863869667053], 'class_indices': [0.0, 0.0, 62.0, 62.0, 56.0, 63.0, 0.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 2 tvs, 1 laptop, 197.2ms\n",
      "Speed: 2.2ms preprocess, 197.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.65769958496094, 157.9210662841797, 467.1884765625, 479.46649169921875], [0.22075653076171875, 289.53759765625, 95.50049591064453, 479.09271240234375], [461.50982666015625, 413.2147216796875, 518.9693603515625, 464.273681640625], [369.74298095703125, 372.79632568359375, 451.07073974609375, 442.72991943359375], [86.927978515625, 358.810546875, 205.58950805664062, 426.2017822265625], [33.43772888183594, 358.69158935546875, 84.96652221679688, 404.23748779296875], [86.79315185546875, 359.05865478515625, 205.93759155273438, 425.77642822265625]], 'scores': [0.9499937295913696, 0.8618319630622864, 0.4750211834907532, 0.4731363356113434, 0.3743489682674408, 0.37069588899612427, 0.3550761938095093], 'class_indices': [0.0, 0.0, 56.0, 62.0, 62.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 195.8ms\n",
      "Speed: 1.7ms preprocess, 195.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.94026184082031, 158.0880889892578, 466.39471435546875, 479.46221923828125], [0.358245849609375, 289.3480224609375, 96.71768951416016, 479.4100341796875], [369.20550537109375, 372.8853759765625, 450.91510009765625, 443.28802490234375], [461.75787353515625, 413.5208740234375, 518.7394409179688, 464.3282470703125], [87.09487915039062, 359.2244873046875, 197.40780639648438, 425.5594482421875], [33.4722900390625, 358.82952880859375, 84.87388610839844, 404.96173095703125], [86.87102508544922, 359.4150390625, 195.72637939453125, 425.0335693359375], [33.6436767578125, 359.189697265625, 74.38105773925781, 387.3201904296875]], 'scores': [0.9513636231422424, 0.8812370300292969, 0.563696026802063, 0.4199889302253723, 0.3658232092857361, 0.35887327790260315, 0.3222084939479828, 0.29629719257354736], 'class_indices': [0.0, 0.0, 62.0, 56.0, 62.0, 0.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 2 tvs, 196.8ms\n",
      "Speed: 2.1ms preprocess, 196.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.61480712890625, 157.55575561523438, 466.83477783203125, 479.5052795410156], [0.338348388671875, 289.72161865234375, 96.1092758178711, 479.41473388671875], [369.3212585449219, 372.9897155761719, 451.1394348144531, 444.1415710449219], [461.5523681640625, 413.53216552734375, 518.9476928710938, 464.63763427734375], [33.52061462402344, 358.388427734375, 74.5809555053711, 387.7593994140625], [86.90863037109375, 359.36029052734375, 191.59835815429688, 425.38543701171875]], 'scores': [0.9506095051765442, 0.870127260684967, 0.5723563432693481, 0.4886486530303955, 0.3928230404853821, 0.33486297726631165], 'class_indices': [0.0, 0.0, 62.0, 56.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 2 tvs, 1 laptop, 191.3ms\n",
      "Speed: 2.5ms preprocess, 191.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.75933837890625, 157.71865844726562, 466.3431396484375, 479.5215148925781], [0.35645294189453125, 289.2252197265625, 95.33211517333984, 479.24774169921875], [369.0490417480469, 372.7613525390625, 450.9195861816406, 443.9852294921875], [33.477378845214844, 358.818115234375, 84.95537567138672, 404.72686767578125], [86.42446899414062, 359.33489990234375, 188.74884033203125, 425.16827392578125], [461.38433837890625, 413.44049072265625, 519.0083618164062, 464.08038330078125], [86.64205932617188, 359.19384765625, 188.57583618164062, 425.3026123046875]], 'scores': [0.9499818086624146, 0.8926157355308533, 0.5428904891014099, 0.4243876338005066, 0.4145851731300354, 0.380780965089798, 0.3231566548347473], 'class_indices': [0.0, 0.0, 62.0, 0.0, 63.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 chair, 2 tvs, 196.8ms\n",
      "Speed: 1.8ms preprocess, 196.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.740814208984375, 157.80645751953125, 466.5969543457031, 479.5401611328125], [0.32749176025390625, 290.07269287109375, 96.57847595214844, 479.302978515625], [369.172119140625, 372.95855712890625, 451.1705322265625, 444.04254150390625], [461.95928955078125, 413.4234619140625, 518.7341918945312, 464.6004638671875], [33.119483947753906, 358.7911376953125, 85.65156555175781, 405.36639404296875], [86.79202270507812, 359.31689453125, 188.12039184570312, 425.58648681640625], [180.51092529296875, 352.3824462890625, 223.89361572265625, 384.74755859375], [33.377525329589844, 359.120849609375, 74.8310317993164, 386.37109375]], 'scores': [0.9527519345283508, 0.8655943274497986, 0.5827540159225464, 0.4825696051120758, 0.3794625997543335, 0.3757576644420624, 0.315171480178833, 0.261273592710495], 'class_indices': [0.0, 0.0, 62.0, 56.0, 0.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 chair, 2 tvs, 198.4ms\n",
      "Speed: 1.9ms preprocess, 198.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.03448486328125, 158.2476806640625, 466.27313232421875, 479.47210693359375], [0.33777618408203125, 289.584228515625, 96.94573211669922, 479.2081298828125], [369.0806884765625, 372.87750244140625, 450.9573974609375, 444.55743408203125], [461.70904541015625, 413.4718322753906, 519.0030517578125, 464.3829040527344], [180.43896484375, 354.539306640625, 224.129638671875, 384.57421875], [87.00955200195312, 359.529296875, 189.25958251953125, 425.49737548828125], [33.85614776611328, 358.3653564453125, 86.5772476196289, 404.66461181640625], [33.92584228515625, 358.6675720214844, 75.30412292480469, 387.1265563964844]], 'scores': [0.9485455751419067, 0.8119029998779297, 0.5965807437896729, 0.5326055884361267, 0.4753495156764984, 0.3311106562614441, 0.27380624413490295, 0.2690984904766083], 'class_indices': [0.0, 0.0, 62.0, 56.0, 0.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 198.2ms\n",
      "Speed: 1.9ms preprocess, 198.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.30488586425781, 158.12762451171875, 466.48211669921875, 479.45904541015625], [0.33937835693359375, 289.84088134765625, 96.4034652709961, 479.4212646484375], [369.555419921875, 372.85986328125, 451.1353759765625, 444.377685546875], [179.03955078125, 352.6640930175781, 224.43975830078125, 384.8669738769531], [461.68145751953125, 413.49639892578125, 519.0501708984375, 464.4283447265625], [33.596954345703125, 358.72467041015625, 84.95378112792969, 404.54180908203125], [87.01473236083984, 359.51153564453125, 187.87484741210938, 425.27972412109375], [86.98297119140625, 359.2783203125, 186.59136962890625, 425.4349365234375]], 'scores': [0.9531421661376953, 0.9028310775756836, 0.5943071246147156, 0.4387117028236389, 0.4353085458278656, 0.3826993405818939, 0.3722328245639801, 0.27865535020828247], 'class_indices': [0.0, 0.0, 62.0, 0.0, 56.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 193.6ms\n",
      "Speed: 2.2ms preprocess, 193.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.59837341308594, 158.32479858398438, 466.00537109375, 479.4676818847656], [0.350799560546875, 289.38482666015625, 95.89299011230469, 479.30621337890625], [369.1718444824219, 372.6195068359375, 451.0731506347656, 444.0733642578125], [461.74127197265625, 413.4198913574219, 518.70751953125, 464.0873718261719], [175.13754272460938, 352.4781494140625, 224.61611938476562, 384.82574462890625], [260.50152587890625, 431.510009765625, 291.3883056640625, 479.761474609375], [33.40797424316406, 358.35382080078125, 84.54937744140625, 403.58245849609375], [86.9334945678711, 359.46234130859375, 185.38400268554688, 425.08160400390625], [86.7255859375, 359.5440368652344, 185.07333374023438, 425.2082824707031], [33.63417053222656, 358.5596923828125, 74.60760498046875, 386.7279052734375]], 'scores': [0.9531089067459106, 0.8715996742248535, 0.5936182141304016, 0.5186933279037476, 0.4998030364513397, 0.4250739812850952, 0.3551982641220093, 0.344795823097229, 0.2849127948284149, 0.2663406729698181], 'class_indices': [0.0, 0.0, 62.0, 56.0, 0.0, 27.0, 0.0, 62.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 195.0ms\n",
      "Speed: 1.7ms preprocess, 195.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.71942138671875, 158.39901733398438, 466.1690673828125, 479.4826965332031], [0.36351776123046875, 289.40399169921875, 96.2704849243164, 479.43328857421875], [369.33245849609375, 372.9364013671875, 451.13665771484375, 445.0379638671875], [462.0169677734375, 413.4825134277344, 518.8450317382812, 464.7353820800781], [34.00164794921875, 358.9615478515625, 85.51449584960938, 404.3773193359375], [86.6742172241211, 359.64324951171875, 182.46554565429688, 425.35308837890625], [173.16827392578125, 353.6097717285156, 224.08331298828125, 385.0939636230469], [259.8515930175781, 431.0809326171875, 290.6180114746094, 479.7237548828125]], 'scores': [0.9511984586715698, 0.9171103835105896, 0.5856238007545471, 0.5213711857795715, 0.39417535066604614, 0.3604241907596588, 0.3259321451187134, 0.2602647542953491], 'class_indices': [0.0, 0.0, 62.0, 56.0, 0.0, 62.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 2 chairs, 2 tvs, 1 laptop, 200.2ms\n",
      "Speed: 2.2ms preprocess, 200.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.70166015625, 158.18984985351562, 466.0614013671875, 479.4679260253906], [0.36749267578125, 288.8033447265625, 96.12796783447266, 479.16949462890625], [368.9229736328125, 372.66229248046875, 451.118896484375, 444.89898681640625], [86.5435791015625, 359.6250915527344, 182.24887084960938, 425.0826110839844], [32.673240661621094, 359.1220703125, 84.03559112548828, 404.1800537109375], [86.76481628417969, 359.5718688964844, 182.1649627685547, 425.1911926269531], [462.34808349609375, 413.4787902832031, 518.9246215820312, 464.7098083496094], [459.5482177734375, 413.2265930175781, 538.352783203125, 479.7398986816406], [173.21487426757812, 352.68353271484375, 223.41970825195312, 384.86541748046875]], 'scores': [0.9516190886497498, 0.89045649766922, 0.5751686096191406, 0.40225863456726074, 0.36615511775016785, 0.3176170289516449, 0.3021048605442047, 0.2714425325393677, 0.26911675930023193], 'class_indices': [0.0, 0.0, 62.0, 63.0, 0.0, 62.0, 56.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 213.5ms\n",
      "Speed: 2.0ms preprocess, 213.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.38909912109375, 158.383056640625, 465.9808349609375, 479.4638671875], [0.22808074951171875, 289.7942199707031, 96.44634246826172, 479.3215637207031], [369.0685119628906, 372.71478271484375, 451.0902404785156, 444.7939453125], [86.68672943115234, 359.55712890625, 184.58871459960938, 425.499267578125], [33.112205505371094, 358.9149169921875, 85.14314270019531, 405.2138671875], [461.48828125, 413.35333251953125, 518.9404907226562, 464.15386962890625], [86.87410736083984, 359.1689453125, 183.29885864257812, 425.19482421875], [174.86831665039062, 352.345703125, 223.88632202148438, 384.7618408203125], [33.41356658935547, 359.16961669921875, 74.49870300292969, 386.95062255859375], [199.24258422851562, 432.40625, 267.2067565917969, 479.7674560546875]], 'scores': [0.9512181282043457, 0.8995288014411926, 0.5732409954071045, 0.40193742513656616, 0.38155990839004517, 0.37590736150741577, 0.3421388864517212, 0.2809566855430603, 0.26168352365493774, 0.2562693953514099], 'class_indices': [0.0, 0.0, 62.0, 63.0, 0.0, 56.0, 62.0, 0.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 chair, 2 tvs, 1 laptop, 217.7ms\n",
      "Speed: 1.6ms preprocess, 217.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.926849365234375, 158.59857177734375, 466.0176086425781, 479.4708251953125], [0.36209869384765625, 289.903564453125, 95.55159759521484, 479.49853515625], [368.8573303222656, 372.9758605957031, 451.1872253417969, 444.4320373535156], [461.6292724609375, 413.62310791015625, 519.0697631835938, 464.50482177734375], [174.69859313964844, 352.24041748046875, 224.1914825439453, 384.98529052734375], [32.55908203125, 359.3701477050781, 85.61518096923828, 405.5631408691406], [86.60350036621094, 359.66888427734375, 184.43629455566406, 426.1922607421875], [32.887107849121094, 359.4429931640625, 74.7135238647461, 387.0889892578125], [86.86715698242188, 359.2132568359375, 182.90191650390625, 425.89794921875]], 'scores': [0.9506661891937256, 0.921402633190155, 0.5797020196914673, 0.49290868639945984, 0.4390210211277008, 0.43813464045524597, 0.37261736392974854, 0.3155824840068817, 0.3130490779876709], 'class_indices': [0.0, 0.0, 62.0, 56.0, 0.0, 0.0, 63.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 tie, 1 chair, 2 tvs, 1 laptop, 237.5ms\n",
      "Speed: 2.2ms preprocess, 237.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.07440185546875, 158.0164794921875, 466.818603515625, 479.481689453125], [0.376434326171875, 290.0760498046875, 96.42243957519531, 479.31036376953125], [369.2076416015625, 372.6675720214844, 450.885498046875, 444.0608215332031], [33.2147216796875, 358.88330078125, 85.37716674804688, 404.6005859375], [461.47845458984375, 413.27294921875, 519.128662109375, 464.35455322265625], [86.42581176757812, 359.5068054199219, 183.37860107421875, 425.0583190917969], [174.7790069580078, 352.5992431640625, 224.24693298339844, 384.90771484375], [86.70811462402344, 359.4387512207031, 183.15675354003906, 425.1380920410156], [33.530364990234375, 359.17156982421875, 74.3786849975586, 386.81195068359375], [261.18804931640625, 432.17120361328125, 289.89141845703125, 479.79473876953125]], 'scores': [0.9520639777183533, 0.9244126081466675, 0.5793827176094055, 0.44329407811164856, 0.44007447361946106, 0.38328683376312256, 0.36807864904403687, 0.31343886256217957, 0.2725791931152344, 0.2646709680557251], 'class_indices': [0.0, 0.0, 62.0, 0.0, 56.0, 63.0, 0.0, 62.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 chair, 2 tvs, 194.3ms\n",
      "Speed: 2.0ms preprocess, 194.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.75042724609375, 158.34228515625, 466.4632568359375, 479.471435546875], [0.3428802490234375, 290.5170593261719, 94.93785858154297, 479.4111022949219], [369.294189453125, 372.98358154296875, 451.20556640625, 444.08038330078125], [462.0654296875, 413.73748779296875, 518.9766845703125, 465.02740478515625], [32.753700256347656, 358.7160949707031, 85.05319213867188, 404.7353820800781], [86.76231384277344, 359.8005676269531, 185.05613708496094, 426.7372741699219], [198.61276245117188, 432.3497009277344, 267.4291687011719, 479.7621154785156], [174.8348388671875, 351.1833190917969, 224.1964111328125, 384.8800354003906]], 'scores': [0.952390730381012, 0.9174796342849731, 0.604114294052124, 0.4844936728477478, 0.45716437697410583, 0.31844621896743774, 0.25731080770492554, 0.25031036138534546], 'class_indices': [0.0, 0.0, 62.0, 56.0, 0.0, 62.0, 27.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 2 tvs, 210.9ms\n",
      "Speed: 1.7ms preprocess, 210.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.554931640625, 158.23867797851562, 466.56048583984375, 479.4734802246094], [0.39603424072265625, 290.46673583984375, 96.0842514038086, 479.50531005859375], [369.45477294921875, 372.725341796875, 451.156982421875, 443.679443359375], [32.62554168701172, 358.70562744140625, 84.46819305419922, 404.98846435546875], [461.91259765625, 413.6031494140625, 518.9503173828125, 464.429931640625], [86.91778564453125, 359.337890625, 187.3265380859375, 425.0927734375]], 'scores': [0.9520907402038574, 0.9246724247932434, 0.5834878087043762, 0.5088601112365723, 0.40151476860046387, 0.3150823712348938], 'class_indices': [0.0, 0.0, 62.0, 0.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 212.3ms\n",
      "Speed: 2.1ms preprocess, 212.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.37568664550781, 158.22369384765625, 466.07916259765625, 479.47979736328125], [0.2222747802734375, 290.3482360839844, 96.74738311767578, 479.3921813964844], [369.12152099609375, 372.84014892578125, 451.132568359375, 444.57049560546875], [461.45098876953125, 413.4245300292969, 518.91943359375, 464.6426086425781], [33.528900146484375, 358.9266357421875, 74.48278045654297, 387.5528564453125], [86.8738021850586, 359.1712951660156, 192.27572631835938, 425.3190612792969], [86.722412109375, 359.24761962890625, 191.6005859375, 425.01812744140625], [33.50904846191406, 358.7776184082031, 84.75834655761719, 404.6856994628906]], 'scores': [0.9495628476142883, 0.8559511303901672, 0.5389137864112854, 0.4497973918914795, 0.3896833062171936, 0.3749321699142456, 0.3585289418697357, 0.32734379172325134], 'class_indices': [0.0, 0.0, 62.0, 56.0, 0.0, 62.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 211.2ms\n",
      "Speed: 2.0ms preprocess, 211.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.03498840332031, 158.35154724121094, 466.01300048828125, 479.5084228515625], [0.35749053955078125, 290.2188720703125, 95.88188171386719, 479.301025390625], [368.989501953125, 373.0042724609375, 451.095458984375, 443.7232666015625], [461.73358154296875, 413.666748046875, 519.1255493164062, 464.583984375], [33.68244171142578, 359.31915283203125, 83.33195495605469, 403.15521240234375], [86.78643798828125, 359.2919921875, 198.87869262695312, 426.0113525390625], [33.458824157714844, 359.26904296875, 74.5624008178711, 387.2706298828125]], 'scores': [0.9501487016677856, 0.9177594780921936, 0.5962955951690674, 0.5501124858856201, 0.4357132315635681, 0.42229312658309937, 0.31961268186569214], 'class_indices': [0.0, 0.0, 62.0, 56.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 214.0ms\n",
      "Speed: 2.3ms preprocess, 214.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.99632263183594, 158.0876007080078, 467.1075439453125, 479.45806884765625], [0.3462066650390625, 290.15130615234375, 96.51595306396484, 479.3077392578125], [369.91510009765625, 372.66473388671875, 451.0146484375, 442.622802734375], [86.78607177734375, 359.3265075683594, 205.06597900390625, 425.8256530761719], [461.79718017578125, 413.41424560546875, 518.8644409179688, 464.54974365234375], [33.06343078613281, 359.075927734375, 74.37225341796875, 387.40478515625], [33.072669982910156, 358.9495849609375, 85.02120208740234, 405.479248046875]], 'scores': [0.9485998749732971, 0.8917438983917236, 0.5356597304344177, 0.46273550391197205, 0.4284106492996216, 0.40283697843551636, 0.30619511008262634], 'class_indices': [0.0, 0.0, 62.0, 62.0, 56.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 216.5ms\n",
      "Speed: 1.8ms preprocess, 216.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[56.74156188964844, 158.7176055908203, 467.8927001953125, 479.44708251953125], [0.362823486328125, 289.8646240234375, 96.23027801513672, 479.3922119140625], [371.20068359375, 372.7218017578125, 451.15185546875, 441.4337158203125], [462.05181884765625, 413.39227294921875, 518.9532470703125, 464.19549560546875], [86.94527435302734, 358.59063720703125, 205.63021850585938, 426.66412353515625], [32.661766052246094, 358.76751708984375, 85.314208984375, 403.96356201171875], [32.86286163330078, 359.05694580078125, 74.80541229248047, 386.915283203125], [86.90027618408203, 358.91094970703125, 205.524169921875, 426.19598388671875]], 'scores': [0.9511620402336121, 0.9139108061790466, 0.5830447673797607, 0.4869295358657837, 0.40573176741600037, 0.3974929749965668, 0.3729589581489563, 0.36387333273887634], 'class_indices': [0.0, 0.0, 62.0, 56.0, 62.0, 0.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 206.8ms\n",
      "Speed: 2.0ms preprocess, 206.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[58.79924011230469, 159.33676147460938, 471.05999755859375, 479.4508972167969], [0.2675018310546875, 290.046875, 97.95855712890625, 479.0006103515625], [376.05535888671875, 372.7325744628906, 450.95867919921875, 426.0680847167969], [33.30560302734375, 359.2488098144531, 74.2520751953125, 385.8837585449219], [86.68951416015625, 358.94439697265625, 205.52392578125, 426.66497802734375], [86.57693481445312, 358.8065185546875, 205.09933471679688, 426.808349609375], [461.93017578125, 413.3609924316406, 518.98486328125, 464.0554504394531], [33.169090270996094, 359.0333251953125, 85.4911880493164, 403.7391357421875]], 'scores': [0.9484980702400208, 0.865532636642456, 0.561747133731842, 0.48008280992507935, 0.4513498544692993, 0.39531901478767395, 0.2867327928543091, 0.28604379296302795], 'class_indices': [0.0, 0.0, 62.0, 0.0, 62.0, 63.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 chair, 1 tv, 2 laptops, 192.8ms\n",
      "Speed: 1.6ms preprocess, 192.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[65.93754577636719, 160.1036376953125, 479.96954345703125, 479.4786376953125], [0.38410186767578125, 289.832275390625, 97.3332290649414, 479.43658447265625], [86.44485473632812, 358.7021484375, 205.02896118164062, 429.49462890625], [465.796875, 413.470947265625, 518.7654418945312, 463.9241943359375], [33.499420166015625, 359.1440124511719, 74.2550048828125, 386.6990051269531], [86.56611633300781, 358.732666015625, 205.55934143066406, 429.203125], [214.48907470703125, 432.3208312988281, 283.2825927734375, 479.7637634277344], [390.8048095703125, 372.7654724121094, 450.947265625, 410.3612365722656]], 'scores': [0.9478685259819031, 0.9118748903274536, 0.5234443545341492, 0.4694960415363312, 0.4437863826751709, 0.40460440516471863, 0.33134493231773376, 0.3195123076438904], 'class_indices': [0.0, 0.0, 63.0, 56.0, 0.0, 62.0, 27.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 202.7ms\n",
      "Speed: 2.1ms preprocess, 202.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[76.197021484375, 161.1025390625, 488.6416015625, 479.296142578125], [0.396820068359375, 289.522216796875, 105.61492919921875, 478.80133056640625], [86.80054473876953, 358.5548400878906, 205.7330322265625, 434.2161560058594], [33.27796936035156, 359.056884765625, 75.03118896484375, 386.7799072265625], [472.772705078125, 413.30828857421875, 519.0884399414062, 464.5482177734375]], 'scores': [0.95064777135849, 0.8402103185653687, 0.5561540722846985, 0.4133240282535553, 0.27816373109817505], 'class_indices': [0.0, 0.0, 62.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 chair, 1 tv, 1 laptop, 220.1ms\n",
      "Speed: 2.3ms preprocess, 220.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[89.6790771484375, 163.3816680908203, 498.8175048828125, 479.49920654296875], [0.4070587158203125, 289.642333984375, 113.01031494140625, 479.32080078125], [86.5828628540039, 358.38372802734375, 205.10958862304688, 438.16619873046875], [86.60491943359375, 358.327880859375, 205.76522827148438, 438.279541015625], [33.064308166503906, 358.8846435546875, 85.54821014404297, 404.801513671875], [216.52151489257812, 259.1943359375, 250.81869506835938, 376.98431396484375], [477.458251953125, 413.74981689453125, 519.0, 464.96038818359375], [33.323814392089844, 359.0865478515625, 74.6656494140625, 387.35333251953125]], 'scores': [0.9566599726676941, 0.8870532512664795, 0.5022811889648438, 0.45261985063552856, 0.44662362337112427, 0.4115469753742218, 0.39419254660606384, 0.2592647969722748], 'class_indices': [0.0, 0.0, 63.0, 62.0, 0.0, 0.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 laptop, 217.8ms\n",
      "Speed: 2.4ms preprocess, 217.8ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[107.51637268066406, 164.12127685546875, 511.1929931640625, 479.441162109375], [0.4351043701171875, 290.0461120605469, 125.53085327148438, 479.3132019042969], [219.24496459960938, 279.8128662109375, 267.8519592285156, 373.809814453125], [86.56271362304688, 358.50640869140625, 205.78103637695312, 446.75518798828125], [33.141845703125, 359.17681884765625, 74.29530334472656, 386.43243408203125], [33.19007110595703, 358.990234375, 85.23760986328125, 404.164794921875]], 'scores': [0.95766282081604, 0.9085714221000671, 0.6887204051017761, 0.6818947792053223, 0.3904765248298645, 0.36967045068740845], 'class_indices': [0.0, 0.0, 0.0, 63.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 tv, 200.7ms\n",
      "Speed: 1.6ms preprocess, 200.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[112.46688842773438, 165.17242431640625, 510.9800720214844, 479.31536865234375], [0.2452850341796875, 290.0771789550781, 128.92074584960938, 479.4432678222656], [218.22802734375, 261.8011779785156, 272.37384033203125, 374.4148864746094], [86.45176696777344, 358.80535888671875, 205.8112335205078, 450.37725830078125], [33.63872528076172, 359.4713134765625, 74.91078186035156, 387.2081298828125], [33.52618408203125, 359.3884582519531, 84.79002380371094, 403.9956970214844]], 'scores': [0.9507218599319458, 0.9200084805488586, 0.7554774284362793, 0.552650511264801, 0.3889647424221039, 0.3507087528705597], 'class_indices': [0.0, 0.0, 0.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 laptop, 199.8ms\n",
      "Speed: 1.6ms preprocess, 199.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[104.30586242675781, 161.34426879882812, 506.84942626953125, 479.4607849121094], [0.37529754638671875, 289.96881103515625, 121.31450653076172, 479.3388671875], [217.91787719726562, 255.81381225585938, 264.40716552734375, 376.4485168457031], [86.64826965332031, 358.54522705078125, 205.7998504638672, 444.1070556640625], [33.635223388671875, 359.12713623046875, 85.89947509765625, 404.71929931640625], [33.861534118652344, 359.28668212890625, 74.9422607421875, 387.23834228515625]], 'scores': [0.9555727243423462, 0.8905397057533264, 0.7584174871444702, 0.6320985555648804, 0.40288403630256653, 0.3257298767566681], 'class_indices': [0.0, 0.0, 0.0, 63.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 cell phone, 196.3ms\n",
      "Speed: 1.9ms preprocess, 196.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[2.4014892578125, 158.24093627929688, 494.73486328125, 479.3438415527344], [0.24979400634765625, 290.41162109375, 91.60476684570312, 475.9337158203125], [86.71450805664062, 359.8822021484375, 109.16082763671875, 413.058349609375], [34.63105773925781, 359.355712890625, 74.96392059326172, 386.1429443359375], [34.5506591796875, 359.2918395996094, 85.83120727539062, 403.8849792480469]], 'scores': [0.9237849116325378, 0.883213460445404, 0.4827505648136139, 0.40565013885498047, 0.31471389532089233], 'class_indices': [0.0, 0.0, 67.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 laptop, 198.9ms\n",
      "Speed: 1.8ms preprocess, 198.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[25.521697998046875, 159.50201416015625, 479.847900390625, 479.33331298828125], [0.20497894287109375, 290.2539367675781, 81.38416290283203, 461.0513610839844], [466.1197509765625, 413.58319091796875, 518.841552734375, 468.07598876953125], [388.4033203125, 373.06829833984375, 451.72064208984375, 407.2806396484375], [33.70985412597656, 359.29150390625, 75.05943298339844, 385.2257080078125], [473.1351318359375, 465.46478271484375, 552.9302978515625, 479.756591796875]], 'scores': [0.9096923470497131, 0.9063365459442139, 0.5931762456893921, 0.5658575892448425, 0.5494598746299744, 0.31816336512565613], 'class_indices': [0.0, 0.0, 56.0, 63.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 192.4ms\n",
      "Speed: 1.9ms preprocess, 192.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[5.520843505859375, 161.85011291503906, 475.53668212890625, 479.3485107421875], [0.20249176025390625, 290.21405029296875, 78.59581756591797, 459.10125732421875], [464.16510009765625, 413.42108154296875, 518.9953002929688, 468.11065673828125], [386.0572509765625, 373.083740234375, 451.2894287109375, 413.7894287109375], [468.19970703125, 465.181640625, 555.4555053710938, 479.74462890625], [33.748687744140625, 359.3121337890625, 74.877685546875, 385.937744140625]], 'scores': [0.9342542290687561, 0.8825362920761108, 0.6808037757873535, 0.5040709376335144, 0.4434676766395569, 0.405267596244812], 'class_indices': [0.0, 0.0, 56.0, 62.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 1 laptop, 1 cell phone, 200.4ms\n",
      "Speed: 2.0ms preprocess, 200.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[25.720367431640625, 163.318603515625, 478.6306457519531, 479.51214599609375], [0.2236328125, 289.97784423828125, 86.25032043457031, 460.635986328125], [465.85675048828125, 413.29803466796875, 518.8914184570312, 468.14605712890625], [34.198944091796875, 358.89019775390625, 75.38660430908203, 385.41180419921875], [391.20086669921875, 372.437744140625, 451.43157958984375, 410.138427734375], [391.33477783203125, 372.42694091796875, 451.37457275390625, 410.07696533203125], [87.50885009765625, 359.9051513671875, 107.68544006347656, 401.56695556640625], [472.02142333984375, 465.67755126953125, 558.5780639648438, 479.74871826171875]], 'scores': [0.8996593952178955, 0.8625885248184204, 0.6022278666496277, 0.4815339148044586, 0.4046661853790283, 0.36971744894981384, 0.3675777316093445, 0.3570519983768463], 'class_indices': [0.0, 0.0, 56.0, 0.0, 62.0, 63.0, 67.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 2 tvs, 197.2ms\n",
      "Speed: 1.9ms preprocess, 197.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[15.003021240234375, 161.24395751953125, 474.06500244140625, 479.37005615234375], [0.23706817626953125, 290.45843505859375, 80.46871948242188, 459.7022705078125], [464.11920166015625, 413.416748046875, 518.5009155273438, 466.3345947265625], [387.84320068359375, 373.16314697265625, 451.28094482421875, 415.0728759765625], [34.49707794189453, 359.48565673828125, 74.96337127685547, 385.47900390625], [468.4761962890625, 464.5518798828125, 554.6812744140625, 479.753662109375], [162.079345703125, 358.444580078125, 205.8028564453125, 393.627197265625]], 'scores': [0.9130149483680725, 0.8939663171768188, 0.7358683943748474, 0.5265023112297058, 0.38076624274253845, 0.37041598558425903, 0.30374813079833984], 'class_indices': [0.0, 0.0, 56.0, 62.0, 0.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 194.4ms\n",
      "Speed: 1.8ms preprocess, 194.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[6.69757080078125, 161.50436401367188, 475.2919006347656, 479.3531188964844], [0.22389984130859375, 289.3297119140625, 80.61241149902344, 461.057373046875], [464.21368408203125, 413.47796630859375, 518.54150390625, 467.93768310546875], [388.06292724609375, 372.931640625, 451.09002685546875, 414.623291015625], [470.00238037109375, 466.542236328125, 559.1841430664062, 479.7568359375], [33.82171630859375, 359.07855224609375, 74.85667419433594, 385.55657958984375]], 'scores': [0.9311445951461792, 0.8219056129455566, 0.8183652758598328, 0.5486997365951538, 0.42932823300361633, 0.35834911465644836], 'class_indices': [0.0, 0.0, 56.0, 62.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 192.5ms\n",
      "Speed: 1.9ms preprocess, 192.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[3.50921630859375, 163.4935302734375, 480.39190673828125, 479.393798828125], [0.21332550048828125, 289.73883056640625, 75.23345947265625, 459.38800048828125], [466.1807861328125, 413.69744873046875, 518.8795166015625, 468.77593994140625], [391.81097412109375, 373.02056884765625, 451.22943115234375, 406.60247802734375], [475.5121154785156, 466.39508056640625, 559.6051025390625, 479.74005126953125]], 'scores': [0.9359636306762695, 0.6876242160797119, 0.4512379765510559, 0.4277001619338989, 0.29839155077934265], 'class_indices': [0.0, 0.0, 56.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 laptop, 193.5ms\n",
      "Speed: 2.2ms preprocess, 193.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.170318603515625, 165.31405639648438, 485.74261474609375, 479.5005798339844], [0.1776123046875, 290.5849914550781, 55.550045013427734, 455.8675231933594], [470.193359375, 413.5279846191406, 518.612060546875, 469.0140686035156], [393.47119140625, 372.87384033203125, 451.378173828125, 401.60186767578125]], 'scores': [0.9509437680244446, 0.7089717388153076, 0.5675883293151855, 0.5424209237098694], 'class_indices': [0.0, 0.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 laptop, 198.8ms\n",
      "Speed: 1.8ms preprocess, 198.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.204132080078125, 168.5631561279297, 485.7996826171875, 479.46429443359375], [468.41778564453125, 413.40484619140625, 518.77880859375, 468.51104736328125], [391.13348388671875, 372.610107421875, 451.35357666015625, 399.9703369140625], [479.72412109375, 466.80731201171875, 559.5408935546875, 479.76287841796875]], 'scores': [0.9472563862800598, 0.6226531863212585, 0.5186949968338013, 0.3148908019065857], 'class_indices': [0.0, 56.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 2 chairs, 1 tv, 1 laptop, 1 cell phone, 196.5ms\n",
      "Speed: 1.9ms preprocess, 196.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.32421875, 168.9107666015625, 480.5269775390625, 479.5751953125], [466.05975341796875, 413.8800964355469, 518.61962890625, 468.2097473144531], [214.51016235351562, 442.275390625, 282.2516784667969, 479.7340087890625], [382.34771728515625, 372.8665771484375, 451.60028076171875, 407.27569580078125], [146.41561889648438, 340.1927795410156, 186.72689819335938, 372.9774475097656], [474.7873229980469, 466.2506103515625, 559.22509765625, 479.735595703125], [86.63731384277344, 359.3431396484375, 191.9760284423828, 440.5086669921875]], 'scores': [0.9404757022857666, 0.6530057191848755, 0.5538522005081177, 0.3911944627761841, 0.3632102608680725, 0.34411028027534485, 0.25413641333580017], 'class_indices': [0.0, 56.0, 27.0, 63.0, 67.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 2 chairs, 1 tv, 1 laptop, 2 cell phones, 198.2ms\n",
      "Speed: 1.9ms preprocess, 198.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.66033935546875, 171.59979248046875, 476.7142333984375, 479.427734375], [464.2850341796875, 413.5932312011719, 518.754638671875, 468.2837219238281], [143.425537109375, 340.1971130371094, 188.32958984375, 370.7221374511719], [372.99908447265625, 372.90789794921875, 451.00592041015625, 418.5216064453125], [86.6692123413086, 360.06878662109375, 200.23977661132812, 422.20623779296875], [471.47442626953125, 466.27838134765625, 558.6471557617188, 479.75543212890625], [206.81753540039062, 438.21868896484375, 273.3264465332031, 479.741455078125], [373.228271484375, 373.1124572753906, 451.0572509765625, 418.2882995605469]], 'scores': [0.9434391260147095, 0.6375353932380676, 0.5322167277336121, 0.4714275896549225, 0.4219353497028351, 0.4058877229690552, 0.3893991708755493, 0.3679766356945038], 'class_indices': [0.0, 56.0, 67.0, 63.0, 67.0, 56.0, 27.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 laptop, 193.5ms\n",
      "Speed: 1.5ms preprocess, 193.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.682373046875, 172.5894012451172, 477.229248046875, 479.47882080078125], [465.1884765625, 413.45538330078125, 518.7919921875, 468.84967041015625], [372.6792907714844, 372.9327392578125, 451.3529357910156, 412.8836669921875], [473.91717529296875, 466.54852294921875, 558.4909057617188, 479.76153564453125]], 'scores': [0.9478022456169128, 0.6293767094612122, 0.5608599185943604, 0.3570060431957245], 'class_indices': [0.0, 56.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 laptop, 195.3ms\n",
      "Speed: 1.7ms preprocess, 195.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.790771484375, 169.97225952148438, 480.7005615234375, 479.4552307128906], [378.9001770019531, 372.5701599121094, 451.3123474121094, 406.9074401855469], [467.843505859375, 413.65191650390625, 519.2027587890625, 469.09674072265625], [477.06488037109375, 466.4211730957031, 558.9520874023438, 479.7625427246094]], 'scores': [0.9596315622329712, 0.7592163681983948, 0.6707628965377808, 0.30905628204345703], 'class_indices': [0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 1 laptop, 197.3ms\n",
      "Speed: 1.7ms preprocess, 197.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.260528564453125, 168.47711181640625, 490.26763916015625, 479.47308349609375], [389.1014404296875, 372.7076416015625, 451.6500244140625, 402.20947265625], [472.14556884765625, 413.4461975097656, 519.1246948242188, 468.8273620605469]], 'scores': [0.9611872434616089, 0.7129536867141724, 0.5025903582572937], 'class_indices': [0.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 laptop, 195.1ms\n",
      "Speed: 1.8ms preprocess, 195.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.7396240234375, 167.91055297851562, 492.8375244140625, 479.6805114746094], [394.4984130859375, 372.9703369140625, 451.2530517578125, 399.71337890625], [0.13734817504882812, 289.896728515625, 50.58651351928711, 470.5103759765625], [473.6309814453125, 413.59857177734375, 560.2452392578125, 479.69769287109375]], 'scores': [0.9548987746238708, 0.7065008282661438, 0.6049965620040894, 0.29866233468055725], 'class_indices': [0.0, 63.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 194.0ms\n",
      "Speed: 1.7ms preprocess, 194.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.501251220703125, 166.5965576171875, 490.40716552734375, 479.44775390625], [0.19337844848632812, 288.6104736328125, 52.06528091430664, 473.8446044921875], [395.55902099609375, 372.92132568359375, 451.1546630859375, 400.69561767578125], [474.17230224609375, 413.44317626953125, 519.2869262695312, 468.65911865234375], [486.68719482421875, 466.3592529296875, 557.7444458007812, 479.77001953125]], 'scores': [0.9566193222999573, 0.6377691626548767, 0.6164290308952332, 0.6038825511932373, 0.33460089564323425], 'class_indices': [0.0, 0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 3 chairs, 1 tv, 1 laptop, 193.5ms\n",
      "Speed: 1.7ms preprocess, 193.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.65362548828125, 165.0501251220703, 485.39520263671875, 479.40960693359375], [0.15797805786132812, 289.092529296875, 51.06666946411133, 473.4368896484375], [472.63067626953125, 413.6092834472656, 519.2498168945312, 468.8703308105469], [393.66961669921875, 372.5947265625, 451.22393798828125, 402.7457275390625], [393.7177429199219, 372.6673278808594, 451.3038635253906, 402.7724914550781], [484.1658020019531, 466.119140625, 556.968994140625, 479.763427734375], [472.60894775390625, 413.5735168457031, 556.4452514648438, 479.4824523925781]], 'scores': [0.9569026827812195, 0.6472918391227722, 0.5645043253898621, 0.5056576728820801, 0.40934208035469055, 0.34770169854164124, 0.2697538137435913], 'class_indices': [0.0, 0.0, 56.0, 63.0, 62.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 3 chairs, 1 laptop, 193.4ms\n",
      "Speed: 1.5ms preprocess, 193.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.787445068359375, 165.1412353515625, 480.80377197265625, 479.3853759765625], [387.24114990234375, 372.3547058105469, 451.49346923828125, 405.8135070800781], [470.7525634765625, 413.5245361328125, 518.753662109375, 468.2054443359375], [0.168365478515625, 289.9946594238281, 40.86506652832031, 468.0563049316406], [479.1369323730469, 466.18231201171875, 557.072998046875, 479.75531005859375], [470.7686767578125, 413.4972229003906, 556.7894287109375, 479.7250671386719]], 'scores': [0.9596065282821655, 0.6265942454338074, 0.4483626186847687, 0.394015371799469, 0.352399617433548, 0.2879655659198761], 'class_indices': [0.0, 63.0, 56.0, 0.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 laptop, 197.5ms\n",
      "Speed: 2.4ms preprocess, 197.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.26092529296875, 166.4506072998047, 477.4000549316406, 479.42681884765625], [466.4049072265625, 413.3499450683594, 518.950439453125, 467.8521423339844], [379.3414611816406, 372.5920104980469, 451.1031799316406, 412.1393127441406], [473.47119140625, 465.32403564453125, 556.4254150390625, 479.76104736328125]], 'scores': [0.96141517162323, 0.6666966676712036, 0.6045911312103271, 0.37554946541786194], 'class_indices': [0.0, 56.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 laptop, 198.6ms\n",
      "Speed: 1.6ms preprocess, 198.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.2662353515625, 167.52745056152344, 473.86285400390625, 479.3978271484375], [377.2430725097656, 372.52117919921875, 451.0467224121094, 414.48431396484375], [465.01904296875, 413.23626708984375, 518.7156372070312, 467.71722412109375], [467.47698974609375, 464.64337158203125, 555.679931640625, 479.72869873046875]], 'scores': [0.9595767259597778, 0.681888997554779, 0.5952414274215698, 0.2717510759830475], 'class_indices': [0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 laptop, 193.6ms\n",
      "Speed: 1.7ms preprocess, 193.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.21026611328125, 168.27249145507812, 474.8758544921875, 479.4031066894531], [378.820068359375, 372.7347412109375, 451.15185546875, 414.3021240234375], [465.4061279296875, 413.35919189453125, 518.7318725585938, 468.24847412109375], [470.7537536621094, 465.4684143066406, 556.4578857421875, 479.7521667480469]], 'scores': [0.9622775316238403, 0.6665247678756714, 0.623790979385376, 0.27880391478538513], 'class_indices': [0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 195.2ms\n",
      "Speed: 2.1ms preprocess, 195.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.510894775390625, 168.2943115234375, 476.9538269042969, 479.41217041015625], [382.3940124511719, 372.702392578125, 451.1856384277344, 412.1011962890625], [466.84820556640625, 413.15899658203125, 518.9248657226562, 468.20233154296875], [473.6488037109375, 465.7171630859375, 556.666259765625, 479.76043701171875], [0.12100982666015625, 289.5565185546875, 41.01134490966797, 361.8304443359375]], 'scores': [0.9620890021324158, 0.63805091381073, 0.5137242078781128, 0.37620651721954346, 0.33223697543144226], 'class_indices': [0.0, 63.0, 56.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 laptop, 197.6ms\n",
      "Speed: 2.1ms preprocess, 197.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.78167724609375, 167.48626708984375, 478.68853759765625, 479.39935302734375], [386.7286376953125, 372.87286376953125, 451.43359375, 408.3233642578125], [469.3179931640625, 413.89642333984375, 519.2667236328125, 468.51983642578125], [476.1256103515625, 466.2137451171875, 556.8955078125, 479.76318359375]], 'scores': [0.9644728899002075, 0.678923487663269, 0.5448195934295654, 0.37618905305862427], 'class_indices': [0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 tv, 1 laptop, 197.6ms\n",
      "Speed: 2.2ms preprocess, 197.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.237701416015625, 166.57115173339844, 481.0110168457031, 479.44580078125], [469.563720703125, 413.63897705078125, 518.9853515625, 468.49713134765625], [388.60394287109375, 372.6317138671875, 451.1248779296875, 407.488037109375], [388.5701904296875, 372.7880554199219, 451.19189453125, 407.3731384277344], [477.380859375, 465.78521728515625, 556.5032958984375, 479.76080322265625]], 'scores': [0.9662126302719116, 0.5483314990997314, 0.5437352657318115, 0.4340875744819641, 0.4073343276977539], 'class_indices': [0.0, 56.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 tv, 1 laptop, 198.9ms\n",
      "Speed: 1.5ms preprocess, 198.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.2642822265625, 167.3795928955078, 479.29144287109375, 479.43487548828125], [468.27978515625, 413.5023193359375, 518.9948120117188, 468.10205078125], [385.5858154296875, 372.81903076171875, 451.3277587890625, 409.14813232421875], [385.59490966796875, 373.033203125, 451.38848876953125, 409.27197265625], [474.0908203125, 465.349853515625, 556.8741455078125, 479.760986328125]], 'scores': [0.9628191590309143, 0.5925710797309875, 0.5637194514274597, 0.4243609607219696, 0.37862512469291687], 'class_indices': [0.0, 56.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 194.8ms\n",
      "Speed: 1.8ms preprocess, 194.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.628387451171875, 167.94639587402344, 473.79345703125, 479.395751953125], [464.63494873046875, 413.38592529296875, 518.929443359375, 467.83074951171875], [379.487548828125, 373.1266784667969, 451.37451171875, 415.5310363769531], [379.40728759765625, 372.88531494140625, 451.22369384765625, 415.58819580078125], [468.2533874511719, 465.31805419921875, 555.824951171875, 479.74554443359375], [0.2063140869140625, 289.3233642578125, 40.69696807861328, 362.531005859375]], 'scores': [0.9597004055976868, 0.7077750563621521, 0.5270792841911316, 0.5146030187606812, 0.40233707427978516, 0.2806621789932251], 'class_indices': [0.0, 56.0, 62.0, 63.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 tv, 1 laptop, 196.7ms\n",
      "Speed: 1.9ms preprocess, 196.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.172515869140625, 168.73406982421875, 472.680419921875, 479.48321533203125], [378.231689453125, 373.0140380859375, 450.9075927734375, 416.685546875], [463.91766357421875, 413.49493408203125, 519.06884765625, 468.01995849609375], [378.0303649902344, 373.2261962890625, 451.0036315917969, 416.638671875], [468.8432922363281, 464.95501708984375, 555.8984375, 479.75311279296875]], 'scores': [0.9617213606834412, 0.6012529730796814, 0.57616126537323, 0.502617597579956, 0.326465368270874], 'class_indices': [0.0, 63.0, 56.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 tv, 1 laptop, 195.8ms\n",
      "Speed: 1.7ms preprocess, 195.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.6661376953125, 169.59620666503906, 470.767333984375, 479.53564453125], [376.91912841796875, 372.74908447265625, 451.2847900390625, 420.543212890625], [376.62481689453125, 372.98480224609375, 451.37115478515625, 419.99298095703125], [462.95489501953125, 413.69854736328125, 519.1605834960938, 467.94085693359375], [0.13896942138671875, 288.81597900390625, 37.37084197998047, 360.84173583984375]], 'scores': [0.9608304500579834, 0.6178081035614014, 0.4593290388584137, 0.434411883354187, 0.33049240708351135], 'class_indices': [0.0, 63.0, 62.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 1 laptop, 190.8ms\n",
      "Speed: 1.9ms preprocess, 190.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.2188720703125, 170.10693359375, 469.02203369140625, 479.4783935546875], [375.4014892578125, 373.0860290527344, 450.8504638671875, 422.0988464355469], [375.127685546875, 372.91839599609375, 450.8861083984375, 422.8822021484375]], 'scores': [0.9586158990859985, 0.5169914960861206, 0.3776574432849884], 'class_indices': [0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 1 laptop, 193.8ms\n",
      "Speed: 2.0ms preprocess, 193.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.3221435546875, 169.63980102539062, 470.1011962890625, 479.5056457519531], [376.0650634765625, 373.06646728515625, 451.049072265625, 421.18365478515625], [376.113037109375, 372.8776550292969, 451.072021484375, 421.6775207519531]], 'scores': [0.9564363956451416, 0.491040974855423, 0.47411608695983887], 'class_indices': [0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 laptop, 197.2ms\n",
      "Speed: 1.5ms preprocess, 197.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.343109130859375, 170.05682373046875, 471.19158935546875, 479.4764404296875], [377.812744140625, 372.6802978515625, 451.09405517578125, 419.4798583984375], [463.257568359375, 413.57135009765625, 518.9658203125, 467.75286865234375], [465.39593505859375, 465.20849609375, 556.7180786132812, 479.72894287109375]], 'scores': [0.9590213894844055, 0.5841694474220276, 0.5558242201805115, 0.31939342617988586], 'class_indices': [0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 tv, 1 laptop, 195.4ms\n",
      "Speed: 2.1ms preprocess, 195.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.208892822265625, 170.2318115234375, 472.4822998046875, 479.4273681640625], [378.51995849609375, 372.9633483886719, 451.31646728515625, 418.2071838378906], [378.3927917480469, 373.13739013671875, 451.3794250488281, 418.18255615234375], [463.41595458984375, 413.4632873535156, 519.2052001953125, 468.3065490722656], [466.044677734375, 465.58270263671875, 556.2421264648438, 479.73638916015625]], 'scores': [0.9619287252426147, 0.541608452796936, 0.49232351779937744, 0.44633257389068604, 0.2675102651119232], 'class_indices': [0.0, 63.0, 62.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 1 tv, 1 laptop, 197.5ms\n",
      "Speed: 2.0ms preprocess, 197.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.656005859375, 168.6417236328125, 473.913330078125, 479.35528564453125], [381.20556640625, 372.884765625, 451.0726318359375, 415.072509765625], [381.37908935546875, 373.1484680175781, 451.14007568359375, 415.0899963378906], [464.57891845703125, 413.4269714355469, 518.9655151367188, 468.1478576660156]], 'scores': [0.9578695893287659, 0.5058738589286804, 0.48070719838142395, 0.4510846734046936], 'class_indices': [0.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 3 chairs, 1 tv, 1 laptop, 192.5ms\n",
      "Speed: 1.6ms preprocess, 192.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.13751220703125, 167.9333038330078, 478.91998291015625, 479.42120361328125], [386.193115234375, 372.65594482421875, 451.10797119140625, 409.52008056640625], [466.61865234375, 413.3189392089844, 519.1141357421875, 468.2914123535156], [386.06707763671875, 372.87030029296875, 451.16400146484375, 409.65936279296875], [474.1381530761719, 465.6818542480469, 556.678955078125, 479.7604675292969], [0.09783554077148438, 369.25860595703125, 31.701303482055664, 452.86822509765625]], 'scores': [0.9619035720825195, 0.5259071588516235, 0.4576070010662079, 0.392021507024765, 0.30063650012016296, 0.2679474949836731], 'class_indices': [0.0, 63.0, 56.0, 62.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 194.0ms\n",
      "Speed: 1.8ms preprocess, 194.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.147979736328125, 167.55374145507812, 479.50775146484375, 479.3778991699219], [468.5357666015625, 413.89019775390625, 518.8540649414062, 468.1331787109375], [389.051513671875, 372.903076171875, 451.4891357421875, 406.9482421875], [477.12030029296875, 466.1751403808594, 556.7749633789062, 479.7619323730469], [0.1255645751953125, 289.36383056640625, 40.83244705200195, 468.2503662109375]], 'scores': [0.9662749767303467, 0.6437368392944336, 0.5420435667037964, 0.43866002559661865, 0.2700277864933014], 'class_indices': [0.0, 56.0, 63.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 laptop, 195.8ms\n",
      "Speed: 2.0ms preprocess, 195.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.17840576171875, 168.67181396484375, 478.5205078125, 479.36859130859375], [386.9476318359375, 372.5531005859375, 451.521484375, 407.615966796875], [467.59649658203125, 413.172119140625, 519.0706176757812, 467.95513916015625], [475.47607421875, 465.5450439453125, 556.4306640625, 479.7615966796875]], 'scores': [0.9600649476051331, 0.6721028685569763, 0.5210044384002686, 0.297391414642334], 'class_indices': [0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 laptop, 198.6ms\n",
      "Speed: 2.1ms preprocess, 198.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.309295654296875, 169.02969360351562, 477.8833312988281, 479.4695129394531], [466.012451171875, 413.56500244140625, 518.9027099609375, 467.94317626953125], [384.2208251953125, 372.771240234375, 451.37164306640625, 412.1280517578125], [471.40948486328125, 465.3580017089844, 556.4066772460938, 479.7346496582031]], 'scores': [0.9599699378013611, 0.6131829023361206, 0.5938588380813599, 0.37794429063796997], 'class_indices': [0.0, 56.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 tv, 1 laptop, 196.9ms\n",
      "Speed: 2.1ms preprocess, 196.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.829620361328125, 170.46853637695312, 476.1887512207031, 479.4056701660156], [382.79095458984375, 372.95343017578125, 451.09710693359375, 413.5811767578125], [465.72149658203125, 413.4590759277344, 518.8170166015625, 468.1916809082031], [383.0972900390625, 373.1882019042969, 451.1796875, 413.3157043457031], [471.82513427734375, 465.41912841796875, 556.7974243164062, 479.74835205078125]], 'scores': [0.9572762846946716, 0.5841342806816101, 0.5319545269012451, 0.412810742855072, 0.3222328722476959], 'class_indices': [0.0, 63.0, 56.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 laptop, 197.1ms\n",
      "Speed: 1.8ms preprocess, 197.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.379608154296875, 170.71368408203125, 477.6922302246094, 479.44537353515625], [383.44720458984375, 372.7856750488281, 451.09832763671875, 411.3273620605469], [465.88861083984375, 413.28692626953125, 519.0353393554688, 468.31072998046875], [472.13397216796875, 465.44476318359375, 556.1017456054688, 479.75201416015625]], 'scores': [0.9558305740356445, 0.6622365117073059, 0.5798674821853638, 0.3841404914855957], 'class_indices': [0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 tv, 1 laptop, 191.5ms\n",
      "Speed: 1.6ms preprocess, 191.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.097808837890625, 170.0634765625, 479.595947265625, 479.4285888671875], [466.34326171875, 413.4443359375, 519.2064208984375, 468.199951171875], [384.71343994140625, 372.9903564453125, 451.1026611328125, 409.7237548828125], [384.695068359375, 373.19976806640625, 451.18170166015625, 409.82647705078125], [473.4671325683594, 466.04864501953125, 556.6419677734375, 479.76739501953125]], 'scores': [0.95301353931427, 0.6761624813079834, 0.5194430351257324, 0.4208817780017853, 0.3778762221336365], 'class_indices': [0.0, 56.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 196.6ms\n",
      "Speed: 2.2ms preprocess, 196.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.91217041015625, 169.54721069335938, 480.32269287109375, 479.3560485839844], [389.111083984375, 372.81744384765625, 451.51123046875, 405.82562255859375], [469.0787353515625, 413.4538269042969, 518.899658203125, 468.1291198730469], [0.14042282104492188, 288.763671875, 38.832645416259766, 465.3575439453125], [476.7667541503906, 465.9170837402344, 556.3319091796875, 479.7621154785156]], 'scores': [0.956917941570282, 0.6471140384674072, 0.6316424608230591, 0.5181888937950134, 0.414937287569046], 'class_indices': [0.0, 63.0, 56.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 tv, 196.7ms\n",
      "Speed: 1.9ms preprocess, 196.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.4189453125, 168.18902587890625, 486.3217468261719, 479.4173583984375], [471.5595703125, 413.6176452636719, 519.3885498046875, 469.0547180175781], [395.106689453125, 373.1874084472656, 451.37078857421875, 402.0155334472656], [471.65789794921875, 413.3997802734375, 555.6788940429688, 479.62164306640625]], 'scores': [0.9619943499565125, 0.4585610628128052, 0.4315904974937439, 0.3483612537384033], 'class_indices': [0.0, 56.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 3 chairs, 1 laptop, 195.0ms\n",
      "Speed: 1.9ms preprocess, 195.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.6942138671875, 168.15830993652344, 487.0738830566406, 479.3919677734375], [0.17612457275390625, 288.3422546386719, 43.93679428100586, 468.4910583496094], [394.3094482421875, 372.514404296875, 451.30810546875, 401.3245849609375], [471.314697265625, 413.40985107421875, 519.13525390625, 468.67486572265625], [480.972900390625, 465.3850402832031, 556.6839599609375, 479.7577819824219], [471.24566650390625, 413.3016662597656, 556.2911987304688, 479.7016906738281]], 'scores': [0.9550538063049316, 0.5424467325210571, 0.5031365752220154, 0.43854448199272156, 0.28308990597724915, 0.2718024253845215], 'class_indices': [0.0, 0.0, 63.0, 56.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 laptop, 193.0ms\n",
      "Speed: 2.2ms preprocess, 193.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.4957275390625, 168.43710327148438, 479.3910827636719, 479.4032287597656], [386.135009765625, 372.55609130859375, 450.99755859375, 409.43206787109375], [466.32025146484375, 413.52197265625, 518.929443359375, 467.89111328125], [472.42938232421875, 465.20672607421875, 555.5498657226562, 479.75445556640625]], 'scores': [0.9580486416816711, 0.6100390553474426, 0.3479444086551666, 0.3260895013809204], 'class_indices': [0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 1 tv, 1 laptop, 193.6ms\n",
      "Speed: 1.7ms preprocess, 193.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.11688232421875, 169.5213623046875, 474.01397705078125, 479.445068359375], [379.89532470703125, 373.126708984375, 451.04486083984375, 417.1005859375], [463.99786376953125, 413.4124755859375, 519.0064086914062, 467.94451904296875], [379.935302734375, 372.9183349609375, 451.0230712890625, 417.204833984375]], 'scores': [0.9607282280921936, 0.5322176218032837, 0.494686096906662, 0.476254940032959], 'class_indices': [0.0, 62.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 tv, 1 laptop, 203.9ms\n",
      "Speed: 1.8ms preprocess, 203.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.565277099609375, 169.62025451660156, 474.16973876953125, 479.4256591796875], [381.7313232421875, 372.5606689453125, 451.1495361328125, 414.09619140625], [464.3221435546875, 413.45013427734375, 518.9656982421875, 468.08477783203125], [382.1547546386719, 372.75177001953125, 451.2715148925781, 413.9306640625], [470.275146484375, 465.4309387207031, 556.35888671875, 479.7490539550781]], 'scores': [0.9448211789131165, 0.4614914655685425, 0.4582139849662781, 0.45808589458465576, 0.26711684465408325], 'class_indices': [0.0, 63.0, 56.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 laptop, 197.2ms\n",
      "Speed: 1.8ms preprocess, 197.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.7095947265625, 169.00811767578125, 478.50860595703125, 479.39306640625], [386.6302490234375, 372.63055419921875, 451.538818359375, 409.78973388671875], [466.61505126953125, 413.4842224121094, 518.7611083984375, 468.0075988769531], [473.8182373046875, 465.8410339355469, 556.7366943359375, 479.7605285644531]], 'scores': [0.958044171333313, 0.624925434589386, 0.5367454886436462, 0.2801903486251831], 'class_indices': [0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 laptop, 196.5ms\n",
      "Speed: 1.9ms preprocess, 196.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.97332763671875, 167.701416015625, 479.637451171875, 479.42718505859375], [390.2425537109375, 373.09075927734375, 451.0447998046875, 406.673828125], [0.10502243041992188, 292.2564392089844, 47.313575744628906, 465.4254455566406], [467.97027587890625, 413.530029296875, 518.8333129882812, 468.40576171875], [476.3549499511719, 465.9378662109375, 556.677734375, 479.759765625], [0.10512924194335938, 291.93450927734375, 47.00299835205078, 412.10052490234375]], 'scores': [0.9587379693984985, 0.6387903690338135, 0.5554161667823792, 0.5248087644577026, 0.37179505825042725, 0.28195294737815857], 'class_indices': [0.0, 63.0, 0.0, 56.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 laptop, 199.0ms\n",
      "Speed: 2.0ms preprocess, 199.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.038177490234375, 168.40216064453125, 479.4435729980469, 479.470703125], [0.08550643920898438, 293.61175537109375, 42.47372055053711, 440.34674072265625], [390.31719970703125, 372.769775390625, 451.18060302734375, 406.26904296875], [467.98785400390625, 413.5184326171875, 518.8018188476562, 468.44207763671875], [0.002643585205078125, 293.78759765625, 42.473690032958984, 355.622802734375], [475.86553955078125, 465.87884521484375, 557.1200561523438, 479.76068115234375]], 'scores': [0.9575240612030029, 0.6198422908782959, 0.5970573425292969, 0.46013393998146057, 0.35193368792533875, 0.31974104046821594], 'class_indices': [0.0, 0.0, 63.0, 56.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 194.1ms\n",
      "Speed: 1.7ms preprocess, 194.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.24072265625, 170.206787109375, 480.0533447265625, 479.48419189453125], [389.9637451171875, 372.9629821777344, 451.46484375, 407.0555725097656], [468.26904296875, 413.3976135253906, 518.6700439453125, 468.1392517089844], [0.09862518310546875, 296.184326171875, 45.591976165771484, 409.566650390625], [475.7733154296875, 465.99163818359375, 556.4276123046875, 479.76373291015625]], 'scores': [0.9536961317062378, 0.6946365237236023, 0.619487464427948, 0.48073992133140564, 0.32054227590560913], 'class_indices': [0.0, 63.0, 56.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 192.1ms\n",
      "Speed: 2.4ms preprocess, 192.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.99127197265625, 170.46151733398438, 477.98687744140625, 479.4361267089844], [466.88543701171875, 413.3453369140625, 519.1212158203125, 468.2799072265625], [389.868408203125, 372.91839599609375, 451.1143798828125, 408.71612548828125], [0.09683990478515625, 297.30047607421875, 46.881507873535156, 408.5933837890625], [389.9984130859375, 372.9957275390625, 451.198486328125, 408.8759765625], [474.1454162597656, 465.45355224609375, 557.1063232421875, 479.755126953125]], 'scores': [0.958004891872406, 0.5535321235656738, 0.532880961894989, 0.4022217392921448, 0.38078930974006653, 0.3497532606124878], 'class_indices': [0.0, 56.0, 63.0, 0.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 200.5ms\n",
      "Speed: 1.9ms preprocess, 200.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.777099609375, 169.46234130859375, 477.060302734375, 479.4669189453125], [388.72607421875, 372.93426513671875, 451.3524169921875, 410.67877197265625], [466.51544189453125, 413.42535400390625, 519.1963500976562, 468.18511962890625], [0.0843048095703125, 298.1595458984375, 45.733238220214844, 409.4422607421875], [473.6580505371094, 465.8730163574219, 556.5885009765625, 479.7624816894531]], 'scores': [0.9525717496871948, 0.6523768305778503, 0.6460363864898682, 0.5629817843437195, 0.3089534640312195], 'class_indices': [0.0, 63.0, 56.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 198.6ms\n",
      "Speed: 1.9ms preprocess, 198.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.329681396484375, 169.91510009765625, 478.38739013671875, 479.48406982421875], [0.10567855834960938, 297.14361572265625, 42.78853988647461, 418.9110107421875], [389.96197509765625, 372.9497375488281, 451.15643310546875, 407.5915832519531], [468.1309814453125, 413.49566650390625, 518.775390625, 468.49725341796875], [389.84027099609375, 373.0860595703125, 451.22662353515625, 407.467041015625], [475.67926025390625, 466.0091552734375, 557.6614379882812, 479.7606201171875]], 'scores': [0.9588056802749634, 0.8168593645095825, 0.5069074630737305, 0.4058555066585541, 0.389719158411026, 0.25560757517814636], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 195.6ms\n",
      "Speed: 2.0ms preprocess, 195.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.325469970703125, 170.285400390625, 480.3014831542969, 479.47198486328125], [0.11638259887695312, 295.8878173828125, 45.3710823059082, 432.5535888671875], [391.07891845703125, 372.71728515625, 451.42425537109375, 406.4140625], [469.58477783203125, 413.43096923828125, 519.03759765625, 468.35638427734375], [477.14263916015625, 466.2312927246094, 556.8364868164062, 479.7594299316406]], 'scores': [0.954885721206665, 0.8318358659744263, 0.6461064219474792, 0.6228545904159546, 0.3371187150478363], 'class_indices': [0.0, 0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 196.0ms\n",
      "Speed: 1.6ms preprocess, 196.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.924163818359375, 171.2159423828125, 479.7529602050781, 479.529541015625], [0.11887359619140625, 296.0740966796875, 44.788604736328125, 454.0465087890625], [469.22332763671875, 413.5887451171875, 518.9745483398438, 468.5635986328125], [392.04327392578125, 373.0389404296875, 451.63592529296875, 406.224609375], [391.80645751953125, 372.95361328125, 451.54168701171875, 406.5758056640625], [477.02001953125, 465.56793212890625, 556.4871826171875, 479.75982666015625]], 'scores': [0.9533816576004028, 0.8488794565200806, 0.5616194009780884, 0.529160737991333, 0.4553922712802887, 0.4145243465900421], 'class_indices': [0.0, 0.0, 56.0, 62.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 193.4ms\n",
      "Speed: 2.0ms preprocess, 193.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.8438720703125, 170.79518127441406, 479.25384521484375, 479.418701171875], [0.1305084228515625, 297.3380126953125, 45.1488037109375, 457.3133544921875], [390.6572265625, 372.816650390625, 451.4359130859375, 407.0338134765625], [468.221923828125, 413.3700866699219, 519.2518310546875, 468.5536804199219], [477.1090393066406, 466.02679443359375, 556.6636962890625, 479.76141357421875]], 'scores': [0.9610403776168823, 0.853696882724762, 0.5766010880470276, 0.5291079878807068, 0.40257444977760315], 'class_indices': [0.0, 0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 laptop, 194.8ms\n",
      "Speed: 2.1ms preprocess, 194.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.1434326171875, 170.45635986328125, 478.2115173339844, 479.40289306640625], [0.1519012451171875, 297.9462890625, 45.39159393310547, 458.024658203125], [389.14166259765625, 372.79901123046875, 451.27691650390625, 409.05731201171875], [81.72943115234375, 388.102783203125, 215.59439086914062, 479.65966796875], [467.0877685546875, 413.40386962890625, 519.1341552734375, 469.12115478515625], [467.014404296875, 413.1990966796875, 555.823974609375, 479.6431884765625]], 'scores': [0.9350854754447937, 0.8224369287490845, 0.6747484803199768, 0.503460705280304, 0.46794790029525757, 0.2833460569381714], 'class_indices': [0.0, 0.0, 63.0, 0.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 196.9ms\n",
      "Speed: 1.7ms preprocess, 196.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.689178466796875, 170.69256591796875, 477.1265869140625, 479.42742919921875], [466.35400390625, 413.2008056640625, 519.2981567382812, 468.1480712890625], [386.69891357421875, 372.95361328125, 451.2359619140625, 410.98052978515625], [0.155242919921875, 296.72674560546875, 42.23816680908203, 448.98468017578125], [386.54412841796875, 373.1297607421875, 451.32440185546875, 411.0035400390625], [471.8800048828125, 465.3084411621094, 556.241943359375, 479.7352600097656]], 'scores': [0.9512165188789368, 0.5658578276634216, 0.513645350933075, 0.47696366906166077, 0.46726882457733154, 0.3304399847984314], 'class_indices': [0.0, 56.0, 63.0, 0.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 laptop, 196.4ms\n",
      "Speed: 1.8ms preprocess, 196.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.932586669921875, 170.65130615234375, 476.6619567871094, 479.41412353515625], [386.53790283203125, 372.74493408203125, 451.19305419921875, 412.15277099609375], [466.2698974609375, 413.26995849609375, 519.179443359375, 468.33074951171875], [0.15666961669921875, 296.90582275390625, 40.878440856933594, 410.29718017578125], [0.10335540771484375, 296.65350341796875, 42.061485290527344, 458.76129150390625]], 'scores': [0.9520561695098877, 0.581284761428833, 0.5313699245452881, 0.48031455278396606, 0.29446613788604736], 'class_indices': [0.0, 63.0, 56.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 2 chairs, 1 laptop, 194.8ms\n",
      "Speed: 1.7ms preprocess, 194.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.0211181640625, 170.58056640625, 478.217529296875, 479.393798828125], [0.12057876586914062, 298.1033935546875, 44.12150955200195, 466.28253173828125], [389.8101806640625, 372.825927734375, 451.545166015625, 407.6380615234375], [467.521484375, 413.628173828125, 519.07568359375, 468.3798828125], [81.11296081542969, 389.43731689453125, 212.6514129638672, 479.56427001953125], [75.06008911132812, 358.0975341796875, 180.97247314453125, 479.0518798828125], [475.68402099609375, 465.58349609375, 556.9972534179688, 479.7598876953125]], 'scores': [0.9527636170387268, 0.8328660130500793, 0.6630786061286926, 0.5594874620437622, 0.4090375006198883, 0.30396902561187744, 0.27675139904022217], 'class_indices': [0.0, 0.0, 63.0, 56.0, 0.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 1 laptop, 192.8ms\n",
      "Speed: 1.7ms preprocess, 192.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.71588134765625, 170.11773681640625, 479.56866455078125, 479.387451171875], [0.11810684204101562, 297.5545654296875, 44.96632385253906, 461.869384765625], [392.0738525390625, 372.8607177734375, 451.25634765625, 406.58251953125], [468.68927001953125, 413.2794189453125, 519.000732421875, 468.0238037109375], [392.1768798828125, 372.94146728515625, 451.3453369140625, 406.3128662109375], [475.50665283203125, 465.7626037597656, 557.1209106445312, 479.7606506347656], [73.77555847167969, 364.0301208496094, 182.4828643798828, 478.7012023925781]], 'scores': [0.9508006572723389, 0.7943207621574402, 0.49646052718162537, 0.4629415273666382, 0.3920474648475647, 0.3303682506084442, 0.32983332872390747], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 2 chairs, 1 tv, 1 laptop, 195.3ms\n",
      "Speed: 2.2ms preprocess, 195.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.11163330078125, 169.83660888671875, 478.83953857421875, 479.35321044921875], [0.15157699584960938, 296.8658752441406, 45.31196594238281, 468.4075012207031], [467.96990966796875, 413.72235107421875, 518.8524780273438, 467.92156982421875], [390.84930419921875, 372.749755859375, 451.62371826171875, 407.54638671875], [75.94387817382812, 364.39813232421875, 180.67657470703125, 479.15875244140625], [476.15570068359375, 466.2444763183594, 556.6923217773438, 479.7653503417969], [391.17669677734375, 372.8482666015625, 451.65057373046875, 407.5662841796875], [82.01463317871094, 389.01678466796875, 213.6568145751953, 479.47076416015625]], 'scores': [0.9543812274932861, 0.8503624796867371, 0.6193289756774902, 0.6112022399902344, 0.4799262583255768, 0.37878596782684326, 0.3114025294780731, 0.3003138601779938], 'class_indices': [0.0, 0.0, 56.0, 63.0, 0.0, 56.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 laptop, 197.5ms\n",
      "Speed: 1.6ms preprocess, 197.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.923431396484375, 170.22898864746094, 476.4178161621094, 479.4425048828125], [0.09253692626953125, 297.6259765625, 44.77841567993164, 466.736083984375], [466.382568359375, 413.54644775390625, 518.8248291015625, 467.79522705078125], [387.551513671875, 372.8590087890625, 451.14642333984375, 412.4122314453125], [471.5169372558594, 465.39599609375, 556.6439208984375, 479.74365234375], [0.10485458374023438, 297.5848388671875, 44.916038513183594, 411.8389892578125]], 'scores': [0.9467385411262512, 0.6783719658851624, 0.6485954523086548, 0.5864273309707642, 0.42480993270874023, 0.250176340341568], 'class_indices': [0.0, 0.0, 56.0, 63.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 196.7ms\n",
      "Speed: 1.7ms preprocess, 196.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.2138671875, 170.78903198242188, 475.3992919921875, 479.3948669433594], [0.13083267211914062, 297.6444091796875, 44.84008026123047, 465.3572998046875], [386.3799133300781, 372.892578125, 451.1452941894531, 413.52490234375], [465.57257080078125, 413.295166015625, 519.0694580078125, 468.0194091796875], [470.2857360839844, 464.92095947265625, 556.3875732421875, 479.74053955078125], [386.0106201171875, 372.91680908203125, 451.240234375, 413.24713134765625]], 'scores': [0.9417958855628967, 0.8255870938301086, 0.5788806676864624, 0.5137826204299927, 0.3517400026321411, 0.29707369208335876], 'class_indices': [0.0, 0.0, 63.0, 56.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 1 laptop, 194.4ms\n",
      "Speed: 2.2ms preprocess, 194.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.9534912109375, 171.35643005371094, 476.24432373046875, 479.3992919921875], [466.17333984375, 413.64105224609375, 518.6927490234375, 468.08123779296875], [0.18801116943359375, 297.45819091796875, 45.13343048095703, 462.41571044921875], [387.1563415527344, 373.1168212890625, 451.1380310058594, 412.62274169921875], [387.3170166015625, 373.00628662109375, 451.0189208984375, 412.83062744140625], [471.1282043457031, 465.09588623046875, 556.54736328125, 479.74359130859375], [0.09368133544921875, 297.57989501953125, 45.204124450683594, 410.76641845703125]], 'scores': [0.9508430361747742, 0.7186378240585327, 0.5239250063896179, 0.5080966949462891, 0.5009245872497559, 0.4564535319805145, 0.3726692795753479], 'class_indices': [0.0, 56.0, 0.0, 62.0, 63.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 2 chairs, 1 tv, 1 laptop, 192.2ms\n",
      "Speed: 1.5ms preprocess, 192.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.7322998046875, 171.806640625, 476.5078125, 479.45751953125], [0.15293502807617188, 297.6866149902344, 44.02095031738281, 458.8080139160156], [466.2066650390625, 413.3742370605469, 518.8927612304688, 468.5168762207031], [82.87149810791016, 389.8854675292969, 210.24154663085938, 479.7032165527344], [387.6083984375, 372.7869873046875, 451.297119140625, 411.6826171875], [387.5379943847656, 372.9439392089844, 451.4460144042969, 411.5104064941406], [472.127685546875, 465.569580078125, 556.3812255859375, 479.7431640625], [71.29476928710938, 364.04742431640625, 174.98509216308594, 479.1754150390625]], 'scores': [0.9531815648078918, 0.8116362690925598, 0.5712551474571228, 0.5033480525016785, 0.49294114112854004, 0.41223734617233276, 0.3724629580974579, 0.31421223282814026], 'class_indices': [0.0, 0.0, 56.0, 0.0, 63.0, 62.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 laptop, 193.1ms\n",
      "Speed: 2.1ms preprocess, 193.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.88507080078125, 171.1279754638672, 475.564208984375, 479.43536376953125], [0.08658218383789062, 296.97698974609375, 44.22816467285156, 465.35662841796875], [386.29296875, 372.8858642578125, 451.111328125, 412.989990234375], [465.79437255859375, 413.1660461425781, 518.5098266601562, 467.9349670410156], [80.51504516601562, 390.830078125, 208.24331665039062, 479.7548828125], [472.3814697265625, 465.82568359375, 556.6595458984375, 479.7596435546875]], 'scores': [0.9429396390914917, 0.7425305247306824, 0.5275382995605469, 0.4690791964530945, 0.3946557343006134, 0.291939377784729], 'class_indices': [0.0, 0.0, 63.0, 56.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 2 chairs, 1 tv, 1 laptop, 194.7ms\n",
      "Speed: 1.9ms preprocess, 194.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.71514892578125, 171.9227294921875, 474.07696533203125, 479.4110107421875], [464.69952392578125, 413.53509521484375, 519.0475463867188, 468.47637939453125], [384.0895080566406, 372.98541259765625, 450.9490051269531, 414.86962890625], [383.789306640625, 373.21661376953125, 451.00592041015625, 415.02874755859375], [0.10492324829101562, 297.4737854003906, 45.07501983642578, 409.2744445800781], [79.95329284667969, 391.6932373046875, 206.2090606689453, 479.7982177734375], [0.17346954345703125, 297.3902587890625, 44.872520446777344, 464.0093994140625], [470.2415771484375, 465.53155517578125, 556.4866943359375, 479.75189208984375]], 'scores': [0.9460113644599915, 0.5871231555938721, 0.5660839080810547, 0.5006508827209473, 0.48837441205978394, 0.37824857234954834, 0.36535993218421936, 0.30625081062316895], 'class_indices': [0.0, 56.0, 63.0, 62.0, 0.0, 0.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 1 laptop, 197.4ms\n",
      "Speed: 1.9ms preprocess, 197.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.62646484375, 172.3126220703125, 473.21173095703125, 479.44989013671875], [0.10194778442382812, 297.77386474609375, 44.46472930908203, 451.07281494140625], [384.1387939453125, 373.2510681152344, 450.9786376953125, 414.7273254394531], [464.35272216796875, 413.42449951171875, 518.8472290039062, 468.40264892578125], [81.67695617675781, 396.711669921875, 195.57899475097656, 479.68994140625], [384.58856201171875, 373.1121826171875, 450.93536376953125, 414.93798828125], [469.5233154296875, 465.41326904296875, 556.2880859375, 479.75225830078125]], 'scores': [0.9440370798110962, 0.7579153776168823, 0.4806431829929352, 0.430635541677475, 0.33849895000457764, 0.3284871280193329, 0.29068443179130554], 'class_indices': [0.0, 0.0, 62.0, 56.0, 0.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 194.1ms\n",
      "Speed: 1.6ms preprocess, 194.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.035614013671875, 172.16014099121094, 473.7262268066406, 479.44091796875], [0.133697509765625, 297.1402587890625, 44.65708923339844, 460.88671875], [464.66949462890625, 413.8157958984375, 518.5545043945312, 468.43408203125], [383.6488037109375, 373.119873046875, 451.16827392578125, 415.0078125], [383.7460021972656, 372.9432373046875, 451.0809020996094, 414.998779296875], [469.64178466796875, 465.665771484375, 556.1932983398438, 479.7523193359375]], 'scores': [0.9372665882110596, 0.8156622052192688, 0.5436115264892578, 0.5170896053314209, 0.49938467144966125, 0.29774585366249084], 'class_indices': [0.0, 0.0, 56.0, 62.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 2 chairs, 1 tv, 1 laptop, 193.5ms\n",
      "Speed: 1.9ms preprocess, 193.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.49359130859375, 171.83738708496094, 473.27020263671875, 479.458984375], [0.1039276123046875, 297.2346496582031, 43.0733528137207, 449.1221618652344], [464.09393310546875, 413.394775390625, 518.7739868164062, 467.825439453125], [383.30389404296875, 372.88983154296875, 450.97698974609375, 415.86236572265625], [383.20294189453125, 373.10992431640625, 451.03778076171875, 415.85284423828125], [468.9388122558594, 465.5182800292969, 556.4200439453125, 479.7551574707031], [0.13751602172851562, 329.16497802734375, 31.49832534790039, 446.92987060546875], [0.15877532958984375, 296.93280029296875, 43.3829345703125, 360.88421630859375]], 'scores': [0.9486284852027893, 0.6481139659881592, 0.5816377997398376, 0.5609322786331177, 0.4971158802509308, 0.3555145859718323, 0.2961956858634949, 0.28199338912963867], 'class_indices': [0.0, 0.0, 56.0, 63.0, 62.0, 56.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 194.7ms\n",
      "Speed: 2.1ms preprocess, 194.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.391632080078125, 171.99575805664062, 472.9676513671875, 479.4512634277344], [0.12656784057617188, 297.2508544921875, 43.03537368774414, 445.814453125], [382.6757507324219, 373.01824951171875, 451.3963928222656, 415.61383056640625], [382.44586181640625, 372.77716064453125, 451.2763671875, 415.56494140625], [463.7977294921875, 413.4276123046875, 518.81201171875, 467.94708251953125], [468.9034423828125, 465.4154052734375, 556.3568115234375, 479.750732421875]], 'scores': [0.9511424899101257, 0.7740764617919922, 0.5003423690795898, 0.4766412675380707, 0.4395756125450134, 0.32047539949417114], 'class_indices': [0.0, 0.0, 62.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 197.2ms\n",
      "Speed: 2.2ms preprocess, 197.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.16455078125, 172.280517578125, 474.194091796875, 479.4959716796875], [0.12882232666015625, 296.90911865234375, 44.01288986206055, 451.17950439453125], [382.897216796875, 372.5469970703125, 451.15673828125, 415.6500244140625], [464.35260009765625, 413.43719482421875, 518.9494018554688, 467.73651123046875], [469.0680847167969, 465.143310546875, 556.162353515625, 479.7535400390625]], 'scores': [0.9447131752967834, 0.7958633899688721, 0.629499077796936, 0.5033358931541443, 0.26431402564048767], 'class_indices': [0.0, 0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 197.6ms\n",
      "Speed: 2.2ms preprocess, 197.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.5455322265625, 172.29962158203125, 474.03729248046875, 479.47454833984375], [0.097564697265625, 297.3977966308594, 42.52043151855469, 455.2284240722656], [382.84356689453125, 373.364501953125, 451.2364501953125, 415.30126953125], [464.03057861328125, 413.64697265625, 519.0756225585938, 468.51513671875], [382.5621032714844, 373.200439453125, 451.1689758300781, 415.27117919921875], [468.888671875, 465.6317138671875, 556.505615234375, 479.7540283203125]], 'scores': [0.9485746622085571, 0.7825307250022888, 0.5420469045639038, 0.47979477047920227, 0.4521966576576233, 0.28443098068237305], 'class_indices': [0.0, 0.0, 62.0, 56.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 tv, 1 laptop, 194.9ms\n",
      "Speed: 2.2ms preprocess, 194.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.5145263671875, 171.8848114013672, 473.838134765625, 479.44720458984375], [382.73199462890625, 373.08709716796875, 451.28436279296875, 415.02227783203125], [0.06218719482421875, 296.8400573730469, 40.12300109863281, 410.1531066894531], [464.2774658203125, 413.2499084472656, 518.9013671875, 467.9893493652344], [382.4383544921875, 372.83154296875, 451.2305908203125, 415.07177734375]], 'scores': [0.948725700378418, 0.4803684651851654, 0.4403551518917084, 0.3952328860759735, 0.35849055647850037], 'class_indices': [0.0, 62.0, 0.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 194.9ms\n",
      "Speed: 1.8ms preprocess, 194.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.826263427734375, 171.91055297851562, 473.5812683105469, 479.4119567871094], [0.10430908203125, 296.77783203125, 43.91987609863281, 457.3585205078125], [464.30218505859375, 413.2075500488281, 518.9104614257812, 467.6753234863281], [383.04034423828125, 372.8114013671875, 451.0645751953125, 414.8118896484375], [383.1653747558594, 373.02301025390625, 451.1660461425781, 414.6597900390625], [469.9333801269531, 465.552978515625, 556.62109375, 479.7506103515625]], 'scores': [0.9486367702484131, 0.7398905754089355, 0.5462514758110046, 0.47173675894737244, 0.42692697048187256, 0.30463549494743347], 'class_indices': [0.0, 0.0, 56.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 196.2ms\n",
      "Speed: 1.9ms preprocess, 196.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.540008544921875, 171.70619201660156, 473.9847412109375, 479.48876953125], [464.043701171875, 413.56146240234375, 518.8065795898438, 468.13201904296875], [382.97235107421875, 372.9712829589844, 451.05010986328125, 415.2817687988281], [0.1429443359375, 296.5408935546875, 40.74617004394531, 449.84149169921875], [382.9998779296875, 373.1907958984375, 451.09259033203125, 415.360107421875], [469.17413330078125, 465.2591857910156, 556.5830688476562, 479.7521057128906]], 'scores': [0.947439968585968, 0.6239758729934692, 0.5966131091117859, 0.5816360116004944, 0.49825695157051086, 0.2814655601978302], 'class_indices': [0.0, 56.0, 63.0, 0.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 tv, 1 laptop, 192.0ms\n",
      "Speed: 1.4ms preprocess, 192.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.405853271484375, 171.38555908203125, 474.8465270996094, 479.4884033203125], [0.09149932861328125, 296.9825439453125, 39.67192840576172, 448.2291259765625], [383.93017578125, 372.9942626953125, 451.0465087890625, 415.0330810546875], [464.72406005859375, 413.3741455078125, 518.3338012695312, 468.190673828125], [383.71075439453125, 373.2065124511719, 451.13128662109375, 415.1196594238281]], 'scores': [0.9562221765518188, 0.6924850940704346, 0.5734451413154602, 0.4702407717704773, 0.4515303373336792], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 194.4ms\n",
      "Speed: 2.1ms preprocess, 194.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.472015380859375, 170.94712829589844, 476.4949951171875, 479.4888916015625], [0.08391189575195312, 297.165283203125, 36.82371139526367, 421.506591796875], [385.85528564453125, 373.06256103515625, 451.28717041015625, 413.97186279296875], [465.66619873046875, 413.6053466796875, 518.8598022460938, 468.1885986328125], [385.3770751953125, 373.21221923828125, 451.41192626953125, 413.84747314453125], [470.2828369140625, 465.07177734375, 556.7315673828125, 479.74005126953125]], 'scores': [0.9603763818740845, 0.7901157140731812, 0.5875851511955261, 0.5136775374412537, 0.44691991806030273, 0.3482828140258789], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 196.7ms\n",
      "Speed: 1.6ms preprocess, 196.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.50286865234375, 170.92652893066406, 476.6182861328125, 479.4903564453125], [0.0900726318359375, 297.1856384277344, 37.84149169921875, 431.9853820800781], [465.57891845703125, 413.6851501464844, 518.9844360351562, 467.9400329589844], [384.9393310546875, 372.8995666503906, 451.142333984375, 414.4782409667969], [384.63458251953125, 373.0719909667969, 451.28509521484375, 414.4039001464844], [472.75372314453125, 466.0928039550781, 556.6814575195312, 479.7629699707031]], 'scores': [0.9608844518661499, 0.7730623483657837, 0.6074985265731812, 0.5268546342849731, 0.4360000193119049, 0.36397334933280945], 'class_indices': [0.0, 0.0, 56.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 195.1ms\n",
      "Speed: 1.7ms preprocess, 195.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.400634765625, 170.51820373535156, 474.78765869140625, 479.4476318359375], [0.10000991821289062, 297.5697021484375, 32.775508880615234, 406.2005615234375], [383.51641845703125, 372.83795166015625, 451.16326904296875, 414.81878662109375], [465.45135498046875, 413.56549072265625, 518.8186645507812, 468.06463623046875], [471.4547119140625, 465.49591064453125, 556.7677001953125, 479.751220703125]], 'scores': [0.9587173461914062, 0.8016906380653381, 0.6051104068756104, 0.4733607769012451, 0.2717249393463135], 'class_indices': [0.0, 0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 195.3ms\n",
      "Speed: 1.8ms preprocess, 195.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.43603515625, 171.3993682861328, 475.162353515625, 479.46026611328125], [0.09014511108398438, 297.2591552734375, 33.09331512451172, 402.7054443359375], [382.60650634765625, 372.94036865234375, 451.18756103515625, 414.99517822265625], [465.7880859375, 413.44342041015625, 518.8519897460938, 468.23406982421875], [471.9082336425781, 465.4781799316406, 556.135986328125, 479.7506408691406]], 'scores': [0.948447048664093, 0.716640293598175, 0.6262960433959961, 0.4785923659801483, 0.26188209652900696], 'class_indices': [0.0, 0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 195.1ms\n",
      "Speed: 1.7ms preprocess, 195.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.2591552734375, 172.79443359375, 475.67401123046875, 479.4351806640625], [0.07857704162597656, 297.20806884765625, 31.128421783447266, 398.06292724609375], [465.5377197265625, 413.61700439453125, 518.7003784179688, 468.11871337890625], [382.49676513671875, 372.8399658203125, 451.23040771484375, 414.4346923828125], [470.2020568847656, 465.22174072265625, 556.3468017578125, 479.739990234375]], 'scores': [0.9539157152175903, 0.709770917892456, 0.6224924921989441, 0.5672400593757629, 0.3821459412574768], 'class_indices': [0.0, 0.0, 56.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 193.6ms\n",
      "Speed: 2.1ms preprocess, 193.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.041900634765625, 175.0509490966797, 475.7965087890625, 479.46405029296875], [0.07849502563476562, 297.14495849609375, 30.388648986816406, 394.68780517578125], [383.269775390625, 372.74896240234375, 451.0662841796875, 413.55975341796875], [465.85955810546875, 413.4306335449219, 518.8276977539062, 468.2085876464844], [471.4049987792969, 465.3301696777344, 556.2607421875, 479.7529602050781]], 'scores': [0.9575109481811523, 0.7030381560325623, 0.6859592795372009, 0.5573201775550842, 0.346402645111084], 'class_indices': [0.0, 0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 197.6ms\n",
      "Speed: 2.3ms preprocess, 197.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.232452392578125, 177.36587524414062, 477.3675537109375, 479.4546813964844], [466.593994140625, 413.5773010253906, 518.7821655273438, 467.6475524902344], [0.07811927795410156, 297.14599609375, 31.088232040405273, 392.02520751953125], [384.45343017578125, 372.814208984375, 451.36676025390625, 412.2987060546875], [471.89825439453125, 465.3579406738281, 556.5877075195312, 479.7402038574219]], 'scores': [0.9520587921142578, 0.6677981615066528, 0.6450291275978088, 0.6013185381889343, 0.34137535095214844], 'class_indices': [0.0, 56.0, 0.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 200.2ms\n",
      "Speed: 1.6ms preprocess, 200.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.52252197265625, 177.27354431152344, 478.4378662109375, 479.4736328125], [0.09622573852539062, 296.75506591796875, 35.681663513183594, 402.96221923828125], [385.56121826171875, 372.9106140136719, 451.31201171875, 412.4362487792969], [466.87677001953125, 413.532470703125, 519.0364379882812, 468.4559326171875], [473.5334167480469, 465.9439392089844, 556.768310546875, 479.7620544433594]], 'scores': [0.9466904401779175, 0.7729096412658691, 0.6225075125694275, 0.4929504096508026, 0.30518263578414917], 'class_indices': [0.0, 0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 202.4ms\n",
      "Speed: 1.8ms preprocess, 202.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.4705810546875, 175.43994140625, 475.85772705078125, 479.45794677734375], [0.09870529174804688, 296.95867919921875, 35.328125, 394.70697021484375], [466.5321044921875, 413.2557678222656, 518.8088989257812, 468.1799011230469], [383.13671875, 372.6819763183594, 451.0115966796875, 413.9190368652344], [383.41497802734375, 372.93145751953125, 451.05841064453125, 413.71923828125], [472.7164611816406, 465.63543701171875, 557.1361083984375, 479.75994873046875]], 'scores': [0.9492670297622681, 0.7968642711639404, 0.5218148231506348, 0.4978792369365692, 0.4922165274620056, 0.28116095066070557], 'class_indices': [0.0, 0.0, 56.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 195.9ms\n",
      "Speed: 1.7ms preprocess, 195.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.15020751953125, 175.79994201660156, 474.4300537109375, 479.4957275390625], [380.65155029296875, 373.06378173828125, 451.26666259765625, 415.63189697265625], [465.7392578125, 413.65081787109375, 519.1964721679688, 468.63836669921875], [470.1731262207031, 465.2098083496094, 555.996337890625, 479.7406921386719], [0.13901329040527344, 297.03741455078125, 23.187957763671875, 350.85784912109375]], 'scores': [0.9487439393997192, 0.6489427089691162, 0.5024635791778564, 0.3442247807979584, 0.33753207325935364], 'class_indices': [0.0, 63.0, 56.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 193.7ms\n",
      "Speed: 1.8ms preprocess, 193.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.137298583984375, 176.35626220703125, 475.6938171386719, 479.45361328125], [0.0518798828125, 297.409423828125, 29.761104583740234, 377.35467529296875], [465.4635009765625, 413.5763244628906, 518.954833984375, 468.5077209472656], [379.39556884765625, 372.89605712890625, 451.253662109375, 415.24029541015625], [469.8330993652344, 465.0589599609375, 556.8873291015625, 479.7308349609375]], 'scores': [0.9527714848518372, 0.6439107656478882, 0.5825814604759216, 0.5478408932685852, 0.2877826690673828], 'class_indices': [0.0, 0.0, 56.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 tv, 1 laptop, 194.1ms\n",
      "Speed: 1.9ms preprocess, 194.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.235382080078125, 176.83802795410156, 475.89892578125, 479.469970703125], [378.613037109375, 372.7484130859375, 451.1983642578125, 415.210205078125], [0.10973930358886719, 296.44647216796875, 28.802993774414062, 369.64385986328125], [378.72406005859375, 372.8363952636719, 451.408935546875, 414.9413146972656], [465.6961669921875, 413.2745056152344, 518.8250732421875, 468.3627014160156]], 'scores': [0.9435603022575378, 0.5578890442848206, 0.45553600788116455, 0.3339553475379944, 0.33281779289245605], 'class_indices': [0.0, 63.0, 0.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 tv, 1 laptop, 197.1ms\n",
      "Speed: 1.7ms preprocess, 197.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.971343994140625, 181.28561401367188, 472.7866516113281, 479.4958190917969], [464.366455078125, 413.536865234375, 518.9508056640625, 467.8167724609375], [377.7384948730469, 373.04510498046875, 451.1448059082031, 417.43865966796875], [377.9273376464844, 372.82421875, 451.0807189941406, 417.4725341796875], [467.7779541015625, 465.3904724121094, 556.0943603515625, 479.7370910644531]], 'scores': [0.9457079172134399, 0.6492118835449219, 0.5501535534858704, 0.4323421120643616, 0.37432947754859924], 'class_indices': [0.0, 56.0, 62.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 1 tv, 1 laptop, 195.4ms\n",
      "Speed: 1.8ms preprocess, 195.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.111236572265625, 182.08363342285156, 471.92242431640625, 479.4361572265625], [377.8907775878906, 372.707763671875, 451.1289978027344, 418.031982421875], [377.66632080078125, 372.92230224609375, 451.18743896484375, 417.98480224609375], [464.19866943359375, 413.60797119140625, 518.9496459960938, 468.33026123046875]], 'scores': [0.902594804763794, 0.5388384461402893, 0.44547322392463684, 0.401218056678772], 'class_indices': [0.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 193.5ms\n",
      "Speed: 1.7ms preprocess, 193.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.910430908203125, 181.44940185546875, 472.98944091796875, 479.476318359375], [378.5975341796875, 372.794921875, 451.05548095703125, 418.4124755859375], [464.24383544921875, 413.4158020019531, 518.7623901367188, 467.9674377441406], [0.09319686889648438, 296.7113037109375, 29.20644760131836, 351.2947998046875], [469.4434814453125, 465.429931640625, 556.415283203125, 479.751220703125]], 'scores': [0.9419203400611877, 0.6316267251968384, 0.5876186490058899, 0.5183643698692322, 0.3304908275604248], 'class_indices': [0.0, 63.0, 56.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 201.5ms\n",
      "Speed: 2.3ms preprocess, 201.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.9720458984375, 181.3525848388672, 473.112060546875, 479.46588134765625], [0.04823112487792969, 296.447998046875, 29.3225040435791, 350.15643310546875], [464.39312744140625, 413.44708251953125, 518.5867309570312, 467.63116455078125], [378.5059814453125, 372.639892578125, 451.143798828125, 417.67022705078125], [378.44622802734375, 372.85992431640625, 451.22991943359375, 417.66107177734375], [467.0550537109375, 465.23907470703125, 556.2403564453125, 479.72967529296875]], 'scores': [0.953030526638031, 0.6750518679618835, 0.6386845707893372, 0.6063767075538635, 0.4969940781593323, 0.3354305624961853], 'class_indices': [0.0, 0.0, 56.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 197.9ms\n",
      "Speed: 1.9ms preprocess, 197.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.822723388671875, 181.23828125, 472.0895080566406, 479.49371337890625], [378.022705078125, 372.9225158691406, 451.1317138671875, 419.7970886230469], [463.76495361328125, 413.79510498046875, 519.1171875, 467.986083984375], [0.08719062805175781, 296.5438232421875, 29.21309471130371, 350.3973388671875], [377.8077392578125, 373.12469482421875, 451.18310546875, 419.38946533203125], [466.1412353515625, 465.2994689941406, 555.9342041015625, 479.7279357910156]], 'scores': [0.9378506541252136, 0.5444004535675049, 0.5014379024505615, 0.49435773491859436, 0.4745790958404541, 0.28999581933021545], 'class_indices': [0.0, 63.0, 56.0, 0.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 195.7ms\n",
      "Speed: 1.8ms preprocess, 195.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.706298828125, 180.48651123046875, 471.35443115234375, 479.5020751953125], [0.06482887268066406, 296.67779541015625, 27.998334884643555, 364.56121826171875], [463.66156005859375, 413.1270446777344, 518.5497436523438, 467.4418640136719], [376.78564453125, 373.0104064941406, 451.03338623046875, 420.1634216308594], [377.10662841796875, 372.958251953125, 450.99359130859375, 420.79913330078125], [465.371337890625, 464.4150390625, 555.9197998046875, 479.728759765625]], 'scores': [0.9460042119026184, 0.6149255037307739, 0.5790777802467346, 0.47558650374412537, 0.39979037642478943, 0.2791367471218109], 'class_indices': [0.0, 0.0, 56.0, 62.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 tv, 1 laptop, 1 cell phone, 196.7ms\n",
      "Speed: 1.6ms preprocess, 196.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.255035400390625, 173.91094970703125, 474.67010498046875, 479.532470703125], [465.19561767578125, 413.343505859375, 518.5110473632812, 467.665771484375], [377.93389892578125, 372.760986328125, 450.98895263671875, 416.9964599609375], [377.79107666015625, 372.9539489746094, 451.0706787109375, 416.9802551269531], [84.8697738647461, 358.9311218261719, 192.28048706054688, 416.5642395019531], [470.357421875, 465.3093566894531, 556.103759765625, 479.7488708496094]], 'scores': [0.9499279856681824, 0.5611504912376404, 0.5143876671791077, 0.48727619647979736, 0.39586856961250305, 0.28936049342155457], 'class_indices': [0.0, 56.0, 63.0, 62.0, 67.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 tv, 201.1ms\n",
      "Speed: 1.7ms preprocess, 201.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[63.9505615234375, 173.62301635742188, 473.18048095703125, 479.5389099121094], [370.4413146972656, 373.42633056640625, 451.1152648925781, 427.95428466796875], [0.37032318115234375, 298.56005859375, 125.22254943847656, 476.6072998046875], [462.4825439453125, 414.087890625, 519.2085571289062, 468.720947265625]], 'scores': [0.8949226140975952, 0.5694659352302551, 0.5118163228034973, 0.26705971360206604], 'class_indices': [0.0, 62.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 3 chairs, 2 tvs, 1 laptop, 197.5ms\n",
      "Speed: 1.8ms preprocess, 197.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[41.7313232421875, 176.36753845214844, 446.22186279296875, 479.5604248046875], [0.2028656005859375, 296.795166015625, 74.46365356445312, 455.529296875], [461.3192138671875, 413.4962158203125, 519.157958984375, 465.720703125], [336.82891845703125, 373.05126953125, 451.53509521484375, 460.5179443359375], [0.3860321044921875, 448.05328369140625, 77.36187744140625, 479.57598876953125], [438.426025390625, 464.66558837890625, 551.2494506835938, 479.7410888671875], [33.90361022949219, 360.3650817871094, 83.11988830566406, 415.7852478027344], [73.28146362304688, 359.52734375, 143.82638549804688, 445.6624755859375], [85.04081726074219, 359.710693359375, 144.07203674316406, 416.1722412109375]], 'scores': [0.9422970414161682, 0.8850957155227661, 0.65085768699646, 0.6270202994346619, 0.5156680345535278, 0.5004978179931641, 0.49179914593696594, 0.27476581931114197, 0.2746772766113281], 'class_indices': [0.0, 0.0, 56.0, 62.0, 56.0, 56.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 chair, 1 tv, 196.8ms\n",
      "Speed: 2.2ms preprocess, 196.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[13.68994140625, 188.8845977783203, 426.36724853515625, 479.37249755859375], [239.75613403320312, 271.0672607421875, 310.1936950683594, 380.1986083984375], [0.173065185546875, 295.8452453613281, 72.61334991455078, 476.0242004394531], [333.0303955078125, 373.02252197265625, 451.3955078125, 459.68585205078125], [461.73992919921875, 413.6170654296875, 519.2544555664062, 464.5552978515625], [133.924560546875, 442.3927001953125, 184.089599609375, 479.7425537109375]], 'scores': [0.9471287131309509, 0.8510549068450928, 0.7975265979766846, 0.7121871709823608, 0.6668952107429504, 0.38491034507751465], 'class_indices': [0.0, 0.0, 0.0, 62.0, 56.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 191.9ms\n",
      "Speed: 2.1ms preprocess, 191.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.44964599609375, 204.23898315429688, 403.07586669921875, 479.5270080566406], [211.13150024414062, 271.3262939453125, 329.4685974121094, 409.58209228515625], [331.76470947265625, 373.60552978515625, 451.59429931640625, 460.56719970703125], [461.6190185546875, 414.27801513671875, 519.114013671875, 465.43499755859375], [339.3555908203125, 415.67547607421875, 385.44482421875, 450.48358154296875], [339.78643798828125, 415.3953857421875, 385.05181884765625, 450.3900146484375]], 'scores': [0.95401930809021, 0.8881446719169617, 0.6850838661193848, 0.39535754919052124, 0.34027427434921265, 0.3310844600200653], 'class_indices': [0.0, 0.0, 62.0, 56.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 197.0ms\n",
      "Speed: 1.9ms preprocess, 197.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.357391357421875, 219.96307373046875, 388.30120849609375, 479.481689453125], [331.40924072265625, 372.64141845703125, 451.372314453125, 459.12091064453125], [461.88726806640625, 413.156494140625, 519.1677856445312, 463.8642578125], [213.5734405517578, 270.5859375, 335.05743408203125, 424.4671630859375], [326.0306701660156, 415.2078857421875, 385.3368835449219, 460.22900390625], [326.51690673828125, 415.29400634765625, 385.25592041015625, 460.15093994140625]], 'scores': [0.9212185740470886, 0.7794912457466125, 0.5782458782196045, 0.5441597104072571, 0.47655677795410156, 0.3617784082889557], 'class_indices': [0.0, 62.0, 56.0, 0.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 196.1ms\n",
      "Speed: 1.7ms preprocess, 196.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.71142578125, 222.10208129882812, 390.22216796875, 479.3294982910156], [331.41949462890625, 373.4078369140625, 451.58526611328125, 459.996826171875], [331.7734375, 415.40948486328125, 384.699951171875, 454.49871826171875], [212.91058349609375, 270.69671630859375, 334.7154541015625, 423.75701904296875], [461.9583740234375, 413.938720703125, 519.0579223632812, 464.9049072265625]], 'scores': [0.9432650804519653, 0.6356824040412903, 0.5808764100074768, 0.5774555802345276, 0.5014841556549072], 'class_indices': [0.0, 62.0, 56.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 198.6ms\n",
      "Speed: 1.6ms preprocess, 198.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.571075439453125, 218.72909545898438, 394.257568359375, 479.5231628417969], [210.88470458984375, 270.5955810546875, 335.85772705078125, 422.8084716796875], [331.7508239746094, 373.2016296386719, 451.3947448730469, 460.2828063964844], [330.31707763671875, 415.2132568359375, 385.09771728515625, 454.32666015625], [461.809814453125, 413.613525390625, 518.7210693359375, 464.7197265625], [330.48687744140625, 415.5198974609375, 385.29571533203125, 454.628173828125]], 'scores': [0.9228527545928955, 0.8327383399009705, 0.7592115998268127, 0.5135626792907715, 0.42667707800865173, 0.299815833568573], 'class_indices': [0.0, 0.0, 62.0, 56.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 194.7ms\n",
      "Speed: 1.8ms preprocess, 194.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.940155029296875, 213.42437744140625, 397.643310546875, 479.466796875], [331.54986572265625, 373.08551025390625, 451.41754150390625, 459.91058349609375], [210.60421752929688, 270.19732666015625, 335.0035095214844, 422.18817138671875], [332.6434326171875, 415.07305908203125, 384.9525146484375, 452.7508544921875], [461.33734130859375, 413.35369873046875, 519.1134643554688, 464.50079345703125], [331.9078369140625, 415.3983154296875, 385.207763671875, 452.81707763671875]], 'scores': [0.9064300656318665, 0.7780714631080627, 0.7110366225242615, 0.4863284230232239, 0.42554065585136414, 0.29721009731292725], 'class_indices': [0.0, 62.0, 0.0, 56.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 193.2ms\n",
      "Speed: 2.2ms preprocess, 193.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.6221923828125, 203.39739990234375, 407.4705810546875, 479.5960693359375], [212.41796875, 270.55328369140625, 331.3359375, 413.16497802734375], [332.09466552734375, 373.333984375, 451.56719970703125, 460.02056884765625], [461.68804931640625, 413.80377197265625, 518.7394409179688, 464.58721923828125], [338.8151550292969, 415.36871337890625, 385.1193542480469, 450.32720947265625]], 'scores': [0.9404059052467346, 0.8789522647857666, 0.7796003818511963, 0.5208645462989807, 0.5103275179862976], 'class_indices': [0.0, 0.0, 62.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 tv, 195.4ms\n",
      "Speed: 1.9ms preprocess, 195.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.6044921875, 192.68463134765625, 419.1474609375, 479.52581787109375], [213.08587646484375, 270.020263671875, 326.58740234375, 397.3619384765625], [332.08197021484375, 372.8084411621094, 451.40765380859375, 459.7303161621094], [461.60009765625, 413.4591064453125, 518.8994140625, 464.531982421875]], 'scores': [0.9591450691223145, 0.8733506202697754, 0.6892010569572449, 0.4953509569168091], 'class_indices': [0.0, 0.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 198.8ms\n",
      "Speed: 2.1ms preprocess, 198.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.65380859375, 183.80917358398438, 431.46990966796875, 479.4610900878906], [215.01461791992188, 269.8634338378906, 315.7552795410156, 382.5564880371094], [332.5100402832031, 372.7889404296875, 451.7364807128906, 460.1483154296875], [0.17156982421875, 296.63299560546875, 69.58078002929688, 461.8946533203125], [461.93426513671875, 413.72064208984375, 518.8261108398438, 464.77056884765625]], 'scores': [0.9598038792610168, 0.8371752500534058, 0.7326810956001282, 0.5264133810997009, 0.47406357526779175], 'class_indices': [0.0, 0.0, 62.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 2 chairs, 1 tv, 197.0ms\n",
      "Speed: 1.8ms preprocess, 197.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.31048583984375, 175.687744140625, 441.57562255859375, 479.312744140625], [227.74986267089844, 270.4761047363281, 306.911865234375, 376.0691833496094], [0.14640045166015625, 296.145751953125, 71.55996704101562, 461.7833251953125], [333.01904296875, 373.23193359375, 451.474365234375, 461.0870361328125], [112.64069366455078, 436.8284912109375, 169.9632568359375, 479.7720947265625], [461.2794189453125, 413.85064697265625, 519.04296875, 465.33782958984375], [434.3851318359375, 463.18951416015625, 547.6279907226562, 479.73114013671875]], 'scores': [0.9568396210670471, 0.8712405562400818, 0.8706925511360168, 0.673718273639679, 0.46767812967300415, 0.4504450857639313, 0.30548402667045593], 'class_indices': [0.0, 0.0, 0.0, 62.0, 27.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 5 persons, 1 tie, 1 chair, 1 tv, 1 cell phone, 194.0ms\n",
      "Speed: 1.7ms preprocess, 194.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.63995361328125, 166.82907104492188, 450.74981689453125, 479.2418518066406], [0.139007568359375, 296.95751953125, 71.07923889160156, 466.446533203125], [248.7427215576172, 270.52545166015625, 308.92828369140625, 375.36590576171875], [125.6461181640625, 427.2022705078125, 186.41226196289062, 479.770751953125], [332.583251953125, 372.82501220703125, 451.37005615234375, 460.12445068359375], [461.51385498046875, 413.3543701171875, 518.5364990234375, 463.85498046875], [32.708274841308594, 359.78424072265625, 72.7256088256836, 394.51397705078125], [32.383140563964844, 359.77032470703125, 74.4384994506836, 413.94732666015625], [87.40916442871094, 360.219970703125, 119.87962341308594, 400.78936767578125]], 'scores': [0.9577992558479309, 0.8941380977630615, 0.7474686503410339, 0.6966780424118042, 0.5686754584312439, 0.4877491891384125, 0.3844434320926666, 0.3290747404098511, 0.25196805596351624], 'class_indices': [0.0, 0.0, 0.0, 27.0, 62.0, 56.0, 0.0, 0.0, 67.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 tvs, 202.4ms\n",
      "Speed: 1.9ms preprocess, 202.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.69696044921875, 163.54510498046875, 465.65655517578125, 479.3094482421875], [0.147369384765625, 296.79742431640625, 70.84051513671875, 473.30615234375], [31.6285457611084, 359.76348876953125, 78.58914947509766, 416.74737548828125], [329.10089111328125, 373.4998474121094, 451.38812255859375, 454.9854431152344], [87.03060913085938, 359.99468994140625, 160.8703155517578, 403.65338134765625]], 'scores': [0.946451723575592, 0.8737392425537109, 0.5476623773574829, 0.4794803261756897, 0.2955991327762604], 'class_indices': [0.0, 0.0, 0.0, 62.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 2 tvs, 1 laptop, 196.9ms\n",
      "Speed: 1.9ms preprocess, 196.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[15.795684814453125, 165.0948486328125, 491.3081970214844, 479.24322509765625], [0.1605682373046875, 296.1807861328125, 73.84304809570312, 475.683349609375], [467.5357666015625, 413.8497009277344, 519.630126953125, 465.8029479980469], [363.7724609375, 373.409912109375, 451.3638916015625, 431.23876953125], [83.40449523925781, 359.88909912109375, 202.8345489501953, 419.54742431640625], [32.1981201171875, 360.0321044921875, 84.56771850585938, 421.681396484375], [84.2629165649414, 359.6529235839844, 203.48007202148438, 419.4211120605469]], 'scores': [0.9470252990722656, 0.8502749800682068, 0.7307875752449036, 0.529848575592041, 0.4044552743434906, 0.40277066826820374, 0.2548770010471344], 'class_indices': [0.0, 0.0, 56.0, 62.0, 63.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 2 tvs, 2 laptops, 197.3ms\n",
      "Speed: 1.8ms preprocess, 197.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[39.02508544921875, 167.73333740234375, 513.0247802734375, 479.2608642578125], [0.165496826171875, 294.3299560546875, 74.33219909667969, 475.6842041015625], [472.19500732421875, 413.65399169921875, 518.6051635742188, 467.36285400390625], [73.38040161132812, 359.3426513671875, 204.92221069335938, 439.818359375], [386.8692932128906, 372.9571228027344, 450.9855041503906, 413.9624328613281], [386.88800048828125, 373.015380859375, 451.04937744140625, 413.9625244140625], [81.92170715332031, 358.0788879394531, 205.2678680419922, 416.8706970214844], [33.491798400878906, 360.5914306640625, 82.277099609375, 415.20037841796875]], 'scores': [0.9458126425743103, 0.7952002286911011, 0.6597034335136414, 0.47710782289505005, 0.39335861802101135, 0.3697674572467804, 0.32578882575035095, 0.3017641603946686], 'class_indices': [0.0, 0.0, 56.0, 63.0, 62.0, 63.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 2 chairs, 1 tv, 2 laptops, 197.5ms\n",
      "Speed: 1.6ms preprocess, 197.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[57.38671875, 168.5659942626953, 530.4189453125, 479.30963134765625], [0.14723968505859375, 292.91424560546875, 74.6843032836914, 455.70953369140625], [198.75692749023438, 297.87591552734375, 254.7552490234375, 392.58758544921875], [473.8123779296875, 414.04693603515625, 518.9185791015625, 462.84454345703125], [406.2461853027344, 373.02679443359375, 451.5047302246094, 399.82196044921875], [0.37603759765625, 448.745361328125, 80.16288757324219, 479.75830078125], [77.19577026367188, 360.02886962890625, 206.15887451171875, 451.42156982421875], [34.09852600097656, 360.948486328125, 81.58946228027344, 416.8280029296875], [83.29937744140625, 359.2406921386719, 206.22314453125, 417.1660461425781]], 'scores': [0.9461715221405029, 0.9000612497329712, 0.5596944689750671, 0.5172924995422363, 0.41039276123046875, 0.3904327154159546, 0.3564630448818207, 0.3200364410877228, 0.25673460960388184], 'class_indices': [0.0, 0.0, 0.0, 56.0, 63.0, 56.0, 63.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 2 laptops, 200.1ms\n",
      "Speed: 1.5ms preprocess, 200.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[68.14471435546875, 168.3118896484375, 535.4326171875, 479.30731201171875], [0.19622039794921875, 288.76910400390625, 79.36628723144531, 455.22467041015625], [196.46444702148438, 274.7383117675781, 274.1766052246094, 387.9286804199219], [78.83154296875, 359.65216064453125, 206.03460693359375, 457.87628173828125], [476.58447265625, 413.8525390625, 519.2938232421875, 461.2769775390625], [0.10013008117675781, 408.9429626464844, 24.45355796813965, 440.3793640136719], [0.4681549072265625, 448.0983581542969, 85.51959228515625, 479.7426452636719]], 'scores': [0.9484477639198303, 0.9030535221099854, 0.8310730457305908, 0.3425496220588684, 0.3042941689491272, 0.2967573404312134, 0.29136982560157776], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 laptop, 200.0ms\n",
      "Speed: 1.9ms preprocess, 200.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.128662109375, 168.36575317382812, 544.38232421875, 479.3512878417969], [0.17059326171875, 282.666748046875, 81.4488525390625, 454.3671875], [197.26699829101562, 270.54315185546875, 289.3447570800781, 385.50335693359375], [83.91073608398438, 359.394287109375, 205.582275390625, 418.507568359375], [0.395416259765625, 448.2847595214844, 86.59518432617188, 479.7660217285156], [487.972900390625, 413.829345703125, 518.996826171875, 453.637451171875]], 'scores': [0.9436435699462891, 0.9103865027427673, 0.8830831050872803, 0.32850924134254456, 0.26716575026512146, 0.2598360776901245], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 192.9ms\n",
      "Speed: 1.9ms preprocess, 192.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[87.4068603515625, 168.19207763671875, 539.5339965820312, 479.32470703125], [0.23792266845703125, 278.16754150390625, 92.6504898071289, 477.80279541015625], [196.43014526367188, 270.9525451660156, 291.7572937011719, 382.9963684082031], [0.0665130615234375, 448.08660888671875, 84.18970489501953, 479.78106689453125], [48.08037567138672, 359.97918701171875, 75.3307876586914, 383.99835205078125]], 'scores': [0.9438042640686035, 0.8881798386573792, 0.8849071860313416, 0.4348451495170593, 0.31133872270584106], 'class_indices': [0.0, 0.0, 0.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 193.1ms\n",
      "Speed: 2.1ms preprocess, 193.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[90.5374755859375, 169.43521118164062, 541.3134765625, 479.3222351074219], [0.159332275390625, 279.5281982421875, 100.56314086914062, 478.2303466796875], [196.27947998046875, 270.54473876953125, 288.699951171875, 382.85797119140625], [0.0, 448.092041015625, 84.02714538574219, 479.7833251953125], [87.50238037109375, 359.21319580078125, 205.25149536132812, 420.0909423828125]], 'scores': [0.946108877658844, 0.8952165842056274, 0.8902900218963623, 0.4278509318828583, 0.3514192998409271], 'class_indices': [0.0, 0.0, 0.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 laptop, 192.7ms\n",
      "Speed: 2.3ms preprocess, 192.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.2702178955078125, 284.4464111328125, 94.22742462158203, 479.5411376953125], [44.411376953125, 170.8734130859375, 570.9119873046875, 479.475830078125], [196.56759643554688, 269.8886413574219, 283.5450744628906, 384.8481750488281], [0.2139110565185547, 409.1377258300781, 25.260238647460938, 432.4853820800781]], 'scores': [0.9295172691345215, 0.913641631603241, 0.8669504523277283, 0.29248082637786865], 'class_indices': [0.0, 0.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tv, 1 cell phone, 204.2ms\n",
      "Speed: 1.9ms preprocess, 204.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[4.484619140625, 176.31507873535156, 562.255859375, 479.500732421875], [0.364013671875, 281.70465087890625, 101.67091369628906, 478.88568115234375], [5.318511962890625, 282.08905029296875, 101.61598205566406, 382.60675048828125], [172.50241088867188, 265.74786376953125, 285.8673095703125, 404.80975341796875], [209.06045532226562, 268.6407470703125, 271.21466064453125, 314.2142333984375], [90.14315795898438, 360.811767578125, 143.54275512695312, 383.41168212890625]], 'scores': [0.9308330416679382, 0.4682827889919281, 0.4223018288612366, 0.4167585074901581, 0.30172619223594666, 0.28470298647880554], 'class_indices': [0.0, 0.0, 0.0, 0.0, 67.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 199.0ms\n",
      "Speed: 1.9ms preprocess, 199.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[14.2386474609375, 183.43096923828125, 519.4398193359375, 479.42059326171875], [169.16720581054688, 272.2340087890625, 293.404052734375, 414.682373046875], [0.30669403076171875, 285.9447937011719, 104.73448181152344, 479.1068420410156], [312.49371337890625, 275.6226806640625, 334.12750244140625, 300.84423828125]], 'scores': [0.937383770942688, 0.7256572246551514, 0.6761789321899414, 0.3474281430244446], 'class_indices': [0.0, 0.0, 0.0, 67.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 195.6ms\n",
      "Speed: 1.7ms preprocess, 195.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[33.372222900390625, 188.26974487304688, 510.2134094238281, 479.2937927246094], [0.35152435302734375, 289.9610595703125, 106.08472442626953, 479.281982421875], [191.19082641601562, 276.4937744140625, 300.39617919921875, 416.53594970703125], [15.038345336914062, 289.68182373046875, 106.17442321777344, 385.51129150390625]], 'scores': [0.9280493855476379, 0.6624075770378113, 0.5486937761306763, 0.35878264904022217], 'class_indices': [0.0, 0.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 196.3ms\n",
      "Speed: 1.7ms preprocess, 196.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[46.069366455078125, 187.56158447265625, 509.0655212402344, 479.375], [0.30309295654296875, 290.9992980957031, 104.40591430664062, 479.4200744628906], [491.5120849609375, 413.69598388671875, 581.0260009765625, 479.75067138671875]], 'scores': [0.9427773952484131, 0.7930842041969299, 0.3185542821884155], 'class_indices': [0.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 195.1ms\n",
      "Speed: 2.0ms preprocess, 195.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[45.80076599121094, 184.25662231445312, 502.6351318359375, 479.3910827636719], [0.20758056640625, 292.4398193359375, 102.28202819824219, 479.4791259765625], [481.8157958984375, 413.99212646484375, 582.8485107421875, 479.74652099609375], [195.28704833984375, 344.49609375, 272.42742919921875, 415.04852294921875], [195.55770874023438, 304.004638671875, 272.9470520019531, 414.33074951171875]], 'scores': [0.9527094960212708, 0.9200124144554138, 0.37567225098609924, 0.2635357081890106, 0.263405442237854], 'class_indices': [0.0, 0.0, 56.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 196.6ms\n",
      "Speed: 2.1ms preprocess, 196.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[40.78755187988281, 175.5792236328125, 502.74517822265625, 479.5616455078125], [0.22966766357421875, 294.645751953125, 101.70503234863281, 479.32952880859375], [195.32711791992188, 276.3839111328125, 275.4870300292969, 413.526123046875], [476.3714599609375, 413.97076416015625, 583.773193359375, 479.72686767578125]], 'scores': [0.9591071009635925, 0.911371111869812, 0.6765617728233337, 0.38840439915657043], 'class_indices': [0.0, 0.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 196.1ms\n",
      "Speed: 1.9ms preprocess, 196.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.917449951171875, 163.9664306640625, 508.4725646972656, 479.81292724609375], [194.15924072265625, 271.12884521484375, 285.89996337890625, 404.091064453125], [478.2309265136719, 414.17083740234375, 581.80517578125, 479.77740478515625]], 'scores': [0.9333370923995972, 0.8804381489753723, 0.4682314097881317], 'class_indices': [0.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 1 laptop, 193.7ms\n",
      "Speed: 1.9ms preprocess, 193.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[92.27777099609375, 157.26585388183594, 507.50482177734375, 479.2010498046875], [0.2901611328125, 295.96502685546875, 112.55268859863281, 479.5745849609375], [191.9361572265625, 273.1436767578125, 280.33892822265625, 386.8900146484375], [0.20833587646484375, 408.5191650390625, 24.991209030151367, 440.91436767578125], [87.36524200439453, 359.60186767578125, 205.9459228515625, 457.12030029296875]], 'scores': [0.9527360200881958, 0.9341191649436951, 0.881150484085083, 0.5036668181419373, 0.3761814832687378], 'class_indices': [0.0, 0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 1 laptop, 194.8ms\n",
      "Speed: 2.0ms preprocess, 194.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[81.85003662109375, 151.20034790039062, 501.629150390625, 479.7386169433594], [0.34568023681640625, 295.6918029785156, 110.24077606201172, 479.4085388183594], [194.5771484375, 275.81744384765625, 266.69830322265625, 374.96661376953125], [87.69479370117188, 359.855712890625, 204.84408569335938, 443.729736328125], [479.88275146484375, 413.6361083984375, 519.4918212890625, 467.5159912109375], [87.43743896484375, 359.69403076171875, 204.96780395507812, 443.69427490234375]], 'scores': [0.9602903723716736, 0.931135892868042, 0.8753650784492493, 0.36821022629737854, 0.35022881627082825, 0.28991541266441345], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 2 laptops, 194.4ms\n",
      "Speed: 1.8ms preprocess, 194.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.09074401855469, 152.5861358642578, 500.86175537109375, 479.73944091796875], [0.32022857666015625, 296.033935546875, 111.234130859375, 479.5399169921875], [194.89102172851562, 278.03118896484375, 256.3006896972656, 373.535888671875], [87.25114440917969, 359.4104919433594, 206.34559631347656, 434.9364318847656], [477.93359375, 413.867431640625, 519.468505859375, 468.42156982421875], [0.19224166870117188, 408.95159912109375, 25.53999900817871, 439.27783203125], [477.8356628417969, 413.890869140625, 575.814453125, 479.77044677734375], [87.37115478515625, 359.9212341308594, 205.52490234375, 436.3044128417969]], 'scores': [0.9609693288803101, 0.9281841516494751, 0.8446005582809448, 0.4156484007835388, 0.3192403316497803, 0.29629459977149963, 0.28996244072914124, 0.28197428584098816], 'class_indices': [0.0, 0.0, 0.0, 62.0, 56.0, 63.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 laptop, 198.7ms\n",
      "Speed: 1.8ms preprocess, 198.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[87.43827819824219, 154.86846923828125, 504.29949951171875, 479.7127685546875], [0.3036346435546875, 296.16070556640625, 109.54438781738281, 479.53533935546875], [194.79757690429688, 277.9151611328125, 255.72329711914062, 375.275390625], [87.3991928100586, 359.45050048828125, 204.82302856445312, 447.05255126953125], [477.6168212890625, 413.43560791015625, 518.9390258789062, 467.3817138671875]], 'scores': [0.9536099433898926, 0.934516429901123, 0.861841082572937, 0.662521243095398, 0.3243190050125122], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 198.2ms\n",
      "Speed: 2.4ms preprocess, 198.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[92.86761474609375, 155.6785888671875, 506.11279296875, 479.65594482421875], [0.43802642822265625, 297.7799072265625, 111.21063232421875, 479.4862060546875], [194.06771850585938, 282.2545166015625, 251.49258422851562, 378.6351318359375], [87.26612091064453, 359.70941162109375, 206.39886474609375, 446.70965576171875]], 'scores': [0.956991970539093, 0.9098280072212219, 0.8003718852996826, 0.33320459723472595], 'class_indices': [0.0, 0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 196.4ms\n",
      "Speed: 1.6ms preprocess, 196.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[91.43423461914062, 152.4239501953125, 499.7783508300781, 479.5946044921875], [0.28765869140625, 290.79534912109375, 132.7385711669922, 479.40570068359375], [477.885986328125, 413.7479553222656, 575.8980712890625, 479.7481384277344], [193.83444213867188, 295.60797119140625, 240.79165649414062, 379.23199462890625]], 'scores': [0.943709135055542, 0.49403059482574463, 0.4804086685180664, 0.39993879199028015], 'class_indices': [0.0, 0.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 cup, 2 chairs, 1 tv, 197.3ms\n",
      "Speed: 2.0ms preprocess, 197.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[3.243255615234375, 152.3861083984375, 494.6324157714844, 479.54559326171875], [85.72512817382812, 268.82818603515625, 229.11932373046875, 421.64105224609375], [0.1549530029296875, 299.6905517578125, 77.42343139648438, 469.1488037109375], [476.27813720703125, 413.53521728515625, 519.0287475585938, 467.16510009765625], [492.76678466796875, 464.54827880859375, 573.9386596679688, 479.74420166015625], [416.51727294921875, 373.0195007324219, 451.24078369140625, 404.0489807128906]], 'scores': [0.9480785727500916, 0.8453285098075867, 0.719557523727417, 0.3950245678424835, 0.38444238901138306, 0.3815784156322479], 'class_indices': [0.0, 41.0, 0.0, 56.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 cup, 2 chairs, 1 tv, 1 hair drier, 197.5ms\n",
      "Speed: 2.2ms preprocess, 197.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.751708984375, 153.1977081298828, 493.3121337890625, 479.57513427734375], [103.12603759765625, 269.0141296386719, 248.442138671875, 419.0325012207031], [0.17153167724609375, 300.20428466796875, 84.7357406616211, 463.00091552734375], [103.5023193359375, 268.82403564453125, 248.32598876953125, 419.03961181640625], [476.09222412109375, 413.9127502441406, 519.1929321289062, 467.5968933105469], [413.3668212890625, 373.26666259765625, 451.04150390625, 403.60296630859375], [490.4894104003906, 465.102294921875, 574.2728271484375, 479.73040771484375]], 'scores': [0.966201663017273, 0.8091947436332703, 0.6924567222595215, 0.5082356333732605, 0.4271658957004547, 0.32948967814445496, 0.28822287917137146], 'class_indices': [0.0, 41.0, 0.0, 78.0, 56.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 1 cell phone, 193.7ms\n",
      "Speed: 1.7ms preprocess, 193.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[4.2735595703125, 152.2283477783203, 494.2008056640625, 479.41033935546875], [0.2133636474609375, 300.4337158203125, 85.35547637939453, 449.9888916015625], [102.3365249633789, 261.3287353515625, 248.37417602539062, 406.78857421875], [475.9959716796875, 413.69793701171875, 519.1668701171875, 467.35797119140625], [412.7156677246094, 373.0918884277344, 451.1700744628906, 403.0287170410156], [490.17578125, 464.95703125, 574.82958984375, 479.758056640625], [6.641487121582031, 300.2506408691406, 85.49713134765625, 390.5876770019531]], 'scores': [0.9463919401168823, 0.4741388261318207, 0.3852187991142273, 0.36643362045288086, 0.34001100063323975, 0.29920876026153564, 0.2981896996498108], 'class_indices': [0.0, 0.0, 67.0, 56.0, 62.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 cup, 2 chairs, 1 tv, 1 hair drier, 194.9ms\n",
      "Speed: 2.0ms preprocess, 194.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.432861328125, 151.5654296875, 494.59686279296875, 479.37213134765625], [476.86627197265625, 413.7135009765625, 519.0586547851562, 468.076171875], [413.1693115234375, 373.15411376953125, 451.13330078125, 403.3275146484375], [103.4598388671875, 252.4033966064453, 251.93399047851562, 393.94866943359375], [492.06951904296875, 464.77630615234375, 574.8146362304688, 479.75091552734375], [0.25299835205078125, 298.13983154296875, 82.57011413574219, 435.82696533203125], [6.513641357421875, 298.81561279296875, 83.15415954589844, 386.51654052734375], [103.11048126220703, 251.7692108154297, 251.97479248046875, 394.25982666015625]], 'scores': [0.9663914442062378, 0.43419960141181946, 0.4328683614730835, 0.3672310709953308, 0.31596750020980835, 0.2884399890899658, 0.27816927433013916, 0.27186763286590576], 'class_indices': [0.0, 56.0, 62.0, 78.0, 56.0, 0.0, 0.0, 41.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 199.9ms\n",
      "Speed: 2.2ms preprocess, 199.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.4542236328125, 152.2154541015625, 495.5987548828125, 479.36004638671875], [0.30106353759765625, 299.2091979980469, 78.85629272460938, 423.6979675292969], [413.919189453125, 373.10308837890625, 451.3267822265625, 403.68121337890625], [476.825927734375, 413.812744140625, 519.0635375976562, 467.7083740234375], [493.00579833984375, 464.74951171875, 575.3904418945312, 479.75146484375], [5.126365661621094, 299.421142578125, 78.56263732910156, 389.1348876953125]], 'scores': [0.9624150395393372, 0.5160030722618103, 0.41654908657073975, 0.33785775303840637, 0.28450414538383484, 0.281218558549881], 'class_indices': [0.0, 0.0, 62.0, 56.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 201.8ms\n",
      "Speed: 1.9ms preprocess, 201.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[1.446929931640625, 152.92799377441406, 496.1566162109375, 479.3212890625], [0.1131439208984375, 298.0745544433594, 77.49352264404297, 421.1894836425781], [415.43585205078125, 373.2334899902344, 451.38494873046875, 402.6883239746094], [477.75250244140625, 413.64312744140625, 576.1449584960938, 479.71697998046875], [477.897705078125, 413.65423583984375, 519.0593872070312, 468.06317138671875]], 'scores': [0.9640195369720459, 0.6358994841575623, 0.2969203591346741, 0.2864915728569031, 0.2753599286079407], 'class_indices': [0.0, 0.0, 62.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 cup, 2 chairs, 1 tv, 1 hair drier, 195.0ms\n",
      "Speed: 1.8ms preprocess, 195.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[2.445343017578125, 152.84539794921875, 497.5977478027344, 479.38397216796875], [104.76212310791016, 254.02099609375, 253.24447631835938, 403.9281005859375], [416.31927490234375, 373.04901123046875, 451.1907958984375, 402.71466064453125], [0.13697052001953125, 298.4020690917969, 87.19808197021484, 438.2478942871094], [104.53077697753906, 254.33584594726562, 253.18833923339844, 404.1289367675781], [478.1380615234375, 413.7938232421875, 519.5167236328125, 468.3182373046875], [477.9222412109375, 413.71044921875, 575.598876953125, 479.7410888671875]], 'scores': [0.9526121020317078, 0.43712368607521057, 0.3652990162372589, 0.358917236328125, 0.3562963902950287, 0.3298751711845398, 0.32971516251564026], 'class_indices': [0.0, 78.0, 62.0, 0.0, 41.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 cup, 1 chair, 1 tv, 194.3ms\n",
      "Speed: 1.9ms preprocess, 194.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.60650634765625, 152.3157958984375, 498.7720947265625, 479.47186279296875], [105.28753662109375, 273.71441650390625, 258.239501953125, 428.36614990234375], [0.149658203125, 299.26458740234375, 91.25628662109375, 461.39862060546875], [478.771484375, 413.9456787109375, 518.9459228515625, 468.2359619140625], [416.903076171875, 373.2281494140625, 451.24774169921875, 403.0345458984375]], 'scores': [0.9650033116340637, 0.8700790405273438, 0.5608555674552917, 0.4053117334842682, 0.3161424398422241], 'class_indices': [0.0, 41.0, 0.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 1 person, 1 cup, 3 chairs, 1 tv, 193.9ms\n",
      "Speed: 2.2ms preprocess, 193.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.5283203125, 151.049072265625, 499.23193359375, 479.64788818359375], [55.259521484375, 288.68817138671875, 222.59051513671875, 451.95025634765625], [477.65301513671875, 413.80718994140625, 575.8845825195312, 479.72271728515625], [477.85162353515625, 413.855712890625, 519.2976684570312, 467.8675537109375], [494.5679931640625, 465.58160400390625, 575.7801513671875, 479.75689697265625], [417.57281494140625, 372.918701171875, 451.14239501953125, 402.7486572265625]], 'scores': [0.9492297172546387, 0.8793174028396606, 0.3328612148761749, 0.319303959608078, 0.2680799663066864, 0.2666464149951935], 'class_indices': [0.0, 41.0, 56.0, 56.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 2 tvs, 198.3ms\n",
      "Speed: 1.8ms preprocess, 198.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.65380859375, 155.47332763671875, 499.10797119140625, 479.802001953125], [0.40643310546875, 301.2656555175781, 109.3697509765625, 479.5035705566406], [182.08810424804688, 304.4609375, 236.9813232421875, 381.23358154296875], [87.19232177734375, 359.3806457519531, 197.89816284179688, 441.1506042480469], [478.822998046875, 413.88848876953125, 519.1180419921875, 467.45257568359375], [494.836181640625, 465.9463806152344, 575.956298828125, 479.7637634277344], [417.90899658203125, 373.0627746582031, 450.86578369140625, 402.2503967285156]], 'scores': [0.9632728695869446, 0.9124821424484253, 0.7382524013519287, 0.3855612277984619, 0.35457643866539, 0.31197378039360046, 0.30284252762794495], 'class_indices': [0.0, 0.0, 0.0, 62.0, 56.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 199.1ms\n",
      "Speed: 1.8ms preprocess, 199.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[77.80157470703125, 156.58326721191406, 499.0445556640625, 479.837890625], [0.32656097412109375, 306.7577819824219, 109.10254669189453, 479.5716857910156], [178.96527099609375, 310.1178894042969, 237.798095703125, 381.4122619628906], [87.35879516601562, 359.8023681640625, 193.56417846679688, 435.9332275390625], [479.3336181640625, 414.12335205078125, 575.48486328125, 479.74884033203125]], 'scores': [0.9620196223258972, 0.9363405108451843, 0.7220512628555298, 0.4610867202281952, 0.31016042828559875], 'class_indices': [0.0, 0.0, 0.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 2 laptops, 195.0ms\n",
      "Speed: 1.7ms preprocess, 195.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[71.3656005859375, 155.27674865722656, 493.88922119140625, 479.5614013671875], [0.1866607666015625, 309.8126220703125, 108.25350952148438, 479.619140625], [176.59320068359375, 308.5019836425781, 231.83685302734375, 379.7422790527344], [87.00743865966797, 360.0478515625, 190.39129638671875, 430.7393798828125], [87.19449615478516, 359.67816162109375, 191.35336303710938, 430.34735107421875], [476.7098388671875, 413.826416015625, 519.1317138671875, 467.1148681640625], [0.19976234436035156, 408.2965087890625, 25.287899017333984, 437.536376953125]], 'scores': [0.9531186819076538, 0.9357274770736694, 0.6287292838096619, 0.486808717250824, 0.4101805090904236, 0.38438284397125244, 0.2815083861351013], 'class_indices': [0.0, 0.0, 0.0, 63.0, 62.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 2 tvs, 1 laptop, 193.8ms\n",
      "Speed: 1.8ms preprocess, 193.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[73.40438842773438, 155.4903564453125, 489.1062316894531, 479.577392578125], [0.322174072265625, 310.3546142578125, 106.95909881591797, 479.6424560546875], [474.4241943359375, 413.67559814453125, 518.970703125, 466.94525146484375], [87.15289306640625, 359.51483154296875, 186.01290893554688, 432.74261474609375], [87.03273010253906, 359.82269287109375, 186.0846710205078, 433.34222412109375], [171.9596710205078, 353.139892578125, 226.98121643066406, 381.83935546875], [485.4033203125, 463.4490966796875, 575.1044921875, 479.72314453125], [406.766845703125, 373.1456298828125, 451.14886474609375, 400.979736328125]], 'scores': [0.959982693195343, 0.9312341809272766, 0.5974968671798706, 0.4770173728466034, 0.46630775928497314, 0.36469322443008423, 0.34014883637428284, 0.3194003701210022], 'class_indices': [0.0, 0.0, 56.0, 62.0, 63.0, 0.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 2 tvs, 194.3ms\n",
      "Speed: 1.6ms preprocess, 194.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[78.02149963378906, 157.2802734375, 489.589599609375, 479.583251953125], [0.35202789306640625, 310.39739990234375, 112.9538803100586, 479.43475341796875], [474.847900390625, 414.0486755371094, 518.8284912109375, 467.3651428222656], [167.77548217773438, 295.05816650390625, 227.0706787109375, 384.64141845703125], [486.23095703125, 464.4569091796875, 575.661376953125, 479.724365234375], [408.3675231933594, 373.1035461425781, 451.3771057128906, 402.6701354980469], [86.82179260253906, 359.8317565917969, 182.2231903076172, 424.9490661621094]], 'scores': [0.9608595967292786, 0.9084457159042358, 0.7096222043037415, 0.5712594389915466, 0.4052501320838928, 0.38362687826156616, 0.3640764355659485], 'class_indices': [0.0, 0.0, 56.0, 0.0, 56.0, 62.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 2 tvs, 1 laptop, 197.0ms\n",
      "Speed: 1.6ms preprocess, 197.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[81.35025024414062, 155.4154052734375, 497.1645812988281, 479.80621337890625], [0.3763275146484375, 309.79229736328125, 108.41581726074219, 479.52655029296875], [165.29766845703125, 295.57025146484375, 232.54461669921875, 385.96337890625], [416.39190673828125, 373.00347900390625, 451.145263671875, 402.61993408203125], [87.08529663085938, 359.3941650390625, 187.7703857421875, 437.58441162109375], [86.90237426757812, 359.59783935546875, 185.52862548828125, 437.595947265625], [494.7325134277344, 464.60302734375, 574.3746337890625, 479.7442626953125], [477.7020263671875, 413.5102844238281, 519.068359375, 467.5580749511719]], 'scores': [0.9629992842674255, 0.9311233162879944, 0.521424412727356, 0.45616012811660767, 0.3255557119846344, 0.306381493806839, 0.30064642429351807, 0.2686512768268585], 'class_indices': [0.0, 0.0, 0.0, 62.0, 62.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 197.2ms\n",
      "Speed: 1.9ms preprocess, 197.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[86.30548095703125, 153.6572265625, 500.04986572265625, 479.7825927734375], [0.40999603271484375, 309.88916015625, 108.6058578491211, 479.4017333984375], [163.42822265625, 291.2080383300781, 236.4736328125, 391.6972351074219], [478.81561279296875, 413.6500244140625, 576.7609252929688, 479.7237548828125], [87.22528076171875, 359.57421875, 191.78121948242188, 445.6502685546875]], 'scores': [0.9636885523796082, 0.9232360124588013, 0.8511489033699036, 0.34819793701171875, 0.33104920387268066], 'class_indices': [0.0, 0.0, 0.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 194.8ms\n",
      "Speed: 1.7ms preprocess, 194.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[91.73947143554688, 155.01486206054688, 498.6781311035156, 479.7638854980469], [0.33562469482421875, 310.5257568359375, 108.82046508789062, 479.5970458984375], [162.21664428710938, 293.1234436035156, 238.95620727539062, 388.3164978027344], [87.19634246826172, 359.7224426269531, 191.8909912109375, 445.2373962402344], [479.12335205078125, 413.93829345703125, 519.4224243164062, 468.82867431640625], [479.01800537109375, 413.9833984375, 576.6868286132812, 479.76483154296875]], 'scores': [0.9618457555770874, 0.9346480965614319, 0.6916412711143494, 0.42312440276145935, 0.2803615629673004, 0.2624956965446472], 'class_indices': [0.0, 0.0, 0.0, 62.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 194.8ms\n",
      "Speed: 1.6ms preprocess, 194.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[87.96035766601562, 153.1129150390625, 496.5323791503906, 479.8232421875], [0.3532867431640625, 310.32269287109375, 110.33251953125, 479.5440673828125], [87.2662582397461, 359.2412109375, 190.91928100585938, 437.95941162109375], [161.28842163085938, 299.432373046875, 235.60610961914062, 386.4019775390625], [477.48846435546875, 413.517333984375, 519.3482055664062, 468.364013671875]], 'scores': [0.961164116859436, 0.9228125810623169, 0.3640744686126709, 0.36075136065483093, 0.2977418601512909], 'class_indices': [0.0, 0.0, 62.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 201.4ms\n",
      "Speed: 2.2ms preprocess, 201.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[86.084716796875, 153.385986328125, 489.19244384765625, 479.84075927734375], [0.3422088623046875, 310.0712890625, 109.35154724121094, 479.6798095703125], [474.422607421875, 413.8212890625, 576.1947021484375, 479.77435302734375], [87.29122161865234, 359.304443359375, 192.04183959960938, 439.10821533203125], [474.255859375, 413.7734680175781, 519.1849365234375, 468.9538879394531]], 'scores': [0.9641014337539673, 0.9327963590621948, 0.3359954357147217, 0.3152785301208496, 0.3073783218860626], 'class_indices': [0.0, 0.0, 56.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 207.7ms\n",
      "Speed: 1.8ms preprocess, 207.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.1710205078125, 154.83023071289062, 487.02081298828125, 479.8221130371094], [0.34154510498046875, 307.96014404296875, 109.82054901123047, 479.546875], [473.70428466796875, 413.92193603515625, 518.9678344726562, 467.60992431640625], [483.87109375, 466.06756591796875, 575.919921875, 479.75079345703125], [87.19526672363281, 359.69561767578125, 194.97532653808594, 431.72247314453125], [87.029541015625, 360.29937744140625, 191.47021484375, 433.157470703125]], 'scores': [0.9654830098152161, 0.936204731464386, 0.5092748403549194, 0.36765044927597046, 0.3422281742095947, 0.2868519723415375], 'class_indices': [0.0, 0.0, 56.0, 56.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 tv, 224.6ms\n",
      "Speed: 1.8ms preprocess, 224.6ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.4324951171875, 153.8291778564453, 487.152587890625, 479.82598876953125], [0.29010009765625, 306.82177734375, 109.68359375, 479.40472412109375], [473.8746337890625, 413.48565673828125, 519.4912109375, 468.22344970703125], [87.28363037109375, 359.64410400390625, 184.44546508789062, 433.261962890625]], 'scores': [0.9659002423286438, 0.9225760698318481, 0.299403578042984, 0.2628082036972046], 'class_indices': [0.0, 0.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 213.4ms\n",
      "Speed: 1.7ms preprocess, 213.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.4305419921875, 154.8870849609375, 485.98590087890625, 479.62774658203125], [0.32544708251953125, 306.8043518066406, 108.84188079833984, 479.5849914550781], [473.0931396484375, 413.9578857421875, 519.083251953125, 468.871337890625], [87.21292114257812, 359.459228515625, 191.9393310546875, 434.4429931640625], [87.044921875, 359.9931640625, 189.0328369140625, 435.29345703125], [473.09716796875, 414.0344543457031, 576.2109375, 479.8085632324219]], 'scores': [0.9645339846611023, 0.9413823485374451, 0.44068458676338196, 0.3877224624156952, 0.35917797684669495, 0.34107187390327454], 'class_indices': [0.0, 0.0, 56.0, 62.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 1 laptop, 198.0ms\n",
      "Speed: 1.8ms preprocess, 198.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.86241149902344, 155.07440185546875, 485.2672119140625, 479.6051025390625], [0.2489013671875, 306.375732421875, 109.098876953125, 479.55938720703125], [472.7330322265625, 413.60443115234375, 519.0977172851562, 468.70074462890625], [87.3625717163086, 359.56793212890625, 185.27847290039062, 434.70623779296875], [169.20046997070312, 356.43408203125, 222.93801879882812, 382.02874755859375], [87.18452453613281, 359.4583740234375, 183.34751892089844, 434.9293212890625]], 'scores': [0.9627350568771362, 0.9363126158714294, 0.4549212157726288, 0.3653905391693115, 0.2929876744747162, 0.2918041944503784], 'class_indices': [0.0, 0.0, 56.0, 62.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 1 laptop, 200.0ms\n",
      "Speed: 1.5ms preprocess, 200.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.81861877441406, 155.37353515625, 485.5933837890625, 479.59515380859375], [0.28498077392578125, 306.48016357421875, 108.95491790771484, 479.636962890625], [87.33120727539062, 359.8792419433594, 186.51492309570312, 434.1943054199219], [472.92047119140625, 413.87939453125, 519.5112915039062, 469.4423828125], [472.877197265625, 413.9407958984375, 576.1048583984375, 479.76220703125], [174.4859619140625, 357.00994873046875, 223.115478515625, 380.12554931640625], [87.23289489746094, 360.1612243652344, 186.8762664794922, 434.9725646972656]], 'scores': [0.9628909826278687, 0.9392606019973755, 0.4394254982471466, 0.37697067856788635, 0.36982065439224243, 0.3162885308265686, 0.31393200159072876], 'class_indices': [0.0, 0.0, 62.0, 56.0, 56.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 3 chairs, 1 tv, 192.9ms\n",
      "Speed: 2.2ms preprocess, 192.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.71875, 154.177001953125, 487.18731689453125, 479.60174560546875], [0.334320068359375, 306.40679931640625, 109.5328369140625, 479.59197998046875], [473.68035888671875, 413.59918212890625, 518.9638061523438, 468.35150146484375], [87.38912963867188, 359.7161560058594, 190.4541015625, 433.8724670410156], [473.98272705078125, 413.65155029296875, 576.2511596679688, 479.80902099609375], [481.39654541015625, 464.2543640136719, 575.8561401367188, 479.7420959472656]], 'scores': [0.9612917900085449, 0.9333785176277161, 0.49802419543266296, 0.4198133647441864, 0.2971707880496979, 0.2783757448196411], 'class_indices': [0.0, 0.0, 56.0, 62.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 1 laptop, 208.3ms\n",
      "Speed: 1.7ms preprocess, 208.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.70181274414062, 154.67306518554688, 487.3625793457031, 479.6033020019531], [0.232635498046875, 306.6938171386719, 110.36874389648438, 479.5303649902344], [87.51068115234375, 359.84844970703125, 193.8392333984375, 431.45770263671875], [473.982666015625, 413.65802001953125, 519.464599609375, 468.84735107421875], [474.22149658203125, 413.69927978515625, 577.0357055664062, 479.81219482421875], [181.08758544921875, 310.590087890625, 223.015625, 377.79052734375], [87.42882537841797, 360.20269775390625, 192.69158935546875, 432.49871826171875]], 'scores': [0.9608728289604187, 0.9361708760261536, 0.4674493074417114, 0.446540892124176, 0.37561464309692383, 0.36691901087760925, 0.35767683386802673], 'class_indices': [0.0, 0.0, 62.0, 56.0, 56.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 1 laptop, 218.9ms\n",
      "Speed: 1.9ms preprocess, 218.9ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.328369140625, 155.3598175048828, 486.86224365234375, 479.59613037109375], [0.25537872314453125, 306.35009765625, 110.58940887451172, 479.5159912109375], [87.39014434814453, 359.833984375, 199.07550048828125, 428.6614990234375], [473.8515625, 413.50927734375, 519.0181884765625, 467.8238525390625], [87.46170043945312, 359.43511962890625, 200.31460571289062, 424.97625732421875], [185.76930236816406, 310.99993896484375, 222.72486877441406, 377.05621337890625], [482.30474853515625, 464.510498046875, 576.0863647460938, 479.743408203125]], 'scores': [0.9600362777709961, 0.9217590093612671, 0.4041394591331482, 0.3952828049659729, 0.36931443214416504, 0.3100484311580658, 0.29007425904273987], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 210.1ms\n",
      "Speed: 1.5ms preprocess, 210.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.4090576171875, 154.7523651123047, 486.39300537109375, 479.57806396484375], [0.3474578857421875, 306.1889953613281, 109.68763732910156, 479.4552917480469], [87.39044189453125, 359.26458740234375, 204.71994018554688, 433.44659423828125], [473.80999755859375, 413.59613037109375, 519.2374267578125, 468.337646484375], [87.38394165039062, 358.94171142578125, 205.48870849609375, 433.05889892578125], [481.1316223144531, 464.3151550292969, 575.9365234375, 479.7416687011719]], 'scores': [0.9606934189796448, 0.9343740344047546, 0.5731777548789978, 0.5403369665145874, 0.3503894507884979, 0.2849038541316986], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 laptop, 209.6ms\n",
      "Speed: 2.2ms preprocess, 209.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.17530822753906, 154.33001708984375, 486.3677978515625, 479.554931640625], [0.16738128662109375, 305.6627197265625, 110.94437408447266, 479.5888671875], [87.23374938964844, 359.7010803222656, 205.02516174316406, 432.8772888183594], [473.381103515625, 413.65008544921875, 519.2036743164062, 468.06329345703125], [480.9571838378906, 464.6161804199219, 575.6383056640625, 479.7282409667969]], 'scores': [0.9604905247688293, 0.9347123503684998, 0.6024382710456848, 0.5017745494842529, 0.25468316674232483], 'class_indices': [0.0, 0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 2 laptops, 207.8ms\n",
      "Speed: 1.9ms preprocess, 207.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.95805358886719, 153.04586791992188, 486.40325927734375, 479.5764465332031], [0.34157562255859375, 306.1220703125, 109.7392807006836, 479.51788330078125], [87.55303955078125, 358.64703369140625, 205.14501953125, 434.29730224609375], [472.80303955078125, 413.6963806152344, 519.0596313476562, 468.3737487792969], [404.72381591796875, 372.84027099609375, 451.19818115234375, 399.74847412109375]], 'scores': [0.957673966884613, 0.9386037588119507, 0.5858316421508789, 0.5817916393280029, 0.25080612301826477], 'class_indices': [0.0, 0.0, 63.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 2 laptops, 197.3ms\n",
      "Speed: 1.8ms preprocess, 197.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.8309326171875, 153.44366455078125, 486.00042724609375, 479.3511962890625], [0.34615325927734375, 306.14739990234375, 109.17017364501953, 479.42230224609375], [473.10955810546875, 413.7290954589844, 519.1019287109375, 468.2006530761719], [87.42647552490234, 358.8074951171875, 205.82101440429688, 435.657958984375], [87.4574966430664, 358.82696533203125, 205.22140502929688, 435.9024658203125], [481.8856506347656, 464.24371337890625, 576.2689208984375, 479.74761962890625], [404.5091552734375, 373.121826171875, 451.24188232421875, 400.7022705078125]], 'scores': [0.9601520299911499, 0.9355233907699585, 0.6396199464797974, 0.49055641889572144, 0.3951004445552826, 0.3173561096191406, 0.29441937804222107], 'class_indices': [0.0, 0.0, 56.0, 62.0, 63.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 2 tvs, 2 laptops, 196.0ms\n",
      "Speed: 1.6ms preprocess, 196.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.57691955566406, 151.688720703125, 486.5015869140625, 479.58831787109375], [0.29286956787109375, 306.4330749511719, 109.7730484008789, 479.5735778808594], [472.92047119140625, 413.7305908203125, 518.8335571289062, 468.05401611328125], [87.48831176757812, 359.00091552734375, 205.76382446289062, 435.17047119140625], [87.50112915039062, 358.964111328125, 205.14993286132812, 435.0677490234375], [481.171875, 464.12841796875, 575.4725341796875, 479.74200439453125], [404.95513916015625, 373.0848388671875, 451.22882080078125, 400.813720703125], [404.902099609375, 373.0667724609375, 451.2379150390625, 400.81561279296875]], 'scores': [0.9578887224197388, 0.9393424391746521, 0.5269518494606018, 0.45269066095352173, 0.42639434337615967, 0.3073345422744751, 0.2736739218235016, 0.2516263425350189], 'class_indices': [0.0, 0.0, 56.0, 62.0, 63.0, 56.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 197.6ms\n",
      "Speed: 1.8ms preprocess, 197.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.62112426757812, 151.11795043945312, 486.1908874511719, 479.5904235839844], [0.3567047119140625, 306.76922607421875, 109.71806335449219, 479.46514892578125], [473.39361572265625, 413.6573791503906, 518.8477783203125, 467.9551696777344], [87.28179931640625, 358.841064453125, 205.77572631835938, 436.2816162109375], [87.2848892211914, 358.77203369140625, 205.11416625976562, 436.0640869140625], [483.2896423339844, 465.8138122558594, 576.3890380859375, 479.7490539550781]], 'scores': [0.9603123664855957, 0.9412842988967896, 0.5129627585411072, 0.4856380224227905, 0.44989368319511414, 0.3403860330581665], 'class_indices': [0.0, 0.0, 56.0, 62.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 2 laptops, 210.0ms\n",
      "Speed: 1.4ms preprocess, 210.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.42877197265625, 150.7880859375, 485.9002685546875, 479.5694580078125], [0.3369293212890625, 305.8039245605469, 109.20381164550781, 479.4919738769531], [87.4915771484375, 358.91021728515625, 205.64349365234375, 436.86016845703125], [472.69775390625, 413.6290588378906, 519.0132446289062, 468.3381042480469], [87.5308837890625, 358.8511962890625, 204.99591064453125, 436.767333984375], [481.4584045410156, 465.245849609375, 575.4193115234375, 479.7381591796875], [404.94989013671875, 373.09442138671875, 451.19427490234375, 401.09112548828125]], 'scores': [0.9611244797706604, 0.9396660923957825, 0.48727336525917053, 0.4871039390563965, 0.44658851623535156, 0.26489728689193726, 0.25794318318367004], 'class_indices': [0.0, 0.0, 62.0, 56.0, 63.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 3 chairs, 1 tv, 2 laptops, 208.4ms\n",
      "Speed: 1.4ms preprocess, 208.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.31243896484375, 150.07843017578125, 485.99713134765625, 479.56915283203125], [0.2579345703125, 306.16290283203125, 109.95082092285156, 479.5526123046875], [87.24742889404297, 358.8035888671875, 205.14312744140625, 433.831298828125], [473.22064208984375, 413.6279296875, 519.5167236328125, 468.87994384765625], [87.2175064086914, 358.98822021484375, 205.59060668945312, 433.35235595703125], [405.1690368652344, 372.89990234375, 451.3697204589844, 401.415283203125], [480.22314453125, 463.87255859375, 573.95654296875, 479.7576904296875], [473.1832275390625, 413.627685546875, 574.0655517578125, 479.71746826171875]], 'scores': [0.960125744342804, 0.93023282289505, 0.5646712779998779, 0.503910481929779, 0.34225180745124817, 0.28735241293907166, 0.2788594365119934, 0.27859774231910706], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 2 tvs, 206.7ms\n",
      "Speed: 2.0ms preprocess, 206.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.1922607421875, 150.63006591796875, 484.287841796875, 479.59686279296875], [0.26470184326171875, 305.9949951171875, 110.64213562011719, 479.5328369140625], [87.39254760742188, 358.91015625, 205.86880493164062, 433.968017578125], [472.04400634765625, 413.7266845703125, 519.2026977539062, 468.703857421875], [472.073974609375, 413.80499267578125, 576.0928955078125, 479.77008056640625], [403.9426574707031, 373.07440185546875, 451.2348327636719, 401.53179931640625]], 'scores': [0.959274172782898, 0.9384253025054932, 0.5586677193641663, 0.5093961358070374, 0.3270185887813568, 0.2963852882385254], 'class_indices': [0.0, 0.0, 62.0, 56.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 1 tv, 1 laptop, 205.2ms\n",
      "Speed: 1.5ms preprocess, 205.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[81.98712158203125, 151.69216918945312, 481.46612548828125, 479.5823669433594], [0.297943115234375, 306.12451171875, 109.06788635253906, 479.5550537109375], [471.33978271484375, 413.652099609375, 518.6396484375, 467.6009521484375], [87.23147583007812, 358.7047119140625, 204.88186645507812, 434.6678466796875], [477.9351806640625, 464.38006591796875, 575.8087158203125, 479.72332763671875], [400.7817687988281, 372.75067138671875, 451.3458557128906, 402.79376220703125]], 'scores': [0.9616336226463318, 0.9387683868408203, 0.683217465877533, 0.6065748929977417, 0.3819064199924469, 0.3750131130218506], 'class_indices': [0.0, 0.0, 56.0, 63.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 2 tvs, 2 laptops, 200.6ms\n",
      "Speed: 1.9ms preprocess, 200.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[80.56806945800781, 151.2105712890625, 480.27178955078125, 479.695068359375], [0.27957916259765625, 306.47552490234375, 110.64876556396484, 479.54107666015625], [468.39471435546875, 413.9369201660156, 519.4146118164062, 468.4277038574219], [87.47645568847656, 359.11212158203125, 205.1254425048828, 427.42755126953125], [393.07989501953125, 373.20782470703125, 451.46392822265625, 405.03399658203125], [473.8240966796875, 463.9132995605469, 576.3753662109375, 479.7214050292969], [393.13824462890625, 373.16748046875, 451.50030517578125, 404.89984130859375], [87.47129821777344, 358.90771484375, 205.6475372314453, 426.0391845703125]], 'scores': [0.9626652002334595, 0.9327475428581238, 0.6141086220741272, 0.528719961643219, 0.3760393559932709, 0.35756129026412964, 0.3498610854148865, 0.3408879041671753], 'class_indices': [0.0, 0.0, 56.0, 63.0, 63.0, 56.0, 62.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 laptop, 198.5ms\n",
      "Speed: 2.2ms preprocess, 198.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.26422119140625, 306.3040466308594, 108.95118713378906, 479.5502014160156], [70.98333740234375, 153.73114013671875, 514.38134765625, 479.4822998046875], [87.39582824707031, 359.5625915527344, 207.0089874267578, 421.2287902832031], [471.63458251953125, 413.7227783203125, 519.2533569335938, 467.858154296875]], 'scores': [0.9359516501426697, 0.9254258871078491, 0.6071860194206238, 0.2628985643386841], 'class_indices': [0.0, 0.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 2 chairs, 1 tv, 2 laptops, 196.5ms\n",
      "Speed: 1.9ms preprocess, 196.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[68.15625, 156.6322784423828, 534.4118041992188, 479.41680908203125], [0.26587677001953125, 306.52313232421875, 107.1426010131836, 479.53167724609375], [87.134765625, 360.21429443359375, 205.84091186523438, 420.41156005859375], [384.8541259765625, 373.18450927734375, 451.2801513671875, 401.548095703125], [87.3033447265625, 359.64300537109375, 206.19635009765625, 420.12237548828125], [220.33529663085938, 420.86334228515625, 263.51141357421875, 479.76849365234375], [198.1544189453125, 420.0855712890625, 263.576171875, 479.7874755859375], [473.0760498046875, 414.0383605957031, 519.27587890625, 463.5419006347656], [472.9259033203125, 413.8740539550781, 573.6134033203125, 480.0]], 'scores': [0.9434211850166321, 0.9317469596862793, 0.5328453183174133, 0.5129753947257996, 0.36554867029190063, 0.35524019598960876, 0.32922446727752686, 0.2864779233932495, 0.256534218788147], 'class_indices': [0.0, 0.0, 63.0, 63.0, 62.0, 27.0, 27.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 chair, 1 laptop, 192.7ms\n",
      "Speed: 1.8ms preprocess, 192.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[80.09600830078125, 158.9012451171875, 546.552734375, 479.45684814453125], [0.25140380859375, 307.5841064453125, 107.880859375, 479.53240966796875], [87.13311767578125, 360.16339111328125, 205.04238891601562, 431.83453369140625], [208.08969116210938, 424.44976806640625, 276.4242248535156, 479.74053955078125], [477.78643798828125, 414.6055908203125, 519.4240112304688, 454.955810546875], [228.26104736328125, 424.14617919921875, 276.93817138671875, 479.77520751953125]], 'scores': [0.9390438199043274, 0.9290714263916016, 0.6880211234092712, 0.3722027838230133, 0.27856603264808655, 0.2708827257156372], 'class_indices': [0.0, 0.0, 63.0, 27.0, 56.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 laptop, 195.3ms\n",
      "Speed: 2.3ms preprocess, 195.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[81.75689697265625, 154.6564178466797, 555.760986328125, 479.41949462890625], [0.20243072509765625, 304.84588623046875, 108.62128448486328, 479.49346923828125], [87.22625732421875, 358.0543518066406, 204.83554077148438, 434.1267395019531], [0.04418373107910156, 343.78912353515625, 13.950923919677734, 383.46148681640625]], 'scores': [0.9427263140678406, 0.9186788201332092, 0.6387061476707458, 0.2575136423110962], 'class_indices': [0.0, 0.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tv, 1 laptop, 197.6ms\n",
      "Speed: 2.0ms preprocess, 197.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[89.6148681640625, 153.30349731445312, 554.5946044921875, 479.4600524902344], [0.25301361083984375, 307.65972900390625, 110.5688247680664, 479.58367919921875], [87.36685180664062, 359.4208068847656, 205.96697998046875, 438.5372619628906], [0.061283111572265625, 329.662353515625, 26.703765869140625, 361.0989990234375], [87.51012420654297, 359.54248046875, 205.4615478515625, 439.035400390625], [0.0, 326.25799560546875, 17.132740020751953, 350.9923095703125]], 'scores': [0.9401015639305115, 0.9309793710708618, 0.37585410475730896, 0.31940844655036926, 0.27831828594207764, 0.26852330565452576], 'class_indices': [0.0, 0.0, 62.0, 0.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 1 laptop, 200.3ms\n",
      "Speed: 1.6ms preprocess, 200.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[86.14495849609375, 153.00315856933594, 553.8311767578125, 479.3123779296875], [0.27239990234375, 306.59515380859375, 110.00277709960938, 479.5595703125], [87.41156768798828, 358.8169250488281, 204.96307373046875, 436.3055725097656], [87.29449462890625, 358.8566589355469, 205.4429931640625, 435.6308898925781], [225.36715698242188, 428.83380126953125, 298.0491638183594, 479.74334716796875]], 'scores': [0.9418094158172607, 0.928993284702301, 0.39364153146743774, 0.37145763635635376, 0.35751885175704956], 'class_indices': [0.0, 0.0, 63.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 199.4ms\n",
      "Speed: 1.6ms preprocess, 199.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[86.88812255859375, 151.6563262939453, 553.28857421875, 479.40948486328125], [0.25777435302734375, 305.8099670410156, 110.35269927978516, 479.6013488769531], [225.2506103515625, 429.07025146484375, 297.23272705078125, 479.77301025390625], [87.25596618652344, 358.70782470703125, 205.42518615722656, 434.89324951171875]], 'scores': [0.9419110417366028, 0.9232967495918274, 0.39348354935646057, 0.3139192759990692], 'class_indices': [0.0, 0.0, 27.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 196.4ms\n",
      "Speed: 1.8ms preprocess, 196.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[88.356201171875, 153.08062744140625, 555.0472412109375, 479.3099365234375], [0.242401123046875, 304.77471923828125, 109.9542236328125, 479.4681396484375], [87.4443359375, 359.5015563964844, 205.81179809570312, 440.2325134277344], [227.35089111328125, 430.64459228515625, 299.0174560546875, 479.64556884765625]], 'scores': [0.9461575150489807, 0.9339821338653564, 0.5515541434288025, 0.37295135855674744], 'class_indices': [0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 2 laptops, 193.4ms\n",
      "Speed: 2.3ms preprocess, 193.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[86.35111999511719, 152.41250610351562, 555.3294067382812, 479.3320007324219], [0.2333831787109375, 303.1022033691406, 111.72062683105469, 479.5254211425781], [0.23665428161621094, 408.0104064941406, 25.59226417541504, 435.1997985839844], [87.4874267578125, 358.45489501953125, 205.42337036132812, 434.6558837890625], [87.57496643066406, 358.42279052734375, 204.83140563964844, 435.62322998046875], [229.0260467529297, 427.66058349609375, 299.66192626953125, 479.76361083984375]], 'scores': [0.9432905912399292, 0.9241070747375488, 0.46169739961624146, 0.43963387608528137, 0.4079330563545227, 0.2591682970523834], 'class_indices': [0.0, 0.0, 63.0, 62.0, 63.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 194.0ms\n",
      "Speed: 1.5ms preprocess, 194.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.21028900146484375, 303.30078125, 111.99594116210938, 479.460693359375], [87.32859802246094, 153.61402893066406, 552.2613525390625, 479.3970947265625], [0.029388427734375, 345.0594482421875, 20.41216278076172, 384.69384765625], [87.53469848632812, 358.7042236328125, 205.75039672851562, 432.1044921875]], 'scores': [0.9314579963684082, 0.92626953125, 0.5446450710296631, 0.5123294591903687], 'class_indices': [0.0, 0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 tv, 1 laptop, 197.3ms\n",
      "Speed: 1.9ms preprocess, 197.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[86.888671875, 154.12789916992188, 554.4913330078125, 479.3598327636719], [0.33538818359375, 303.41943359375, 110.0036392211914, 479.57763671875], [229.02053833007812, 428.56939697265625, 299.9300231933594, 479.74273681640625], [87.34352111816406, 358.986328125, 205.56044006347656, 436.07659912109375], [0.043643951416015625, 343.05078125, 20.24236297607422, 385.3485107421875], [87.29902648925781, 359.16107177734375, 204.95997619628906, 437.05145263671875]], 'scores': [0.9444650411605835, 0.9282944798469543, 0.5239520072937012, 0.4162183701992035, 0.3726954758167267, 0.35989344120025635], 'class_indices': [0.0, 0.0, 27.0, 62.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 199.9ms\n",
      "Speed: 2.0ms preprocess, 199.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.2464141845703125, 301.6894836425781, 109.42074584960938, 479.5003356933594], [85.877685546875, 153.44564819335938, 555.9859619140625, 479.3412780761719], [87.60015869140625, 358.5484313964844, 205.61489868164062, 437.9235534667969], [0.047027587890625, 341.07244873046875, 20.777408599853516, 384.39459228515625]], 'scores': [0.9430812001228333, 0.9423767328262329, 0.5175334811210632, 0.3604780435562134], 'class_indices': [0.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 1 laptop, 197.8ms\n",
      "Speed: 2.0ms preprocess, 197.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[87.23454284667969, 154.02569580078125, 553.7925415039062, 479.49560546875], [0.25838470458984375, 300.4332275390625, 110.2942886352539, 479.51611328125], [87.74778747558594, 358.748291015625, 204.9852752685547, 436.6949462890625], [87.44110107421875, 358.46453857421875, 205.43658447265625, 435.37200927734375], [228.66470336914062, 429.7572937011719, 300.1241149902344, 479.7453308105469]], 'scores': [0.9400002956390381, 0.924525260925293, 0.3724427819252014, 0.34471654891967773, 0.30675047636032104], 'class_indices': [0.0, 0.0, 63.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 194.3ms\n",
      "Speed: 2.2ms preprocess, 194.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[87.635498046875, 154.3834228515625, 555.161376953125, 479.27288818359375], [0.2314453125, 300.6204833984375, 110.35806274414062, 479.51446533203125], [87.51231384277344, 358.7183837890625, 205.70240783691406, 434.66064453125]], 'scores': [0.9480049014091492, 0.8461215496063232, 0.40755394101142883], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 195.6ms\n",
      "Speed: 2.0ms preprocess, 195.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[87.68864440917969, 154.08966064453125, 553.9597778320312, 479.3505859375], [0.261871337890625, 300.9320068359375, 109.03694152832031, 479.4842529296875], [87.2625961303711, 358.69342041015625, 205.31851196289062, 440.1715087890625], [229.6907958984375, 427.9267578125, 299.9791259765625, 479.760498046875]], 'scores': [0.9431785941123962, 0.936129629611969, 0.4858490824699402, 0.451322466135025], 'class_indices': [0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 198.3ms\n",
      "Speed: 1.5ms preprocess, 198.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[87.83409118652344, 153.31011962890625, 554.1251220703125, 479.4366455078125], [0.2567291259765625, 300.63812255859375, 108.65928649902344, 479.484130859375], [87.39707946777344, 358.37945556640625, 205.51515197753906, 440.84173583984375], [228.86309814453125, 429.4679870605469, 300.34686279296875, 479.7368469238281]], 'scores': [0.9412306547164917, 0.9254959225654602, 0.41894811391830444, 0.3332880735397339], 'class_indices': [0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 230.9ms\n",
      "Speed: 2.1ms preprocess, 230.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[87.905517578125, 154.32757568359375, 556.5543823242188, 479.2696533203125], [0.2530059814453125, 300.56341552734375, 110.61135864257812, 479.66522216796875], [87.30136108398438, 358.422119140625, 205.70111083984375, 432.3466796875], [87.4138412475586, 358.6962585449219, 205.18197631835938, 434.4227600097656]], 'scores': [0.9438839554786682, 0.8708163499832153, 0.3838118314743042, 0.35734543204307556], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 222.4ms\n",
      "Speed: 2.0ms preprocess, 222.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[87.98468017578125, 154.005859375, 557.2733154296875, 479.29327392578125], [0.2425994873046875, 301.0797119140625, 110.7817153930664, 479.61004638671875], [87.32684326171875, 358.80450439453125, 205.7091064453125, 439.66448974609375]], 'scores': [0.9448480010032654, 0.9330480694770813, 0.5092827081680298], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 200.5ms\n",
      "Speed: 1.6ms preprocess, 200.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[90.51220703125, 153.93589782714844, 558.593505859375, 479.3131103515625], [0.2852783203125, 300.00335693359375, 109.91900634765625, 479.592529296875], [87.22025299072266, 358.53839111328125, 205.32949829101562, 441.57794189453125]], 'scores': [0.9482557773590088, 0.8711452484130859, 0.3072879910469055], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 198.0ms\n",
      "Speed: 1.7ms preprocess, 198.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[90.33924865722656, 153.44076538085938, 558.578125, 479.2875061035156], [0.2430419921875, 300.23907470703125, 109.50990295410156, 479.56488037109375], [87.38600158691406, 358.7303466796875, 205.53065490722656, 442.69970703125], [235.13516235351562, 426.290771484375, 305.6081237792969, 479.71923828125]], 'scores': [0.9477310180664062, 0.9381568431854248, 0.5221377015113831, 0.3352063298225403], 'class_indices': [0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 201.9ms\n",
      "Speed: 2.1ms preprocess, 201.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[89.287109375, 153.001953125, 559.358154296875, 479.489990234375], [0.27272796630859375, 299.89312744140625, 110.35858917236328, 479.59674072265625], [87.23739624023438, 358.7911376953125, 205.72262573242188, 441.1668701171875], [205.90866088867188, 273.3189697265625, 251.13864135742188, 380.816162109375]], 'scores': [0.9511828422546387, 0.9332072138786316, 0.510854959487915, 0.3021915555000305], 'class_indices': [0.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 2 laptops, 200.6ms\n",
      "Speed: 1.5ms preprocess, 200.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[88.9774169921875, 152.7327880859375, 558.3642578125, 479.27935791015625], [0.276611328125, 298.71173095703125, 110.64117431640625, 479.66619873046875], [87.31509399414062, 358.56549072265625, 205.091552734375, 436.68072509765625], [87.27557373046875, 358.5191345214844, 205.74868774414062, 435.0685729980469], [0.1350860595703125, 408.5108337402344, 26.04258918762207, 438.8689270019531], [234.41766357421875, 428.0281982421875, 305.08612060546875, 479.7315673828125]], 'scores': [0.9467373490333557, 0.8999680876731873, 0.4470849335193634, 0.3250899612903595, 0.3084920644760132, 0.2669861614704132], 'class_indices': [0.0, 0.0, 63.0, 62.0, 63.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 tv, 1 laptop, 197.2ms\n",
      "Speed: 2.1ms preprocess, 197.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[89.4322509765625, 153.92401123046875, 557.099609375, 479.50372314453125], [0.36806488037109375, 297.90771484375, 109.31420135498047, 479.669921875], [87.44666290283203, 358.78289794921875, 205.2012939453125, 442.62841796875], [87.32015991210938, 358.5020751953125, 206.03353881835938, 442.706298828125], [234.74087524414062, 428.586669921875, 305.9900817871094, 479.7423095703125], [205.4576416015625, 297.35577392578125, 250.45077514648438, 381.22821044921875]], 'scores': [0.9482128620147705, 0.8936285376548767, 0.5027605891227722, 0.3683260381221771, 0.26688313484191895, 0.2602120041847229], 'class_indices': [0.0, 0.0, 63.0, 62.0, 27.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 tvs, 196.8ms\n",
      "Speed: 2.1ms preprocess, 196.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[89.55113220214844, 153.96112060546875, 557.539306640625, 479.28759765625], [0.33758544921875, 298.115234375, 110.15333557128906, 479.467529296875], [87.1700210571289, 358.82940673828125, 205.78482055664062, 442.44000244140625], [204.36322021484375, 292.4595947265625, 251.07000732421875, 381.1068115234375], [87.174560546875, 358.47906494140625, 206.091064453125, 416.47210693359375]], 'scores': [0.9485228657722473, 0.9189974665641785, 0.4411008954048157, 0.30486321449279785, 0.2842225730419159], 'class_indices': [0.0, 0.0, 62.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 193.7ms\n",
      "Speed: 2.2ms preprocess, 193.7ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[89.07122802734375, 153.8487548828125, 557.4619140625, 479.29534912109375], [0.3496551513671875, 298.5, 110.19157409667969, 479.36676025390625], [87.14035034179688, 358.885009765625, 205.54757690429688, 440.3974609375], [202.7269744873047, 269.5557861328125, 251.5084991455078, 381.00006103515625]], 'scores': [0.9470305442810059, 0.9295202493667603, 0.4341491758823395, 0.254870742559433], 'class_indices': [0.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 tv, 199.6ms\n",
      "Speed: 1.7ms preprocess, 199.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[88.87702941894531, 154.19271850585938, 558.6721801757812, 479.2593688964844], [0.33644866943359375, 298.59771728515625, 111.9583969116211, 479.53558349609375], [196.1788330078125, 298.8621826171875, 250.9189453125, 381.0360107421875], [87.25680541992188, 358.9913330078125, 206.01199340820312, 435.3702392578125], [234.3988494873047, 429.60565185546875, 306.69525146484375, 479.78570556640625]], 'scores': [0.951470673084259, 0.907734215259552, 0.5382441282272339, 0.4207048714160919, 0.380764365196228], 'class_indices': [0.0, 0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 195.5ms\n",
      "Speed: 1.9ms preprocess, 195.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[88.33595275878906, 153.7828369140625, 556.7056884765625, 479.2685546875], [0.28018951416015625, 297.6976318359375, 108.62821197509766, 479.5692138671875], [192.60952758789062, 298.288330078125, 250.31600952148438, 381.033447265625], [87.29048919677734, 359.2308349609375, 207.87734985351562, 442.200927734375]], 'scores': [0.9471933841705322, 0.9221196174621582, 0.5903751850128174, 0.4150094985961914], 'class_indices': [0.0, 0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 198.8ms\n",
      "Speed: 1.7ms preprocess, 198.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.82630920410156, 153.82269287109375, 551.9659423828125, 479.38427734375], [0.34888458251953125, 296.72222900390625, 108.97647857666016, 479.4334716796875], [189.92982482910156, 300.68670654296875, 245.8273468017578, 381.89666748046875], [87.31517028808594, 358.8858642578125, 208.38807678222656, 437.0050048828125]], 'scores': [0.9401857256889343, 0.9329022169113159, 0.5718077421188354, 0.4556874930858612], 'class_indices': [0.0, 0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 196.3ms\n",
      "Speed: 1.7ms preprocess, 196.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[79.66801452636719, 153.2550506591797, 556.0166625976562, 479.41864013671875], [0.3097991943359375, 297.18377685546875, 107.9417495727539, 479.59039306640625], [87.42288208007812, 359.1087951660156, 209.52447509765625, 433.9791564941406], [227.3374481201172, 419.1356506347656, 297.43389892578125, 479.6930847167969]], 'scores': [0.9376295804977417, 0.9148076772689819, 0.48048821091651917, 0.28856360912323], 'class_indices': [0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 202.4ms\n",
      "Speed: 1.7ms preprocess, 202.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[79.835205078125, 152.96841430664062, 557.502197265625, 479.3471984863281], [0.2871551513671875, 296.99102783203125, 109.96211242675781, 479.52362060546875], [87.14666748046875, 358.69927978515625, 209.18408203125, 430.9322509765625], [87.20330047607422, 359.0050048828125, 208.9730224609375, 431.8680419921875]], 'scores': [0.9430506229400635, 0.9119236469268799, 0.3757641911506653, 0.36720290780067444], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 199.9ms\n",
      "Speed: 1.6ms preprocess, 199.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[81.531494140625, 150.94436645507812, 568.1204833984375, 479.3385314941406], [0.31272125244140625, 296.6353759765625, 108.06935119628906, 479.51983642578125], [190.4499969482422, 295.1965637207031, 248.4451446533203, 386.8398742675781], [87.38733673095703, 359.1883850097656, 207.15093994140625, 440.3999938964844]], 'scores': [0.9431453347206116, 0.9218367338180542, 0.7911843657493591, 0.44522377848625183], 'class_indices': [0.0, 0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 1 laptop, 194.0ms\n",
      "Speed: 1.9ms preprocess, 194.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[80.41192626953125, 149.87721252441406, 560.0120239257812, 479.3348388671875], [0.27237701416015625, 297.02838134765625, 109.43242645263672, 479.60833740234375], [190.576171875, 296.0372314453125, 248.02908325195312, 385.8287353515625], [87.34103393554688, 358.89935302734375, 207.97146606445312, 433.2169189453125], [87.49630737304688, 359.1121826171875, 207.209228515625, 434.06591796875]], 'scores': [0.9462031126022339, 0.9313456416130066, 0.7354664206504822, 0.41749274730682373, 0.3119224011898041], 'class_indices': [0.0, 0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 192.2ms\n",
      "Speed: 1.8ms preprocess, 192.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[80.60931396484375, 150.87298583984375, 562.09521484375, 479.298583984375], [0.3271942138671875, 297.00994873046875, 108.82966613769531, 479.53643798828125], [189.79290771484375, 296.6422119140625, 246.29376220703125, 385.8935546875], [87.123291015625, 358.7939453125, 209.18865966796875, 437.2003173828125]], 'scores': [0.9376683235168457, 0.9173925518989563, 0.5327490568161011, 0.4671919345855713], 'class_indices': [0.0, 0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 194.2ms\n",
      "Speed: 2.2ms preprocess, 194.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[81.45188903808594, 152.60391235351562, 558.6461181640625, 479.3223571777344], [0.31328582763671875, 296.91571044921875, 109.0269546508789, 479.36639404296875], [87.40753936767578, 359.329345703125, 207.3720703125, 436.1988525390625], [225.7059326171875, 417.19696044921875, 293.58282470703125, 479.69964599609375]], 'scores': [0.9473764300346375, 0.9256970286369324, 0.46574267745018005, 0.2588888108730316], 'class_indices': [0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tv, 1 laptop, 197.1ms\n",
      "Speed: 1.7ms preprocess, 197.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.41761779785156, 155.5287628173828, 559.7232666015625, 479.27752685546875], [0.32180023193359375, 297.3251953125, 116.3805923461914, 479.34033203125], [87.50785827636719, 358.620361328125, 206.13404846191406, 421.27386474609375], [187.39321899414062, 352.7268371582031, 239.60964965820312, 385.9384460449219], [87.46270751953125, 358.88787841796875, 205.72232055664062, 419.957763671875], [0.013643264770507812, 342.0126647949219, 15.67984390258789, 385.0431213378906]], 'scores': [0.9475167393684387, 0.8811453580856323, 0.3570568263530731, 0.3196786046028137, 0.3169423043727875, 0.25350314378738403], 'class_indices': [0.0, 0.0, 62.0, 0.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tv, 197.1ms\n",
      "Speed: 1.4ms preprocess, 197.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[81.6859130859375, 155.7474365234375, 559.6092529296875, 479.31414794921875], [0.35388946533203125, 297.462890625, 110.14418029785156, 479.3876953125], [87.28850555419922, 359.104736328125, 205.63604736328125, 433.7261962890625], [187.05972290039062, 355.28125, 240.30361938476562, 386.377685546875], [0.0175018310546875, 342.1066589355469, 16.390716552734375, 384.8348693847656]], 'scores': [0.9491313695907593, 0.9345135688781738, 0.44920995831489563, 0.3794424831867218, 0.300678014755249], 'class_indices': [0.0, 0.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 laptop, 198.9ms\n",
      "Speed: 1.5ms preprocess, 198.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.3291473388671875, 296.69805908203125, 109.2359848022461, 479.40802001953125], [82.23779296875, 154.5668487548828, 563.7265625, 479.39178466796875], [87.5744857788086, 358.50018310546875, 203.48983764648438, 436.30120849609375], [224.26516723632812, 419.23297119140625, 291.5715637207031, 479.81549072265625], [185.67501831054688, 353.4927978515625, 242.12753295898438, 385.1458740234375], [0.025266647338867188, 341.11737060546875, 16.546133041381836, 384.44720458984375]], 'scores': [0.9334040880203247, 0.9216415286064148, 0.5719006061553955, 0.5693053007125854, 0.5213344693183899, 0.26189833879470825], 'class_indices': [0.0, 0.0, 63.0, 27.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tv, 198.5ms\n",
      "Speed: 1.9ms preprocess, 198.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.13427734375, 152.91921997070312, 565.0509033203125, 479.2497863769531], [0.3761749267578125, 298.47314453125, 110.40947723388672, 479.32275390625], [87.36542510986328, 359.5118408203125, 202.36920166015625, 436.191162109375], [183.08099365234375, 353.30120849609375, 247.619873046875, 385.2711181640625], [0.020486831665039062, 342.21636962890625, 16.878263473510742, 385.04681396484375]], 'scores': [0.94471275806427, 0.9260123372077942, 0.4872075915336609, 0.32797878980636597, 0.26731356978416443], 'class_indices': [0.0, 0.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tv, 1 laptop, 203.7ms\n",
      "Speed: 1.6ms preprocess, 203.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.3380126953125, 150.88082885742188, 568.2510986328125, 479.3650207519531], [0.2986297607421875, 298.2975158691406, 110.66026306152344, 479.6089172363281], [179.5413360595703, 350.68536376953125, 251.6215362548828, 384.99761962890625], [87.39835357666016, 359.50665283203125, 198.57522583007812, 439.03594970703125], [179.24057006835938, 306.8800048828125, 251.69393920898438, 385.0006103515625], [87.61672973632812, 359.55462646484375, 198.58908081054688, 438.22833251953125]], 'scores': [0.9511231184005737, 0.9345569014549255, 0.39905065298080444, 0.3772220313549042, 0.35423147678375244, 0.27719125151634216], 'class_indices': [0.0, 0.0, 0.0, 62.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 217.4ms\n",
      "Speed: 2.0ms preprocess, 217.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[85.01806640625, 149.896484375, 570.9529418945312, 479.28997802734375], [0.3304901123046875, 297.7003479003906, 108.68421936035156, 479.6323547363281], [87.42732238769531, 359.3319091796875, 196.1474151611328, 443.97943115234375]], 'scores': [0.9455336332321167, 0.9049513936042786, 0.5510507225990295], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 224.7ms\n",
      "Speed: 2.3ms preprocess, 224.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.41130065917969, 149.63494873046875, 567.9070434570312, 479.26806640625], [0.3297576904296875, 298.96502685546875, 110.11601257324219, 479.38104248046875], [232.71099853515625, 419.2322998046875, 302.116943359375, 479.7115478515625], [87.292236328125, 360.049560546875, 193.87252807617188, 440.3121337890625]], 'scores': [0.945706844329834, 0.927598774433136, 0.377217561006546, 0.31171077489852905], 'class_indices': [0.0, 0.0, 27.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 224.9ms\n",
      "Speed: 1.9ms preprocess, 224.9ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.92071533203125, 149.4315185546875, 569.6669311523438, 479.29669189453125], [0.376739501953125, 298.6386413574219, 108.30509948730469, 479.4170227050781], [87.45028686523438, 359.3359375, 196.08956909179688, 442.402587890625], [87.50707244873047, 359.4896240234375, 196.7132568359375, 443.36236572265625]], 'scores': [0.9463961124420166, 0.933780312538147, 0.39535483717918396, 0.3517334461212158], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 205.1ms\n",
      "Speed: 2.1ms preprocess, 205.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.96868896484375, 149.9947509765625, 568.5347290039062, 479.3419189453125], [0.34630584716796875, 298.48687744140625, 108.2258071899414, 479.55780029296875], [173.20547485351562, 310.78753662109375, 253.55654907226562, 387.08795166015625], [232.73770141601562, 418.98291015625, 301.9368591308594, 479.72125244140625]], 'scores': [0.9520158767700195, 0.9081197381019592, 0.42362698912620544, 0.2528127431869507], 'class_indices': [0.0, 0.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 2 laptops, 198.0ms\n",
      "Speed: 2.5ms preprocess, 198.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.67169189453125, 149.79608154296875, 569.4303588867188, 479.3048095703125], [0.39527130126953125, 298.1905517578125, 111.04045104980469, 479.544677734375], [87.44284057617188, 359.57843017578125, 190.50543212890625, 433.55706787109375], [0.11115074157714844, 407.6780090332031, 25.509056091308594, 431.0427551269531], [87.28884887695312, 359.70556640625, 190.31048583984375, 435.1688232421875]], 'scores': [0.9479882717132568, 0.9298306703567505, 0.4865678548812866, 0.2949284017086029, 0.28273624181747437], 'class_indices': [0.0, 0.0, 62.0, 63.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 tv, 198.0ms\n",
      "Speed: 1.7ms preprocess, 198.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.77215576171875, 150.55731201171875, 568.4677734375, 479.2877197265625], [0.29410552978515625, 298.116455078125, 109.82740020751953, 479.61865234375], [173.57125854492188, 296.7203369140625, 253.88681030273438, 387.39892578125], [87.3299560546875, 360.13946533203125, 191.5880126953125, 440.01239013671875], [233.11471557617188, 419.766845703125, 301.6939392089844, 479.6871337890625]], 'scores': [0.947869062423706, 0.9185386300086975, 0.6659042835235596, 0.3862234354019165, 0.34743160009384155], 'class_indices': [0.0, 0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 tv, 208.0ms\n",
      "Speed: 2.0ms preprocess, 208.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.04127502441406, 150.57232666015625, 567.208251953125, 479.291748046875], [0.31097412109375, 298.90374755859375, 109.28465270996094, 479.57867431640625], [87.50363159179688, 359.365966796875, 194.03317260742188, 441.67706298828125], [176.10110473632812, 312.3023376464844, 254.845458984375, 386.9330139160156], [232.89527893066406, 419.1466369628906, 302.06689453125, 479.7204284667969]], 'scores': [0.9479222893714905, 0.87702876329422, 0.5411680340766907, 0.4507208466529846, 0.3805464804172516], 'class_indices': [0.0, 0.0, 62.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 208.4ms\n",
      "Speed: 1.9ms preprocess, 208.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.07337951660156, 150.1991729736328, 567.669189453125, 479.27496337890625], [0.38993072509765625, 298.77227783203125, 108.98258972167969, 479.4437255859375], [87.3005599975586, 359.79022216796875, 193.63906860351562, 442.95428466796875], [179.8734893798828, 312.7825012207031, 254.68605041503906, 386.6280822753906]], 'scores': [0.946032702922821, 0.9306035041809082, 0.5335960984230042, 0.4933738112449646], 'class_indices': [0.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 223.5ms\n",
      "Speed: 2.1ms preprocess, 223.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[85.24163818359375, 150.3256072998047, 566.18310546875, 479.27374267578125], [0.3351593017578125, 299.02435302734375, 109.03190612792969, 479.47149658203125], [87.16863250732422, 359.85833740234375, 199.62188720703125, 443.44195556640625]], 'scores': [0.9440666437149048, 0.9234460592269897, 0.4073241651058197], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 tvs, 201.9ms\n",
      "Speed: 1.7ms preprocess, 201.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.47955322265625, 150.55642700195312, 567.5274658203125, 479.3359069824219], [0.3364410400390625, 298.14300537109375, 110.54717254638672, 479.635498046875], [87.03546142578125, 359.0715637207031, 203.3382568359375, 417.0024108886719], [87.27017211914062, 359.3465881347656, 203.19378662109375, 443.2210388183594], [7.350044250488281, 298.81646728515625, 101.2372817993164, 397.12738037109375]], 'scores': [0.9474895000457764, 0.6866586208343506, 0.3817170560359955, 0.3182695209980011, 0.2799176573753357], 'class_indices': [0.0, 0.0, 62.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 198.2ms\n",
      "Speed: 1.7ms preprocess, 198.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[85.05982971191406, 150.38922119140625, 567.560791015625, 479.31610107421875], [0.37877655029296875, 297.6455383300781, 109.19722747802734, 479.6804504394531], [87.46621704101562, 359.73114013671875, 201.48654174804688, 442.63165283203125], [9.435218811035156, 298.5933837890625, 99.6265640258789, 398.10791015625]], 'scores': [0.9444323778152466, 0.586702287197113, 0.5363584160804749, 0.4116919934749603], 'class_indices': [0.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 197.2ms\n",
      "Speed: 1.7ms preprocess, 197.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.02433776855469, 150.61598205566406, 569.0823364257812, 479.2711181640625], [0.38526153564453125, 298.11090087890625, 109.22219848632812, 479.686279296875], [87.38412475585938, 359.21575927734375, 200.28311157226562, 443.29534912109375]], 'scores': [0.9449064135551453, 0.8835498690605164, 0.46539080142974854], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 198.9ms\n",
      "Speed: 1.4ms preprocess, 198.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.47027587890625, 150.35208129882812, 565.870849609375, 479.3452453613281], [0.4162445068359375, 298.88250732421875, 110.72801208496094, 479.44232177734375], [87.40130615234375, 359.2401123046875, 199.15435791015625, 438.9884033203125], [230.35760498046875, 420.6094055175781, 301.4718017578125, 479.7053527832031]], 'scores': [0.9440776109695435, 0.9158498644828796, 0.5339226722717285, 0.4391454756259918], 'class_indices': [0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 194.7ms\n",
      "Speed: 1.6ms preprocess, 194.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.0306396484375, 150.3312530517578, 566.94677734375, 479.29144287109375], [0.393463134765625, 299.932861328125, 109.4627456665039, 479.5008544921875], [87.30608367919922, 359.35235595703125, 200.6502685546875, 442.3074951171875], [235.7532958984375, 419.5074462890625, 301.84295654296875, 479.7236328125]], 'scores': [0.9440479278564453, 0.9259229302406311, 0.4445207417011261, 0.3095603585243225], 'class_indices': [0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 tv, 199.4ms\n",
      "Speed: 1.7ms preprocess, 199.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.9921875, 150.75994873046875, 566.0076293945312, 479.29315185546875], [0.31972503662109375, 299.4266357421875, 109.8049545288086, 479.3255615234375], [178.40322875976562, 354.589111328125, 254.14602661132812, 386.649658203125], [232.64810180664062, 419.630615234375, 302.2896423339844, 479.7169189453125], [87.46099853515625, 359.7027587890625, 196.2281494140625, 439.274169921875], [0.03661823272705078, 345.0115966796875, 14.671346664428711, 383.3865966796875]], 'scores': [0.9411218762397766, 0.9001328349113464, 0.416897177696228, 0.40512990951538086, 0.3698830306529999, 0.2953549325466156], 'class_indices': [0.0, 0.0, 0.0, 27.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 tv, 196.4ms\n",
      "Speed: 2.0ms preprocess, 196.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.50628662109375, 151.25396728515625, 567.1181640625, 479.207275390625], [0.32926177978515625, 299.60552978515625, 109.98442840576172, 479.3544921875], [87.41925048828125, 359.5267333984375, 198.65908813476562, 436.051513671875], [0.04280281066894531, 344.93243408203125, 14.45402717590332, 383.87139892578125], [232.95440673828125, 418.93341064453125, 301.73321533203125, 479.71673583984375]], 'scores': [0.936672031879425, 0.9221649169921875, 0.3233606815338135, 0.3131536543369293, 0.2620886266231537], 'class_indices': [0.0, 0.0, 62.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 tv, 1 laptop, 194.7ms\n",
      "Speed: 1.8ms preprocess, 194.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.9940185546875, 150.74530029296875, 567.1123657226562, 479.32122802734375], [0.26610565185546875, 299.23992919921875, 108.60564422607422, 479.49749755859375], [0.045258522033691406, 344.70709228515625, 13.792069435119629, 384.00146484375], [87.34076690673828, 359.99725341796875, 197.6322021484375, 440.43463134765625], [87.30624389648438, 359.81036376953125, 197.65426635742188, 440.45513916015625], [179.08653259277344, 354.64801025390625, 251.9262237548828, 386.09136962890625], [231.47320556640625, 419.7120361328125, 301.0635986328125, 479.6793212890625]], 'scores': [0.9423247575759888, 0.9281485080718994, 0.4401470720767975, 0.39415445923805237, 0.35431939363479614, 0.2781098484992981, 0.2502347528934479], 'class_indices': [0.0, 0.0, 0.0, 63.0, 62.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tv, 193.9ms\n",
      "Speed: 1.7ms preprocess, 193.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.524658203125, 150.70849609375, 568.099609375, 479.281494140625], [0.28702545166015625, 299.5455322265625, 109.63871002197266, 479.3642578125], [87.33999633789062, 360.0848388671875, 197.41921997070312, 436.681396484375], [0.032416343688964844, 345.02642822265625, 14.117342948913574, 384.01153564453125], [178.77584838867188, 354.44073486328125, 251.1871337890625, 385.73480224609375]], 'scores': [0.9425893425941467, 0.9164361357688904, 0.40323910117149353, 0.38362857699394226, 0.32265278697013855], 'class_indices': [0.0, 0.0, 62.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 1 laptop, 204.4ms\n",
      "Speed: 1.6ms preprocess, 204.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[81.70875549316406, 150.8797607421875, 568.39453125, 479.22552490234375], [0.28871917724609375, 300.0108642578125, 109.93621063232422, 479.4920654296875], [87.378662109375, 359.63531494140625, 199.46975708007812, 436.16607666015625], [0.035439491271972656, 344.58648681640625, 14.227408409118652, 384.12652587890625], [87.30567169189453, 360.10247802734375, 196.85009765625, 437.39581298828125]], 'scores': [0.9440829157829285, 0.9318404197692871, 0.42057427763938904, 0.32774803042411804, 0.26977473497390747], 'class_indices': [0.0, 0.0, 62.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tv, 197.1ms\n",
      "Speed: 1.8ms preprocess, 197.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.2823486328125, 150.84130859375, 568.8645629882812, 479.26055908203125], [0.3185272216796875, 299.8491516113281, 111.37200927734375, 479.4441833496094], [0.03462696075439453, 344.4841003417969, 14.3947172164917, 384.1532897949219], [87.47757720947266, 359.74102783203125, 196.46060180664062, 434.849609375], [178.96923828125, 354.1605224609375, 251.24957275390625, 386.061279296875]], 'scores': [0.944506824016571, 0.9141499400138855, 0.4649139940738678, 0.4349074959754944, 0.3786800801753998], 'class_indices': [0.0, 0.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 tv, 1 laptop, 199.7ms\n",
      "Speed: 1.7ms preprocess, 199.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.8760986328125, 150.36764526367188, 566.9052734375, 479.3262634277344], [0.24981689453125, 299.7138671875, 109.88204956054688, 479.4417724609375], [0.1455841064453125, 412.5343017578125, 30.54436683654785, 435.82861328125], [87.55592346191406, 359.6318359375, 198.3643341064453, 434.942138671875], [228.68792724609375, 419.87548828125, 300.53253173828125, 479.701416015625], [0.035457611083984375, 344.6793212890625, 14.182512283325195, 384.1546630859375]], 'scores': [0.9422866702079773, 0.8904911279678345, 0.5564236044883728, 0.5344120264053345, 0.3737882971763611, 0.28004300594329834], 'class_indices': [0.0, 0.0, 63.0, 62.0, 27.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tv, 195.4ms\n",
      "Speed: 1.9ms preprocess, 195.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.881591796875, 150.46632385253906, 568.1385498046875, 479.2945556640625], [0.2742462158203125, 299.75421142578125, 110.05638122558594, 479.48492431640625], [0.03798961639404297, 344.5469970703125, 14.422999382019043, 384.29736328125], [87.48008728027344, 359.2293701171875, 199.43199157714844, 439.0943603515625], [178.89181518554688, 354.1171875, 251.56442260742188, 386.7686767578125]], 'scores': [0.9465199708938599, 0.9318348169326782, 0.42112594842910767, 0.4024241864681244, 0.37128111720085144], 'class_indices': [0.0, 0.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 201.9ms\n",
      "Speed: 1.7ms preprocess, 201.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[87.29168701171875, 150.20777893066406, 567.2183837890625, 479.3072509765625], [0.2840118408203125, 298.9967956542969, 109.86724853515625, 479.4176330566406], [87.28899383544922, 359.533935546875, 197.909423828125, 441.7913818359375], [0.03543853759765625, 344.7322998046875, 13.425908088684082, 384.36798095703125]], 'scores': [0.945874035358429, 0.9257122278213501, 0.4435921609401703, 0.3392159938812256], 'class_indices': [0.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 tv, 206.9ms\n",
      "Speed: 1.9ms preprocess, 206.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[91.24713134765625, 150.451171875, 568.0709838867188, 479.26959228515625], [0.35799407958984375, 299.09051513671875, 110.07144927978516, 479.31341552734375], [0.03764533996582031, 344.62713623046875, 14.023558616638184, 384.96978759765625], [179.09713745117188, 354.1513671875, 251.16567993164062, 387.5106201171875], [227.81199645996094, 419.2008056640625, 300.10693359375, 479.719482421875], [87.39051055908203, 359.79107666015625, 196.3883056640625, 444.89959716796875]], 'scores': [0.9454196095466614, 0.9221223592758179, 0.4229847490787506, 0.41676315665245056, 0.36670246720314026, 0.32346856594085693], 'class_indices': [0.0, 0.0, 0.0, 0.0, 27.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 tv, 213.9ms\n",
      "Speed: 1.7ms preprocess, 213.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[90.73399353027344, 150.74940490722656, 568.54052734375, 479.2335205078125], [0.3824462890625, 299.40924072265625, 109.17121887207031, 479.3145751953125], [87.1815414428711, 359.7994384765625, 196.37118530273438, 444.151123046875], [179.32608032226562, 353.91064453125, 249.47659301757812, 387.0050048828125], [0.036388397216796875, 344.9471435546875, 13.524051666259766, 383.660888671875], [226.4540252685547, 419.5621337890625, 298.91143798828125, 479.7569580078125]], 'scores': [0.944234311580658, 0.9254786372184753, 0.4504680037498474, 0.4380394518375397, 0.4257891774177551, 0.3028070628643036], 'class_indices': [0.0, 0.0, 62.0, 0.0, 0.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tv, 200.5ms\n",
      "Speed: 3.2ms preprocess, 200.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[91.85516357421875, 150.49461364746094, 570.1846923828125, 479.1861572265625], [0.409393310546875, 299.3416748046875, 110.01378631591797, 479.2838134765625], [179.80386352539062, 353.8733215332031, 253.88229370117188, 383.6419982910156], [87.30215454101562, 359.5929870605469, 195.74777221679688, 441.8045349121094], [0.03802299499511719, 344.7787170410156, 13.582945823669434, 383.8337707519531]], 'scores': [0.9459173679351807, 0.9250507354736328, 0.43007147312164307, 0.40606430172920227, 0.29647529125213623], 'class_indices': [0.0, 0.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tv, 195.4ms\n",
      "Speed: 1.8ms preprocess, 195.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[91.7501220703125, 150.43450927734375, 570.2901611328125, 479.229736328125], [0.31047821044921875, 299.3139343261719, 110.91647338867188, 479.4399108886719], [179.48846435546875, 353.4453125, 255.04537963867188, 383.529052734375], [87.4585189819336, 359.3345947265625, 197.05300903320312, 440.0919189453125], [0.03673362731933594, 345.0904541015625, 14.28368854522705, 383.3870849609375]], 'scores': [0.9460437893867493, 0.9066948294639587, 0.5071583986282349, 0.4551111161708832, 0.3684832453727722], 'class_indices': [0.0, 0.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 197.9ms\n",
      "Speed: 2.1ms preprocess, 197.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[90.4808349609375, 151.107421875, 570.5743408203125, 479.3328857421875], [0.3583221435546875, 299.4024658203125, 110.68788146972656, 479.48828125], [87.35116577148438, 359.6595153808594, 196.42864990234375, 441.5785217285156], [0.03740978240966797, 345.0457763671875, 14.222596168518066, 383.45550537109375]], 'scores': [0.9485002160072327, 0.8970649838447571, 0.3852784335613251, 0.28826335072517395], 'class_indices': [0.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 203.7ms\n",
      "Speed: 1.5ms preprocess, 203.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[87.736328125, 151.68341064453125, 572.2952880859375, 479.34954833984375], [0.27094268798828125, 298.98004150390625, 108.93612670898438, 479.4874267578125], [179.54327392578125, 313.34405517578125, 262.537841796875, 381.17864990234375], [87.370361328125, 360.3061828613281, 195.56781005859375, 441.5104064941406]], 'scores': [0.9522852897644043, 0.9176038503646851, 0.6373745799064636, 0.49331846833229065], 'class_indices': [0.0, 0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 218.9ms\n",
      "Speed: 2.3ms preprocess, 218.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[92.89463806152344, 154.13296508789062, 573.575927734375, 479.3877868652344], [0.3082122802734375, 298.4417724609375, 111.81108093261719, 479.535400390625], [178.88253784179688, 309.71240234375, 266.08868408203125, 380.58038330078125], [87.34618377685547, 360.112548828125, 196.23406982421875, 444.655029296875]], 'scores': [0.9484536647796631, 0.9219948649406433, 0.7852531671524048, 0.3446911573410034], 'class_indices': [0.0, 0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 212.5ms\n",
      "Speed: 2.3ms preprocess, 212.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[106.8453369140625, 159.7425537109375, 571.9955444335938, 479.23876953125], [0.3203277587890625, 299.12518310546875, 112.91807556152344, 479.46124267578125], [180.37423706054688, 297.4176025390625, 269.8495178222656, 379.15814208984375]], 'scores': [0.9503822922706604, 0.8940344452857971, 0.7886742949485779], 'class_indices': [0.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 1 laptop, 211.3ms\n",
      "Speed: 1.7ms preprocess, 211.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[85.858642578125, 161.87969970703125, 563.7899169921875, 479.22021484375], [0.3565673828125, 299.28680419921875, 109.51515197753906, 479.43377685546875], [183.9664306640625, 304.88946533203125, 263.1375732421875, 374.98248291015625], [87.35865020751953, 360.28729248046875, 195.30865478515625, 431.40887451171875], [87.4686050415039, 359.86968994140625, 196.09512329101562, 431.56512451171875]], 'scores': [0.9458854794502258, 0.9150192141532898, 0.6041964292526245, 0.46496301889419556, 0.40541473031044006], 'class_indices': [0.0, 0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 laptop, 213.2ms\n",
      "Speed: 2.2ms preprocess, 213.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[80.91107177734375, 160.18307495117188, 550.1669921875, 479.2442932128906], [0.40789031982421875, 299.55322265625, 106.8995361328125, 479.36328125], [87.10771942138672, 359.92364501953125, 194.64447021484375, 425.96722412109375], [0.0314788818359375, 346.8026123046875, 12.452620506286621, 383.864501953125]], 'scores': [0.9406082630157471, 0.9049933552742004, 0.6865028738975525, 0.2992934584617615], 'class_indices': [0.0, 0.0, 63.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 221.0ms\n",
      "Speed: 1.9ms preprocess, 221.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[76.4149169921875, 158.98634338378906, 546.7077026367188, 479.3668212890625], [0.38545989990234375, 299.8843688964844, 109.1523666381836, 479.5584411621094], [86.95147705078125, 360.28509521484375, 193.69540405273438, 420.76190185546875], [87.28366088867188, 359.89727783203125, 194.88204956054688, 420.58392333984375]], 'scores': [0.9439824223518372, 0.8502399921417236, 0.3684948980808258, 0.327518105506897], 'class_indices': [0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 214.1ms\n",
      "Speed: 1.4ms preprocess, 214.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[85.1636962890625, 159.52633666992188, 555.478759765625, 479.1765441894531], [0.3375244140625, 299.73980712890625, 110.0970458984375, 479.36981201171875], [87.27740478515625, 360.12744140625, 195.41448974609375, 429.9864501953125]], 'scores': [0.9427709579467773, 0.9290830492973328, 0.4585563540458679], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 213.3ms\n",
      "Speed: 1.9ms preprocess, 213.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.6260986328125, 157.97186279296875, 567.388916015625, 479.23590087890625], [0.35028839111328125, 298.39801025390625, 109.27342987060547, 479.2952880859375], [87.02352905273438, 360.03033447265625, 194.5167236328125, 430.06207275390625], [87.27392578125, 359.8111267089844, 195.14682006835938, 429.9880676269531]], 'scores': [0.9496899843215942, 0.9123945236206055, 0.3550757169723511, 0.3076494634151459], 'class_indices': [0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 214.4ms\n",
      "Speed: 1.8ms preprocess, 214.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[81.51817321777344, 154.77792358398438, 566.2486572265625, 479.2177429199219], [0.3734588623046875, 297.90240478515625, 108.04169464111328, 479.47503662109375], [87.24433135986328, 359.7441101074219, 195.6634521484375, 427.6087341308594], [87.1467514038086, 360.02191162109375, 194.50100708007812, 428.02362060546875]], 'scores': [0.9455847144126892, 0.9154356122016907, 0.4043755829334259, 0.2595304548740387], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 212.1ms\n",
      "Speed: 1.6ms preprocess, 212.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[79.8387451171875, 153.67050170898438, 568.4522705078125, 479.2263488769531], [0.3662109375, 295.77978515625, 109.20964050292969, 479.447021484375], [87.58932495117188, 360.23748779296875, 194.22903442382812, 424.82257080078125], [87.5576400756836, 359.90411376953125, 195.54684448242188, 424.669677734375]], 'scores': [0.9462997913360596, 0.8840641379356384, 0.3602437376976013, 0.3493120074272156], 'class_indices': [0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 217.2ms\n",
      "Speed: 1.7ms preprocess, 217.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[85.61865234375, 152.86782836914062, 578.5974731445312, 479.3080139160156], [0.366058349609375, 293.25152587890625, 110.27659606933594, 479.42913818359375], [87.50714111328125, 359.65313720703125, 192.94619750976562, 421.97930908203125], [237.19430541992188, 418.22637939453125, 309.3332824707031, 479.70123291015625]], 'scores': [0.949653685092926, 0.8038732409477234, 0.385163277387619, 0.2897752523422241], 'class_indices': [0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 tv, 1 laptop, 215.5ms\n",
      "Speed: 2.0ms preprocess, 215.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.10450744628906, 152.89047241210938, 576.80810546875, 479.3308410644531], [0.37554931640625, 294.49945068359375, 110.13067626953125, 479.30120849609375], [183.32400512695312, 291.86236572265625, 255.86328125, 373.2557373046875], [87.53744506835938, 359.90655517578125, 191.7574462890625, 425.55828857421875], [87.41079711914062, 360.091796875, 191.29302978515625, 430.87664794921875], [236.5669403076172, 419.0654602050781, 308.06439208984375, 479.7157897949219]], 'scores': [0.9511604905128479, 0.9111970067024231, 0.6704356074333191, 0.33907321095466614, 0.27218982577323914, 0.2567528188228607], 'class_indices': [0.0, 0.0, 0.0, 62.0, 63.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 212.2ms\n",
      "Speed: 1.8ms preprocess, 212.2ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.86151123046875, 153.12835693359375, 575.327392578125, 479.33258056640625], [0.36583709716796875, 293.4302978515625, 109.44518280029297, 479.3594970703125], [181.93756103515625, 281.17254638671875, 253.94610595703125, 374.54022216796875], [87.59542083740234, 359.4217224121094, 192.88681030273438, 429.2288513183594]], 'scores': [0.9523320198059082, 0.9242398738861084, 0.7763757705688477, 0.44508811831474304], 'class_indices': [0.0, 0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 tv, 211.0ms\n",
      "Speed: 1.9ms preprocess, 211.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.51702880859375, 154.63343811035156, 576.0823974609375, 479.3458251953125], [0.39719390869140625, 294.8724670410156, 112.00493621826172, 479.3277893066406], [187.11766052246094, 277.7554626464844, 254.3351593017578, 374.1889343261719], [87.55843353271484, 359.5050048828125, 200.63589477539062, 427.791015625], [232.64256286621094, 419.60467529296875, 308.0101318359375, 479.74090576171875]], 'scores': [0.9530156850814819, 0.9254438281059265, 0.8361915946006775, 0.4716719388961792, 0.3468162417411804], 'class_indices': [0.0, 0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 1 laptop, 213.6ms\n",
      "Speed: 1.8ms preprocess, 213.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.86444091796875, 154.10671997070312, 576.487060546875, 479.3664245605469], [0.33649444580078125, 294.06451416015625, 112.77190399169922, 479.33642578125], [201.440185546875, 275.3868408203125, 254.3270263671875, 373.958984375], [87.56293487548828, 359.267822265625, 207.215576171875, 425.73455810546875], [87.49083709716797, 359.2591552734375, 206.42803955078125, 426.8443603515625]], 'scores': [0.9526675939559937, 0.9217553734779358, 0.8298190832138062, 0.3745424747467041, 0.3538789749145508], 'class_indices': [0.0, 0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 1 laptop, 213.2ms\n",
      "Speed: 1.6ms preprocess, 213.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[85.90631103515625, 153.87518310546875, 578.5061645507812, 479.3438720703125], [0.336212158203125, 294.7386169433594, 108.32515716552734, 479.3560485839844], [204.04641723632812, 276.6072692871094, 254.15713500976562, 373.6667785644531], [87.32891845703125, 358.998291015625, 205.83453369140625, 438.75146484375], [87.28825378417969, 359.03662109375, 204.9321746826172, 438.8280029296875]], 'scores': [0.9527243375778198, 0.9297629594802856, 0.47197243571281433, 0.43589964509010315, 0.41304123401641846], 'class_indices': [0.0, 0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 212.4ms\n",
      "Speed: 2.1ms preprocess, 212.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.37217712402344, 154.36297607421875, 573.8980712890625, 479.31719970703125], [0.33420562744140625, 295.77410888671875, 108.46186828613281, 479.38446044921875], [208.65097045898438, 284.18939208984375, 252.26205444335938, 374.10125732421875], [87.52651977539062, 359.00286865234375, 205.91244506835938, 437.54522705078125]], 'scores': [0.9489783644676208, 0.9185920357704163, 0.6911526918411255, 0.47418105602264404], 'class_indices': [0.0, 0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 1 laptop, 211.9ms\n",
      "Speed: 1.8ms preprocess, 211.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.96014404296875, 153.88638305664062, 573.7156982421875, 479.3142395019531], [0.34391021728515625, 295.0828552246094, 109.30009460449219, 479.3627014160156], [87.13203430175781, 358.8891906738281, 205.9571075439453, 435.5527038574219], [87.08378601074219, 358.84759521484375, 205.1900177001953, 435.32061767578125], [231.8133087158203, 420.95159912109375, 302.82757568359375, 479.71124267578125]], 'scores': [0.9513305425643921, 0.9327875375747681, 0.5302157998085022, 0.3815017342567444, 0.2607400417327881], 'class_indices': [0.0, 0.0, 62.0, 63.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 212.5ms\n",
      "Speed: 1.9ms preprocess, 212.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[85.76655578613281, 155.12005615234375, 573.0319213867188, 479.158447265625], [0.296417236328125, 295.6556396484375, 109.93110656738281, 479.4615478515625], [87.65126037597656, 358.9703063964844, 205.6824188232422, 435.3360290527344]], 'scores': [0.9507627487182617, 0.9087241291999817, 0.6106217503547668], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 221.4ms\n",
      "Speed: 2.1ms preprocess, 221.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[85.92152404785156, 154.84896850585938, 572.2867431640625, 479.2323913574219], [0.2441558837890625, 296.20281982421875, 110.23094177246094, 479.45562744140625], [87.47191619873047, 358.6142578125, 204.91766357421875, 434.5462646484375], [87.48883056640625, 358.6546325683594, 205.70364379882812, 434.8207092285156]], 'scores': [0.951295793056488, 0.9344097971916199, 0.49401336908340454, 0.4320968985557556], 'class_indices': [0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 214.4ms\n",
      "Speed: 1.6ms preprocess, 214.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[85.49755859375, 154.95831298828125, 572.8770751953125, 479.17449951171875], [0.33814239501953125, 296.88189697265625, 111.7609634399414, 479.41534423828125], [87.35421752929688, 358.6514892578125, 205.98947143554688, 431.914306640625], [229.18556213378906, 422.0763244628906, 300.197509765625, 479.7186584472656]], 'scores': [0.9520576000213623, 0.9397494792938232, 0.5380949378013611, 0.29179883003234863], 'class_indices': [0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 213.1ms\n",
      "Speed: 2.0ms preprocess, 213.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[86.25816345214844, 154.55953979492188, 573.8349609375, 479.3836975097656], [0.347930908203125, 296.83392333984375, 110.35855102539062, 479.31756591796875], [87.3753662109375, 358.5223388671875, 205.89266967773438, 435.548828125], [87.3494644165039, 358.42669677734375, 205.14395141601562, 435.49102783203125]], 'scores': [0.9491546750068665, 0.9331519603729248, 0.4809933602809906, 0.4499826431274414], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 210.0ms\n",
      "Speed: 1.9ms preprocess, 210.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.83880615234375, 154.20407104492188, 571.3248291015625, 479.3292541503906], [0.32196807861328125, 297.32525634765625, 111.52092742919922, 479.4339599609375], [87.47663116455078, 358.90838623046875, 205.93988037109375, 428.70074462890625], [87.41667175292969, 358.91571044921875, 205.35731506347656, 428.89337158203125]], 'scores': [0.953416109085083, 0.9368302822113037, 0.5289435386657715, 0.35807833075523376], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 216.1ms\n",
      "Speed: 1.7ms preprocess, 216.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.93916320800781, 153.30419921875, 571.9447631835938, 479.30389404296875], [0.41204833984375, 296.17205810546875, 112.17803192138672, 479.42462158203125], [87.58780670166016, 358.77923583984375, 206.32278442382812, 428.1783447265625]], 'scores': [0.9519834518432617, 0.8595577478408813, 0.5743306279182434], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 214.4ms\n",
      "Speed: 1.6ms preprocess, 214.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.67497253417969, 151.731201171875, 571.3663940429688, 479.28924560546875], [0.36649322509765625, 298.029052734375, 108.49861907958984, 479.2830810546875], [87.57408905029297, 358.788818359375, 205.86761474609375, 429.38238525390625], [225.27049255371094, 422.500732421875, 295.6795654296875, 479.749267578125]], 'scores': [0.953683614730835, 0.919900119304657, 0.5827310681343079, 0.3461567759513855], 'class_indices': [0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 1 laptop, 211.9ms\n",
      "Speed: 2.5ms preprocess, 211.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.99711608886719, 151.85862731933594, 573.2969360351562, 479.3018798828125], [0.38770294189453125, 297.72607421875, 110.19043731689453, 479.33837890625], [87.3844985961914, 358.888916015625, 205.87515258789062, 429.9366455078125], [223.99057006835938, 423.60418701171875, 295.6712341308594, 479.8079833984375], [87.36054992675781, 358.98455810546875, 205.2807159423828, 430.35467529296875]], 'scores': [0.9521595239639282, 0.9320967793464661, 0.5366325378417969, 0.5351333022117615, 0.3744591176509857], 'class_indices': [0.0, 0.0, 62.0, 27.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 210.0ms\n",
      "Speed: 1.8ms preprocess, 210.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.72611999511719, 152.29356384277344, 572.7454223632812, 479.28564453125], [0.2239532470703125, 294.85736083984375, 108.76200866699219, 479.49072265625], [87.33633422851562, 358.9729309082031, 205.881103515625, 433.1453552246094]], 'scores': [0.9522463083267212, 0.9307223558425903, 0.5682293176651001], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 1 laptop, 208.1ms\n",
      "Speed: 1.9ms preprocess, 208.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.48992919921875, 152.14642333984375, 572.87158203125, 479.291015625], [0.3304443359375, 295.2960205078125, 108.87103271484375, 479.277587890625], [87.18912506103516, 358.966796875, 205.01132202148438, 433.7276611328125], [87.26847076416016, 358.864013671875, 205.73306274414062, 433.974853515625], [225.27053833007812, 423.4512939453125, 296.8831481933594, 479.7911376953125]], 'scores': [0.950597882270813, 0.9248681664466858, 0.49399930238723755, 0.4164498746395111, 0.3563421070575714], 'class_indices': [0.0, 0.0, 63.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 201.3ms\n",
      "Speed: 1.6ms preprocess, 201.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.8431396484375, 152.170654296875, 572.3677978515625, 479.24786376953125], [0.30419921875, 294.3843994140625, 112.21435546875, 479.3826904296875], [87.51412963867188, 359.1542663574219, 205.7764892578125, 428.1943664550781], [224.6072998046875, 423.87518310546875, 295.98675537109375, 479.7850341796875]], 'scores': [0.9514983296394348, 0.9130697250366211, 0.5361606478691101, 0.2635668218135834], 'class_indices': [0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 202.4ms\n",
      "Speed: 1.7ms preprocess, 202.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.06727600097656, 151.70620727539062, 571.3876953125, 479.2519836425781], [0.28453826904296875, 293.92828369140625, 111.00155639648438, 479.37896728515625], [87.79611206054688, 359.1817626953125, 205.94635009765625, 429.08685302734375], [224.4120330810547, 423.7798156738281, 294.67083740234375, 479.7704772949219]], 'scores': [0.9530293345451355, 0.939046323299408, 0.5963702201843262, 0.31057286262512207], 'class_indices': [0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 200.8ms\n",
      "Speed: 1.8ms preprocess, 200.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.2467041015625, 150.95623779296875, 570.689453125, 479.31842041015625], [0.3658599853515625, 293.6600341796875, 108.95223999023438, 479.4720458984375], [87.2507095336914, 359.0074462890625, 204.99996948242188, 431.4578857421875]], 'scores': [0.954646110534668, 0.8624370098114014, 0.6289637684822083], 'class_indices': [0.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 1 laptop, 198.4ms\n",
      "Speed: 2.2ms preprocess, 198.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.815185546875, 150.773193359375, 570.6891479492188, 479.2529296875], [0.33745574951171875, 292.62396240234375, 109.3795394897461, 479.33233642578125], [87.40679931640625, 358.94012451171875, 205.04925537109375, 432.09954833984375], [87.4040756225586, 358.9797668457031, 205.82656860351562, 432.1471252441406], [224.96847534179688, 422.5540771484375, 294.8921813964844, 479.7806396484375]], 'scores': [0.9517442584037781, 0.9359316229820251, 0.4621541500091553, 0.42142483592033386, 0.3317532539367676], 'class_indices': [0.0, 0.0, 63.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 tv, 1 laptop, 205.3ms\n",
      "Speed: 2.0ms preprocess, 205.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.51275634765625, 150.7475128173828, 570.7306518554688, 479.28753662109375], [0.28331756591796875, 293.69976806640625, 109.9572982788086, 479.41375732421875], [87.41162109375, 359.132080078125, 204.82229614257812, 429.9791259765625], [87.43974304199219, 359.09515380859375, 205.57362365722656, 429.83258056640625], [224.62728881835938, 423.11505126953125, 294.9168395996094, 479.74395751953125], [215.16261291503906, 300.541015625, 245.13307189941406, 372.351806640625]], 'scores': [0.9554905295372009, 0.9366251826286316, 0.4497515857219696, 0.42238011956214905, 0.3853231370449066, 0.25163164734840393], 'class_indices': [0.0, 0.0, 63.0, 62.0, 27.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 laptop, 205.6ms\n",
      "Speed: 2.1ms preprocess, 205.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.54498291015625, 150.9286651611328, 570.7406005859375, 479.28289794921875], [0.334228515625, 293.7916259765625, 108.43846130371094, 479.34124755859375], [87.41036987304688, 359.00189208984375, 204.97689819335938, 431.92877197265625], [224.94992065429688, 423.25823974609375, 295.4141540527344, 479.79132080078125]], 'scores': [0.9554645419120789, 0.9032559990882874, 0.6473139524459839, 0.2869793772697449], 'class_indices': [0.0, 0.0, 63.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 201.3ms\n",
      "Speed: 1.5ms preprocess, 201.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.16363525390625, 151.6639404296875, 570.048828125, 479.2542724609375], [0.34400177001953125, 293.46826171875, 109.3168716430664, 479.32080078125], [87.73665618896484, 359.236328125, 206.00119018554688, 429.77288818359375]], 'scores': [0.9536786675453186, 0.9266483187675476, 0.474631667137146], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 197.4ms\n",
      "Speed: 1.6ms preprocess, 197.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.76025390625, 151.81468200683594, 571.2102661132812, 479.2862548828125], [0.4013214111328125, 293.74407958984375, 110.11601257324219, 479.5123291015625], [87.71931457519531, 359.072509765625, 205.7935333251953, 428.642578125], [5.558509826660156, 295.00628662109375, 102.05785369873047, 396.49969482421875]], 'scores': [0.9540305137634277, 0.7851271629333496, 0.502898633480072, 0.2952599823474884], 'class_indices': [0.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 196.0ms\n",
      "Speed: 1.7ms preprocess, 196.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[82.15336608886719, 152.16854858398438, 570.8429565429688, 479.2650451660156], [0.3598175048828125, 293.52630615234375, 109.68720245361328, 479.34869384765625], [87.43133544921875, 358.9886779785156, 205.9630126953125, 429.9528503417969], [87.39948272705078, 359.06072998046875, 205.25323486328125, 429.94488525390625]], 'scores': [0.9525882005691528, 0.9309147596359253, 0.46354642510414124, 0.4182228147983551], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 198.1ms\n",
      "Speed: 1.7ms preprocess, 198.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[81.72369384765625, 152.4254913330078, 571.1329956054688, 479.26800537109375], [0.35019683837890625, 295.0152282714844, 109.37655639648438, 479.4451599121094], [87.67069244384766, 359.2454833984375, 205.72305297851562, 429.994384765625]], 'scores': [0.9518597722053528, 0.904068648815155, 0.5053322911262512], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 208.0ms\n",
      "Speed: 1.8ms preprocess, 208.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[81.36395263671875, 151.85516357421875, 572.1690673828125, 479.302734375], [0.3233489990234375, 294.47540283203125, 109.32884216308594, 479.39764404296875], [87.50770568847656, 359.2672119140625, 205.75328063964844, 427.41363525390625], [222.5958251953125, 422.52850341796875, 293.56982421875, 479.73895263671875]], 'scores': [0.9536591172218323, 0.9098563194274902, 0.4956315755844116, 0.3681170642375946], 'class_indices': [0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 214.6ms\n",
      "Speed: 2.2ms preprocess, 214.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[81.50291442871094, 151.9964599609375, 571.7427978515625, 479.31561279296875], [0.29436492919921875, 294.71966552734375, 108.46871948242188, 479.40704345703125], [87.36941528320312, 359.28045654296875, 205.70556640625, 430.24969482421875]], 'scores': [0.9535494446754456, 0.910317063331604, 0.5075051188468933], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 207.4ms\n",
      "Speed: 1.6ms preprocess, 207.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[81.7974853515625, 151.7171173095703, 570.953857421875, 479.33624267578125], [0.322784423828125, 293.1064453125, 108.28456115722656, 479.3626708984375], [87.6678695678711, 358.7003173828125, 205.90719604492188, 432.0416259765625]], 'scores': [0.9546932578086853, 0.9344527721405029, 0.5124872326850891], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 217.7ms\n",
      "Speed: 1.7ms preprocess, 217.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.22807312011719, 151.95143127441406, 570.2005004882812, 479.282958984375], [0.317413330078125, 293.52392578125, 109.79238891601562, 479.3863525390625], [87.54342651367188, 358.8798828125, 206.021484375, 430.897216796875]], 'scores': [0.9518141150474548, 0.9408546090126038, 0.5758407711982727], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 1 laptop, 210.3ms\n",
      "Speed: 1.4ms preprocess, 210.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.50018310546875, 152.03363037109375, 570.3353881835938, 479.26593017578125], [0.354034423828125, 293.7287902832031, 108.29458618164062, 479.3230285644531], [87.54910278320312, 359.256591796875, 205.78851318359375, 432.1102294921875], [87.58883666992188, 359.2410888671875, 205.07852172851562, 432.4556884765625], [223.9058074951172, 423.74114990234375, 293.48675537109375, 479.73321533203125]], 'scores': [0.9547287821769714, 0.9317579865455627, 0.4310971796512604, 0.38753458857536316, 0.3518565595149994], 'class_indices': [0.0, 0.0, 62.0, 63.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 197.0ms\n",
      "Speed: 1.8ms preprocess, 197.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.80865478515625, 153.04302978515625, 569.4803466796875, 479.16339111328125], [0.32830810546875, 294.7291259765625, 110.30015563964844, 479.380126953125], [87.69705200195312, 359.0312805175781, 205.69537353515625, 430.0486755371094], [87.78152465820312, 359.0877685546875, 204.89962768554688, 430.249267578125]], 'scores': [0.9529069066047668, 0.9025411009788513, 0.46904903650283813, 0.4554362893104553], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 203.9ms\n",
      "Speed: 1.8ms preprocess, 203.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.64013671875, 152.5890350341797, 569.6978149414062, 479.31341552734375], [0.348419189453125, 295.0655517578125, 108.99143981933594, 479.291748046875], [87.29168701171875, 359.01629638671875, 204.8272705078125, 432.29669189453125], [87.3381118774414, 358.993896484375, 205.58908081054688, 432.22222900390625]], 'scores': [0.9535692930221558, 0.909705400466919, 0.4952643811702728, 0.40465906262397766], 'class_indices': [0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 195.3ms\n",
      "Speed: 1.8ms preprocess, 195.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.2867431640625, 153.16079711914062, 568.9508056640625, 479.1645202636719], [0.3175506591796875, 294.1448974609375, 110.45144653320312, 479.4158935546875], [87.62663269042969, 359.0915222167969, 205.63710021972656, 429.5382385253906], [87.66224670410156, 359.15789794921875, 204.97288513183594, 429.76580810546875]], 'scores': [0.9534904956817627, 0.8995354771614075, 0.46523892879486084, 0.40271252393722534], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 207.0ms\n",
      "Speed: 2.3ms preprocess, 207.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.2103271484375, 152.66319274902344, 569.3143310546875, 479.2901611328125], [0.34989166259765625, 294.90631103515625, 109.5473403930664, 479.37164306640625], [87.51696014404297, 359.06640625, 205.58221435546875, 431.6932373046875], [0.039895057678222656, 347.51580810546875, 14.958687782287598, 385.23931884765625]], 'scores': [0.9515104293823242, 0.9339967966079712, 0.49065151810646057, 0.3170963525772095], 'class_indices': [0.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 212.5ms\n",
      "Speed: 1.9ms preprocess, 212.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.28839111328125, 152.45001220703125, 569.4124145507812, 479.279296875], [0.3348846435546875, 294.23651123046875, 110.53682708740234, 479.36395263671875], [87.67251586914062, 359.0682067871094, 205.63027954101562, 430.0149230957031]], 'scores': [0.9529128670692444, 0.9370687007904053, 0.6012689471244812], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 212.0ms\n",
      "Speed: 2.3ms preprocess, 212.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.65583801269531, 152.46279907226562, 569.8916625976562, 479.3045349121094], [0.36243438720703125, 294.59228515625, 110.42604064941406, 479.4539794921875], [87.54519653320312, 358.784912109375, 205.1202392578125, 430.6812744140625]], 'scores': [0.9516961574554443, 0.9205701351165771, 0.59433513879776], 'class_indices': [0.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 217.1ms\n",
      "Speed: 2.5ms preprocess, 217.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.63056945800781, 152.65086364746094, 569.2488403320312, 479.3260498046875], [0.3169708251953125, 294.3555603027344, 110.89813232421875, 479.3860778808594], [87.45855712890625, 358.73089599609375, 205.74838256835938, 430.87933349609375], [87.44795227050781, 358.69110107421875, 205.08799743652344, 430.87078857421875]], 'scores': [0.9521127939224243, 0.9379987120628357, 0.4828210771083832, 0.47053059935569763], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 2 laptops, 207.2ms\n",
      "Speed: 2.0ms preprocess, 207.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.32086181640625, 153.0907440185547, 569.6220703125, 479.31353759765625], [0.3297576904296875, 296.309814453125, 110.57861328125, 479.439208984375], [87.26171875, 358.98468017578125, 204.91937255859375, 430.65338134765625], [87.31369018554688, 358.98468017578125, 205.6640625, 430.64324951171875], [0.1508617401123047, 411.24884033203125, 27.26885414123535, 436.24420166015625]], 'scores': [0.9498706459999084, 0.9165537357330322, 0.5092817544937134, 0.4525626003742218, 0.33760181069374084], 'class_indices': [0.0, 0.0, 63.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 201.3ms\n",
      "Speed: 1.7ms preprocess, 201.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.97991943359375, 152.77398681640625, 569.8511962890625, 479.2799072265625], [0.4026336669921875, 297.3446044921875, 109.54571533203125, 479.4276123046875], [87.5208969116211, 359.0738525390625, 205.66024780273438, 431.30914306640625]], 'scores': [0.9506272673606873, 0.9176161289215088, 0.5416461229324341], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 1 laptop, 205.6ms\n",
      "Speed: 2.2ms preprocess, 205.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.97149658203125, 153.29608154296875, 570.16064453125, 479.1715087890625], [0.37762451171875, 298.231689453125, 110.21988677978516, 479.34375], [87.328125, 358.9425048828125, 205.83160400390625, 429.6478271484375], [87.2733154296875, 358.8983154296875, 205.17605590820312, 429.7974853515625], [226.13658142089844, 423.67584228515625, 294.5196533203125, 479.82696533203125]], 'scores': [0.9497870802879333, 0.9394118785858154, 0.5425354838371277, 0.3783740997314453, 0.25091224908828735], 'class_indices': [0.0, 0.0, 62.0, 63.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 199.4ms\n",
      "Speed: 2.0ms preprocess, 199.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.37367248535156, 152.34518432617188, 570.25341796875, 479.3122863769531], [0.4104156494140625, 298.572265625, 108.68219757080078, 479.356689453125], [87.41658020019531, 358.953857421875, 205.5456085205078, 431.5703125]], 'scores': [0.953468918800354, 0.9198961853981018, 0.5953090190887451], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 207.7ms\n",
      "Speed: 1.8ms preprocess, 207.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.45863342285156, 153.1193084716797, 570.48876953125, 479.20806884765625], [0.3683013916015625, 298.36981201171875, 109.12081909179688, 479.4482421875], [87.45793914794922, 358.8111572265625, 205.8099365234375, 431.5570068359375]], 'scores': [0.9506309628486633, 0.9159409999847412, 0.5975723266601562], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 211.5ms\n",
      "Speed: 2.1ms preprocess, 211.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.86106872558594, 152.62188720703125, 570.156494140625, 479.29071044921875], [0.3878936767578125, 298.99456787109375, 111.07727813720703, 479.34918212890625], [87.3621826171875, 358.78399658203125, 205.68484497070312, 428.56005859375], [0.13394546508789062, 411.238037109375, 28.52337646484375, 437.0909423828125]], 'scores': [0.9520608186721802, 0.9201738238334656, 0.5038849711418152, 0.4716607332229614], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 204.6ms\n",
      "Speed: 1.6ms preprocess, 204.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.1956787109375, 152.91973876953125, 569.6201171875, 479.18963623046875], [0.37917327880859375, 298.85137939453125, 109.70783233642578, 479.36602783203125], [87.32778930664062, 358.9093933105469, 205.79800415039062, 431.5102844238281]], 'scores': [0.9499140977859497, 0.9406123757362366, 0.5717789530754089], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 2 laptops, 207.7ms\n",
      "Speed: 2.0ms preprocess, 207.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.8272705078125, 152.59072875976562, 569.6573486328125, 479.2590637207031], [0.33928680419921875, 298.69952392578125, 109.3438949584961, 479.37396240234375], [87.2729721069336, 358.8531494140625, 205.77389526367188, 430.3602294921875], [87.18270874023438, 358.86297607421875, 205.1678466796875, 430.61590576171875], [0.1484508514404297, 411.3128356933594, 28.983734130859375, 433.8407897949219]], 'scores': [0.9533982276916504, 0.9307156205177307, 0.5157585144042969, 0.40779349207878113, 0.2961687445640564], 'class_indices': [0.0, 0.0, 62.0, 63.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 207.2ms\n",
      "Speed: 1.5ms preprocess, 207.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.58482360839844, 152.2097625732422, 570.083251953125, 479.26837158203125], [0.3713836669921875, 299.00726318359375, 108.48661804199219, 479.34735107421875], [87.22694396972656, 358.83221435546875, 205.89125061035156, 431.81134033203125], [87.19330596923828, 358.86175537109375, 205.1861572265625, 431.64654541015625]], 'scores': [0.9497858881950378, 0.9345143437385559, 0.4828265309333801, 0.4732430577278137], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 209.4ms\n",
      "Speed: 2.1ms preprocess, 209.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.876708984375, 152.14990234375, 569.7200927734375, 479.27618408203125], [0.3917083740234375, 298.9488525390625, 110.5323715209961, 479.4801025390625], [87.61477661132812, 359.09014892578125, 205.58770751953125, 427.78887939453125]], 'scores': [0.9512936472892761, 0.9068440794944763, 0.5589423775672913], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 203.0ms\n",
      "Speed: 1.9ms preprocess, 203.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.61717224121094, 152.0780029296875, 570.1143798828125, 479.152099609375], [0.35626220703125, 298.79119873046875, 108.544921875, 479.42315673828125], [87.4688949584961, 358.5982360839844, 205.71273803710938, 432.1685485839844], [87.4706039428711, 358.5897216796875, 205.03097534179688, 432.198974609375]], 'scores': [0.9520743489265442, 0.937025249004364, 0.4973140060901642, 0.43387091159820557], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 206.8ms\n",
      "Speed: 2.2ms preprocess, 206.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.78375244140625, 152.53421020507812, 569.855224609375, 479.1776428222656], [0.39349365234375, 298.9769287109375, 110.23716735839844, 479.36376953125], [87.56327056884766, 358.7289123535156, 205.35745239257812, 429.5394592285156], [87.55810546875, 358.6917724609375, 206.00344848632812, 429.1468505859375]], 'scores': [0.9500680565834045, 0.930482029914856, 0.47378137707710266, 0.451322466135025], 'class_indices': [0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 207.6ms\n",
      "Speed: 2.4ms preprocess, 207.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.567626953125, 153.00320434570312, 569.558837890625, 479.1268005371094], [0.378143310546875, 299.34814453125, 111.32257843017578, 479.3455810546875], [87.2227783203125, 358.8234558105469, 205.27639770507812, 427.8176574707031], [87.29857635498047, 358.8145446777344, 205.84686279296875, 427.5765686035156]], 'scores': [0.951103150844574, 0.9259472489356995, 0.541025698184967, 0.4210437536239624], 'class_indices': [0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 203.8ms\n",
      "Speed: 1.8ms preprocess, 203.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.38978576660156, 152.9359130859375, 569.0447998046875, 479.14129638671875], [0.37017059326171875, 298.451416015625, 109.46989440917969, 479.53594970703125], [87.47265625, 358.703857421875, 205.83950805664062, 432.1888427734375], [87.53106689453125, 358.61773681640625, 205.091552734375, 432.04400634765625]], 'scores': [0.9497103095054626, 0.8693124651908875, 0.5301467180252075, 0.39535391330718994], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 211.2ms\n",
      "Speed: 1.7ms preprocess, 211.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.44859313964844, 153.354248046875, 569.0313720703125, 479.19512939453125], [0.36119842529296875, 299.08447265625, 110.11846923828125, 479.391357421875], [87.59912872314453, 359.00982666015625, 205.75909423828125, 430.62725830078125]], 'scores': [0.9515011310577393, 0.9365487098693848, 0.5969318151473999], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 208.0ms\n",
      "Speed: 1.8ms preprocess, 208.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.88926696777344, 152.7608642578125, 569.2572021484375, 479.2493896484375], [0.40534210205078125, 298.68194580078125, 109.04694366455078, 479.34442138671875], [87.33760833740234, 358.5970153808594, 205.71157836914062, 433.4172668457031], [87.3075180053711, 358.56396484375, 205.10037231445312, 433.41973876953125]], 'scores': [0.9516453146934509, 0.9361880421638489, 0.49886271357536316, 0.43141359090805054], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 196.6ms\n",
      "Speed: 1.8ms preprocess, 196.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.50091552734375, 153.96881103515625, 568.714599609375, 479.145263671875], [0.3511199951171875, 298.824951171875, 109.96507263183594, 479.409912109375], [87.52384948730469, 358.9200439453125, 205.77928161621094, 431.95166015625]], 'scores': [0.9513404369354248, 0.9173594117164612, 0.5548813343048096], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 205.4ms\n",
      "Speed: 1.7ms preprocess, 205.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.90753173828125, 153.80401611328125, 568.9666137695312, 479.13543701171875], [0.4335479736328125, 299.0504150390625, 110.83208465576172, 479.44927978515625], [87.41232299804688, 358.8590087890625, 205.71902465820312, 429.4873046875], [87.40788269042969, 358.76495361328125, 205.11778259277344, 429.81756591796875]], 'scores': [0.9493633508682251, 0.9100227355957031, 0.5278886556625366, 0.40431657433509827], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 206.2ms\n",
      "Speed: 1.7ms preprocess, 206.2ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.24334716796875, 153.65170288085938, 568.891357421875, 479.2019958496094], [0.3672332763671875, 299.0367126464844, 111.03416442871094, 479.3959045410156], [87.2186050415039, 358.76983642578125, 204.97915649414062, 430.39593505859375], [87.2647705078125, 358.8294677734375, 205.65426635742188, 430.218994140625]], 'scores': [0.9502189755439758, 0.9314002394676208, 0.5330895781517029, 0.45023736357688904], 'class_indices': [0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 208.2ms\n",
      "Speed: 1.7ms preprocess, 208.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.38348388671875, 153.8776397705078, 568.9569091796875, 479.20196533203125], [0.36548614501953125, 299.47271728515625, 110.0632095336914, 479.39794921875], [87.6549072265625, 358.74603271484375, 205.91961669921875, 431.09271240234375]], 'scores': [0.9485365748405457, 0.935363233089447, 0.598768413066864], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 205.3ms\n",
      "Speed: 1.8ms preprocess, 205.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.2532958984375, 153.82867431640625, 569.0043334960938, 479.20086669921875], [0.35668182373046875, 298.624267578125, 108.56391143798828, 479.3662109375], [87.32577514648438, 358.78302001953125, 205.64544677734375, 432.55523681640625]], 'scores': [0.9505118131637573, 0.940033495426178, 0.5045870542526245], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 210.0ms\n",
      "Speed: 1.9ms preprocess, 210.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.73712158203125, 153.91537475585938, 568.8649291992188, 479.1808166503906], [0.36536407470703125, 299.44439697265625, 108.4946517944336, 479.35223388671875], [87.39823150634766, 358.927734375, 205.81448364257812, 432.489501953125]], 'scores': [0.9506895542144775, 0.9309781789779663, 0.6079744100570679], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 203.9ms\n",
      "Speed: 2.2ms preprocess, 203.9ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.51043701171875, 153.38833618164062, 568.888427734375, 479.2569274902344], [0.3904571533203125, 298.7877502441406, 110.97111511230469, 479.3786315917969], [87.43848419189453, 358.65802001953125, 205.7659912109375, 429.25152587890625], [87.37095642089844, 358.6083984375, 205.1694793701172, 429.6353759765625]], 'scores': [0.9493570327758789, 0.9363506436347961, 0.5136894583702087, 0.4723599851131439], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 212.5ms\n",
      "Speed: 1.7ms preprocess, 212.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.35906982421875, 153.07066345214844, 568.8341674804688, 479.3096923828125], [0.34819793701171875, 299.32183837890625, 111.25674438476562, 479.43560791015625], [87.49897003173828, 358.72003173828125, 205.15655517578125, 430.27325439453125], [87.52035522460938, 358.73382568359375, 205.82369995117188, 430.26202392578125]], 'scores': [0.9498916268348694, 0.918954610824585, 0.5042392015457153, 0.46765080094337463], 'class_indices': [0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 1 laptop, 204.1ms\n",
      "Speed: 2.0ms preprocess, 204.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.57701110839844, 153.82073974609375, 567.711181640625, 479.1644287109375], [0.37060546875, 299.24420166015625, 110.21051025390625, 479.46734619140625], [87.52383422851562, 358.755615234375, 205.65313720703125, 430.033935546875], [87.54409790039062, 358.73297119140625, 205.00616455078125, 430.51116943359375], [223.72418212890625, 423.8484802246094, 293.6473388671875, 479.7630310058594]], 'scores': [0.9505529403686523, 0.9160205125808716, 0.5285269618034363, 0.38744109869003296, 0.2907135486602783], 'class_indices': [0.0, 0.0, 62.0, 63.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 200.1ms\n",
      "Speed: 2.1ms preprocess, 200.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.41015625, 153.3264923095703, 568.8564453125, 479.26947021484375], [0.3506622314453125, 299.4178466796875, 110.74298095703125, 479.409912109375], [87.42349243164062, 358.8468017578125, 205.54330444335938, 430.899658203125]], 'scores': [0.9496452808380127, 0.9422059059143066, 0.5601619482040405], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 203.4ms\n",
      "Speed: 2.3ms preprocess, 203.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.58905029296875, 153.88693237304688, 568.1033325195312, 479.2842712402344], [0.3250274658203125, 299.31005859375, 109.97508239746094, 479.4251708984375], [87.55560302734375, 358.78094482421875, 205.7828369140625, 431.24151611328125]], 'scores': [0.9494631886482239, 0.9398384690284729, 0.5950766801834106], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 207.3ms\n",
      "Speed: 2.4ms preprocess, 207.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.86376953125, 154.539794921875, 567.9541015625, 479.1376953125], [0.38259124755859375, 298.89697265625, 109.84220123291016, 479.3795166015625], [87.38314819335938, 358.79364013671875, 205.59310913085938, 434.2080078125]], 'scores': [0.9494447112083435, 0.9378395676612854, 0.6227620840072632], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 206.3ms\n",
      "Speed: 2.0ms preprocess, 206.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.671630859375, 154.33154296875, 567.508544921875, 479.17718505859375], [0.381072998046875, 298.8947448730469, 110.62239837646484, 479.5686340332031], [87.24101257324219, 358.6103210449219, 205.0863800048828, 431.7222595214844], [87.25975799560547, 358.65838623046875, 205.73114013671875, 431.67974853515625]], 'scores': [0.9479503631591797, 0.8368051648139954, 0.4952224791049957, 0.471798300743103], 'class_indices': [0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 202.0ms\n",
      "Speed: 1.9ms preprocess, 202.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.50701904296875, 154.21470642089844, 568.3194580078125, 479.1473388671875], [0.29866790771484375, 298.6436767578125, 110.38720703125, 479.44610595703125], [87.48709106445312, 358.80517578125, 205.6187744140625, 431.633544921875], [87.50349426269531, 358.85540771484375, 204.9553680419922, 431.95648193359375]], 'scores': [0.9502633213996887, 0.9180160164833069, 0.5152252316474915, 0.37640753388404846], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 211.6ms\n",
      "Speed: 1.3ms preprocess, 211.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.32989501953125, 153.75552368164062, 568.252197265625, 479.2643737792969], [0.307525634765625, 298.52398681640625, 112.59613037109375, 479.43927001953125], [87.43077087402344, 359.00115966796875, 205.6163787841797, 425.0098876953125]], 'scores': [0.9512028098106384, 0.9206147789955139, 0.5427144169807434], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 195.6ms\n",
      "Speed: 1.8ms preprocess, 195.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.198974609375, 154.43511962890625, 567.4688110351562, 479.16748046875], [0.3343048095703125, 299.35595703125, 110.90031433105469, 479.4569091796875], [87.37059783935547, 358.6251220703125, 205.845947265625, 429.8546142578125], [87.3463134765625, 358.6285400390625, 205.29708862304688, 430.5736083984375]], 'scores': [0.9501950740814209, 0.9156414270401001, 0.5288195013999939, 0.3848225474357605], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 219.8ms\n",
      "Speed: 1.8ms preprocess, 219.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.34910583496094, 154.11767578125, 568.1905517578125, 479.1500244140625], [0.40216064453125, 299.01580810546875, 109.3009033203125, 479.54876708984375], [87.5166244506836, 358.7455749511719, 205.79263305664062, 432.3311462402344], [223.48104858398438, 422.714111328125, 294.0470886230469, 479.77996826171875]], 'scores': [0.9501587748527527, 0.8330218195915222, 0.5875766277313232, 0.26196175813674927], 'class_indices': [0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 203.0ms\n",
      "Speed: 1.6ms preprocess, 203.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.83062744140625, 153.5565185546875, 568.6549072265625, 479.2938232421875], [0.37259674072265625, 299.21209716796875, 111.87474822998047, 479.41949462890625], [87.43746948242188, 358.8763122558594, 205.644287109375, 429.6263732910156]], 'scores': [0.9504539966583252, 0.937949538230896, 0.6689932346343994], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 200.2ms\n",
      "Speed: 1.5ms preprocess, 200.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.30604553222656, 153.67193603515625, 568.3779296875, 479.14996337890625], [0.4093017578125, 299.2243957519531, 109.93238830566406, 479.3822326660156], [87.53915405273438, 358.8970947265625, 205.451416015625, 430.0391845703125]], 'scores': [0.9509309530258179, 0.9333404302597046, 0.5981008410453796], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 205.6ms\n",
      "Speed: 1.7ms preprocess, 205.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.15667724609375, 153.0745086669922, 568.5462036132812, 479.23614501953125], [0.317840576171875, 298.585205078125, 110.1767578125, 479.4586181640625], [87.48164367675781, 358.88812255859375, 205.58653259277344, 431.05548095703125]], 'scores': [0.9505223631858826, 0.9100208878517151, 0.5981934666633606], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 205.5ms\n",
      "Speed: 2.8ms preprocess, 205.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.20111083984375, 153.38031005859375, 569.249755859375, 479.25286865234375], [0.433258056640625, 298.3916015625, 110.02550506591797, 479.623779296875], [87.53408813476562, 358.7346496582031, 205.656982421875, 431.1509094238281], [0.13524436950683594, 411.28448486328125, 28.582355499267578, 443.72381591796875]], 'scores': [0.9488506317138672, 0.8438036441802979, 0.5680087208747864, 0.3436247408390045], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 196.9ms\n",
      "Speed: 1.9ms preprocess, 196.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.84869384765625, 153.3765411376953, 568.9921875, 479.26885986328125], [0.40792083740234375, 298.80108642578125, 109.90132904052734, 479.36822509765625], [87.385009765625, 358.73895263671875, 205.6517333984375, 429.45684814453125]], 'scores': [0.9493034482002258, 0.9401816129684448, 0.581666111946106], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 197.0ms\n",
      "Speed: 1.8ms preprocess, 197.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.06103515625, 153.3344268798828, 569.1065063476562, 479.24127197265625], [0.347991943359375, 299.26513671875, 109.6845703125, 479.475830078125], [87.4306411743164, 358.7569274902344, 205.78018188476562, 430.4886169433594]], 'scores': [0.9494813680648804, 0.891366183757782, 0.565748393535614], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 205.3ms\n",
      "Speed: 1.9ms preprocess, 205.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.3516845703125, 154.110595703125, 568.2721557617188, 479.13714599609375], [0.3974151611328125, 299.3868408203125, 110.47555541992188, 479.48193359375], [87.5235366821289, 358.73614501953125, 205.87796020507812, 428.96295166015625]], 'scores': [0.949195146560669, 0.8840257525444031, 0.5210247039794922], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 211.2ms\n",
      "Speed: 1.8ms preprocess, 211.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.81622314453125, 153.83056640625, 569.586669921875, 479.25262451171875], [0.41345977783203125, 298.90069580078125, 110.70565032958984, 479.38116455078125], [87.54991912841797, 358.7723693847656, 205.7056884765625, 430.1376647949219]], 'scores': [0.9507519602775574, 0.9198532104492188, 0.6070006489753723], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 tv, 205.0ms\n",
      "Speed: 1.8ms preprocess, 205.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.54069519042969, 154.5072021484375, 568.4135131835938, 479.1085205078125], [0.37917327880859375, 299.125244140625, 111.0439453125, 479.3712158203125], [87.62605285644531, 358.9119873046875, 205.6679229736328, 429.2923583984375], [224.38909912109375, 423.690673828125, 294.38671875, 479.7650146484375]], 'scores': [0.9505606293678284, 0.9338949918746948, 0.6207471489906311, 0.297781765460968], 'class_indices': [0.0, 0.0, 62.0, 27.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 205.9ms\n",
      "Speed: 1.6ms preprocess, 205.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.9688720703125, 154.80303955078125, 568.0341796875, 479.151611328125], [0.36800384521484375, 298.9290771484375, 110.33139038085938, 479.4320068359375], [87.44161224365234, 358.71722412109375, 205.68350219726562, 430.33917236328125]], 'scores': [0.949028491973877, 0.9279606938362122, 0.5849055051803589], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 202.2ms\n",
      "Speed: 1.9ms preprocess, 202.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[83.71240234375, 154.49905395507812, 568.5731811523438, 479.2910461425781], [0.3975067138671875, 298.9641418457031, 110.7188720703125, 479.4628601074219], [87.45083618164062, 358.9121398925781, 205.42019653320312, 430.8634338378906]], 'scores': [0.9496363401412964, 0.9231339693069458, 0.5966284275054932], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 201.8ms\n",
      "Speed: 2.2ms preprocess, 201.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.25189208984375, 157.62371826171875, 565.49365234375, 479.208251953125], [0.27313995361328125, 298.85546875, 109.70098114013672, 479.644775390625], [87.37199401855469, 358.54296875, 205.9593963623047, 434.1102294921875]], 'scores': [0.9479175806045532, 0.9277512431144714, 0.5516918897628784], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 204.7ms\n",
      "Speed: 2.0ms preprocess, 204.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[93.34356689453125, 165.05224609375, 561.91357421875, 479.3583984375], [0.372222900390625, 298.87823486328125, 110.80245208740234, 479.35797119140625], [87.56901550292969, 358.5888671875, 205.5636749267578, 447.85125732421875]], 'scores': [0.9471126794815063, 0.9385899305343628, 0.48814114928245544], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 laptop, 205.0ms\n",
      "Speed: 1.5ms preprocess, 205.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[99.07661437988281, 169.626708984375, 566.0065307617188, 479.28851318359375], [0.32263946533203125, 298.92376708984375, 110.61577606201172, 479.44696044921875], [220.91732788085938, 284.42010498046875, 270.0784606933594, 389.55419921875], [0.17665481567382812, 411.3316650390625, 27.561782836914062, 434.575439453125]], 'scores': [0.9495180249214172, 0.9083929061889648, 0.8294928669929504, 0.4159686267375946], 'class_indices': [0.0, 0.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 207.2ms\n",
      "Speed: 1.7ms preprocess, 207.2ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[22.69403076171875, 172.08514404296875, 566.10693359375, 479.4156494140625], [0.33745574951171875, 299.6724548339844, 108.3538589477539, 479.2984313964844], [87.84260559082031, 360.657958984375, 141.37710571289062, 427.0992431640625]], 'scores': [0.9220420122146606, 0.9157649278640747, 0.41504356265068054], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 208.5ms\n",
      "Speed: 1.5ms preprocess, 208.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.2640533447265625, 299.58203125, 110.27879333496094, 479.5960693359375], [35.50848388671875, 173.4529571533203, 550.1461791992188, 479.44256591796875], [0.1887493133544922, 411.68634033203125, 27.154077529907227, 434.38507080078125], [87.33857727050781, 359.97589111328125, 171.2782745361328, 445.51165771484375]], 'scores': [0.9368007779121399, 0.932353675365448, 0.5179266333580017, 0.35586628317832947], 'class_indices': [0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 laptop, 1 cell phone, 200.4ms\n",
      "Speed: 1.7ms preprocess, 200.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[47.3018798828125, 171.58741760253906, 511.497802734375, 479.4993896484375], [0.35079193115234375, 299.3883056640625, 109.6669692993164, 479.41650390625], [87.61540222167969, 359.9457092285156, 185.50746154785156, 448.5727844238281], [232.12176513671875, 276.90130615234375, 261.91900634765625, 313.37567138671875], [478.15118408203125, 413.72833251953125, 555.8400268554688, 479.91326904296875]], 'scores': [0.9459075331687927, 0.934644877910614, 0.5936216711997986, 0.5128105282783508, 0.2509586811065674], 'class_indices': [0.0, 0.0, 63.0, 67.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 1 cell phone, 204.6ms\n",
      "Speed: 1.9ms preprocess, 204.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[58.27618408203125, 166.20860290527344, 510.71722412109375, 479.439453125], [0.28741455078125, 299.0068359375, 110.51673889160156, 479.558837890625], [245.90328979492188, 270.55072021484375, 263.59344482421875, 313.38604736328125], [87.1568603515625, 359.95220947265625, 193.91714477539062, 456.8572998046875], [87.25157928466797, 359.86224365234375, 193.90228271484375, 456.07720947265625]], 'scores': [0.9364872574806213, 0.9276936054229736, 0.5803489685058594, 0.35263097286224365, 0.3288370668888092], 'class_indices': [0.0, 0.0, 67.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 tv, 2 cell phones, 204.4ms\n",
      "Speed: 3.2ms preprocess, 204.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[66.3131103515625, 162.9717254638672, 513.3038940429688, 479.38861083984375], [0.33972930908203125, 298.9123229980469, 114.45800018310547, 479.5020446777344], [248.39266967773438, 261.33270263671875, 262.5429382324219, 310.27294921875], [306.469970703125, 262.12567138671875, 364.8165283203125, 299.91656494140625], [87.26815795898438, 359.4578857421875, 197.39874267578125, 421.767822265625], [482.1903076171875, 413.10791015625, 555.7581787109375, 479.7860107421875]], 'scores': [0.9331905841827393, 0.8710700869560242, 0.6490079164505005, 0.30033940076828003, 0.276228129863739, 0.2539244294166565], 'class_indices': [0.0, 0.0, 67.0, 67.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 2 laptops, 1 cell phone, 205.0ms\n",
      "Speed: 1.7ms preprocess, 205.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[76.83525085449219, 163.08489990234375, 514.5922241210938, 479.31787109375], [0.244293212890625, 299.76959228515625, 113.75776672363281, 479.55902099609375], [0.1839923858642578, 411.6280517578125, 27.749773025512695, 436.986572265625], [250.12957763671875, 261.20745849609375, 263.36358642578125, 310.18450927734375], [87.13185119628906, 359.98681640625, 200.64576721191406, 458.479736328125]], 'scores': [0.9307792782783508, 0.9234461784362793, 0.4861738085746765, 0.4066571295261383, 0.3973534107208252], 'class_indices': [0.0, 0.0, 63.0, 67.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 2 cell phones, 208.7ms\n",
      "Speed: 1.6ms preprocess, 208.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[74.12646484375, 164.2587890625, 514.826904296875, 479.296875], [0.30826568603515625, 299.61395263671875, 113.40304565429688, 479.51165771484375], [87.53704071044922, 359.71722412109375, 194.6329345703125, 419.09161376953125], [251.10231018066406, 260.45379638671875, 264.5452880859375, 307.51470947265625], [245.77713012695312, 260.620849609375, 264.3951416015625, 311.38934326171875]], 'scores': [0.9263156652450562, 0.9118195176124573, 0.32733267545700073, 0.286129891872406, 0.2514473497867584], 'class_indices': [0.0, 0.0, 62.0, 67.0, 67.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 210.8ms\n",
      "Speed: 1.4ms preprocess, 210.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[84.77857971191406, 164.6805419921875, 514.861083984375, 479.313720703125], [0.273101806640625, 298.44329833984375, 118.07633972167969, 479.47515869140625]], 'scores': [0.9220518469810486, 0.9090558886528015], 'class_indices': [0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 208.2ms\n",
      "Speed: 2.2ms preprocess, 208.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[78.72169494628906, 165.2015380859375, 514.1802978515625, 479.34796142578125], [0.27733612060546875, 299.95477294921875, 114.8853530883789, 479.39251708984375], [247.9700927734375, 264.7251892089844, 262.89862060546875, 311.1604309082031]], 'scores': [0.9285375475883484, 0.8848589658737183, 0.533254086971283], 'class_indices': [0.0, 0.0, 67.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 1 cell phone, 206.9ms\n",
      "Speed: 1.8ms preprocess, 206.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[85.45973205566406, 165.15989685058594, 513.94140625, 479.53466796875], [0.3635406494140625, 298.73291015625, 112.4498062133789, 479.4156494140625], [238.52224731445312, 280.79547119140625, 261.3783264160156, 313.68768310546875], [87.0802001953125, 359.3426208496094, 198.8275146484375, 466.6468811035156]], 'scores': [0.9201182126998901, 0.9085614681243896, 0.35341206192970276, 0.2839522063732147], 'class_indices': [0.0, 0.0, 67.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 208.9ms\n",
      "Speed: 1.8ms preprocess, 208.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[74.14991760253906, 165.5283966064453, 513.8809814453125, 479.37493896484375], [0.35970306396484375, 299.646728515625, 110.46050262451172, 479.506591796875], [245.96182250976562, 282.5592041015625, 260.07696533203125, 313.3604736328125]], 'scores': [0.925715982913971, 0.9155476093292236, 0.2691383361816406], 'class_indices': [0.0, 0.0, 67.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 1 cell phone, 205.8ms\n",
      "Speed: 1.8ms preprocess, 205.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[81.06935119628906, 165.45904541015625, 513.86083984375, 479.40338134765625], [0.26678466796875, 299.397216796875, 114.28770446777344, 479.626708984375], [87.04727935791016, 359.59796142578125, 200.14553833007812, 461.25421142578125], [289.06878662109375, 266.47161865234375, 356.65277099609375, 317.33038330078125]], 'scores': [0.9195261597633362, 0.8889064192771912, 0.35666346549987793, 0.2522645890712738], 'class_indices': [0.0, 0.0, 63.0, 67.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 1 cell phone, 206.6ms\n",
      "Speed: 1.7ms preprocess, 206.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.33251953125, 299.2919921875, 114.56636047363281, 479.4805908203125], [85.95292663574219, 165.826904296875, 513.3495483398438, 479.54241943359375], [87.29784393310547, 359.80242919921875, 197.20794677734375, 460.10284423828125], [240.0992431640625, 266.9403076171875, 259.0743408203125, 314.11737060546875]], 'scores': [0.913544237613678, 0.9131684303283691, 0.38538506627082825, 0.27761369943618774], 'class_indices': [0.0, 0.0, 63.0, 67.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 205.9ms\n",
      "Speed: 1.4ms preprocess, 205.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[64.3765869140625, 166.198486328125, 512.9962158203125, 479.33154296875], [0.3717803955078125, 299.26153564453125, 111.47145080566406, 479.55413818359375], [87.21173095703125, 359.63214111328125, 184.62069702148438, 456.81268310546875]], 'scores': [0.9238386154174805, 0.8129585385322571, 0.3092110753059387], 'class_indices': [0.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 206.2ms\n",
      "Speed: 1.9ms preprocess, 206.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[27.84625244140625, 166.0636444091797, 512.7904663085938, 479.44866943359375], [0.302978515625, 298.9673156738281, 112.61778259277344, 479.4662780761719], [87.98529052734375, 360.2906494140625, 146.25799560546875, 417.463134765625]], 'scores': [0.9446113705635071, 0.9097976088523865, 0.43966934084892273], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 cell phones, 202.4ms\n",
      "Speed: 1.8ms preprocess, 202.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[21.199020385742188, 166.92861938476562, 512.9378051757812, 479.3883972167969], [0.276397705078125, 299.52508544921875, 109.14091491699219, 479.50543212890625], [87.860107421875, 360.51220703125, 130.77291870117188, 426.0091552734375], [219.94900512695312, 280.49853515625, 271.2864990234375, 385.742431640625], [185.9849853515625, 366.0452880859375, 204.24041748046875, 389.96630859375]], 'scores': [0.9416037797927856, 0.9076688289642334, 0.4396606385707855, 0.35169607400894165, 0.3134286105632782], 'class_indices': [0.0, 0.0, 67.0, 0.0, 67.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 211.2ms\n",
      "Speed: 1.5ms preprocess, 211.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[44.957275390625, 167.761474609375, 512.4661254882812, 479.35382080078125], [0.3484344482421875, 298.94989013671875, 112.1683349609375, 479.3243408203125]], 'scores': [0.9246220588684082, 0.8942069411277771], 'class_indices': [0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 210.0ms\n",
      "Speed: 2.1ms preprocess, 210.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[36.4356689453125, 169.07748413085938, 512.9886474609375, 479.2023010253906], [0.34725189208984375, 299.76708984375, 111.0206298828125, 479.5499267578125], [87.77410888671875, 360.39752197265625, 115.160888671875, 413.84771728515625]], 'scores': [0.9198713898658752, 0.9108902812004089, 0.5557738542556763], 'class_indices': [0.0, 0.0, 67.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 207.7ms\n",
      "Speed: 2.0ms preprocess, 207.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[70.6585693359375, 169.85488891601562, 515.6006469726562, 479.2533874511719], [0.397857666015625, 299.171142578125, 100.83656311035156, 479.27630615234375], [87.67912292480469, 360.5674743652344, 127.46101379394531, 405.6378479003906]], 'scores': [0.9313178658485413, 0.9275565147399902, 0.6297072172164917], 'class_indices': [0.0, 0.0, 67.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 205.5ms\n",
      "Speed: 2.0ms preprocess, 205.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[42.78326416015625, 168.2109375, 519.6231689453125, 479.58209228515625], [12.183067321777344, 300.04669189453125, 99.09554290771484, 394.21270751953125], [223.12557983398438, 283.35211181640625, 269.6064147949219, 373.14984130859375], [0.313385009765625, 299.6317138671875, 99.10157012939453, 479.50543212890625]], 'scores': [0.928198516368866, 0.710546612739563, 0.32201868295669556, 0.2832323908805847], 'class_indices': [0.0, 0.0, 0.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 hot dog, 204.0ms\n",
      "Speed: 1.7ms preprocess, 204.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[40.23516845703125, 167.57009887695312, 522.1805419921875, 479.5397033691406], [0.30815887451171875, 298.2926025390625, 99.68875885009766, 479.5604248046875], [219.72738647460938, 284.43359375, 267.0304870605469, 386.7750244140625], [8.327011108398438, 298.8157958984375, 99.28707885742188, 387.6915283203125], [61.27372741699219, 378.670166015625, 207.0408172607422, 410.8140869140625]], 'scores': [0.9244271516799927, 0.6927495002746582, 0.6808556914329529, 0.3922065496444702, 0.29381945729255676], 'class_indices': [0.0, 0.0, 0.0, 0.0, 52.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 hot dog, 209.9ms\n",
      "Speed: 1.7ms preprocess, 209.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[47.16229248046875, 167.63543701171875, 520.5540771484375, 479.5062255859375], [0.30523681640625, 298.70703125, 99.22369384765625, 479.6785888671875], [10.840805053710938, 299.421875, 99.37629699707031, 389.31658935546875], [110.7852783203125, 355.9603271484375, 212.71664428710938, 389.90570068359375], [219.78372192382812, 297.654541015625, 263.0662536621094, 387.686767578125]], 'scores': [0.9300044775009155, 0.642164945602417, 0.5334255695343018, 0.43523284792900085, 0.25105172395706177], 'class_indices': [0.0, 0.0, 0.0, 52.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 206.3ms\n",
      "Speed: 1.7ms preprocess, 206.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[50.21673583984375, 168.3333282470703, 520.4092407226562, 479.45928955078125], [11.436958312988281, 299.3371276855469, 99.01616668701172, 396.0636291503906], [0.1689910888671875, 383.8055419921875, 80.98365783691406, 479.5609130859375], [0.243377685546875, 298.84918212890625, 98.49247741699219, 479.66162109375]], 'scores': [0.9313174486160278, 0.7318388819694519, 0.34285178780555725, 0.27929145097732544], 'class_indices': [0.0, 0.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 205.9ms\n",
      "Speed: 1.4ms preprocess, 205.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[68.87522888183594, 168.28530883789062, 518.8692626953125, 479.6458435058594], [0.417327880859375, 298.60699462890625, 99.4813232421875, 479.48822021484375], [87.63235473632812, 360.1917724609375, 121.83628845214844, 406.92138671875]], 'scores': [0.9431200623512268, 0.8388346433639526, 0.36174628138542175], 'class_indices': [0.0, 0.0, 67.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 205.2ms\n",
      "Speed: 1.9ms preprocess, 205.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[67.70710754394531, 167.88685607910156, 520.8182983398438, 479.5330810546875], [0.2742156982421875, 295.3218994140625, 109.21281433105469, 479.48260498046875], [87.48760986328125, 359.8995361328125, 179.1337890625, 430.234375], [87.3447265625, 360.1446533203125, 178.8349609375, 430.83795166015625]], 'scores': [0.9408599734306335, 0.9042205810546875, 0.3784151077270508, 0.28563418984413147], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 207.7ms\n",
      "Speed: 1.6ms preprocess, 207.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[69.33331298828125, 167.2078857421875, 519.752197265625, 479.381591796875], [0.23043060302734375, 291.5148010253906, 109.35706329345703, 479.5211486816406], [87.04798126220703, 358.9353942871094, 205.32281494140625, 453.7827453613281]], 'scores': [0.9383178949356079, 0.9238898754119873, 0.32936686277389526], 'class_indices': [0.0, 0.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 205.5ms\n",
      "Speed: 2.1ms preprocess, 205.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[95.32911682128906, 168.46771240234375, 517.1884765625, 479.5841064453125], [0.1896820068359375, 291.6119384765625, 107.38368225097656, 479.52392578125], [87.06553649902344, 357.9715576171875, 205.4803924560547, 419.0980224609375]], 'scores': [0.9450081586837769, 0.9143524765968323, 0.3306123912334442], 'class_indices': [0.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 206.6ms\n",
      "Speed: 1.8ms preprocess, 206.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[102.67259216308594, 168.36209106445312, 518.032958984375, 479.5578918457031], [0.25717926025390625, 290.1615905761719, 107.65470886230469, 479.5953674316406], [87.14204406738281, 357.8529357910156, 205.71327209472656, 419.2198791503906], [87.36712646484375, 358.4642333984375, 205.1727294921875, 462.1513671875]], 'scores': [0.9367061257362366, 0.9267944097518921, 0.29120442271232605, 0.2867076098918915], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 laptop, 207.1ms\n",
      "Speed: 2.7ms preprocess, 207.1ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[104.54454040527344, 168.3558349609375, 514.8387451171875, 479.54388427734375], [0.26218414306640625, 290.40576171875, 107.0119857788086, 479.5362548828125], [87.41203308105469, 358.89715576171875, 205.3305206298828, 419.90557861328125], [0.12398910522460938, 411.85504150390625, 27.47498321533203, 435.07049560546875]], 'scores': [0.9391298294067383, 0.9201903343200684, 0.389850914478302, 0.27068331837654114], 'class_indices': [0.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 cell phone, 207.9ms\n",
      "Speed: 2.2ms preprocess, 207.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[111.34254455566406, 166.3267822265625, 507.354248046875, 479.32623291015625], [0.30213165283203125, 288.70166015625, 106.87005615234375, 479.555419921875], [86.91482543945312, 357.92071533203125, 205.705322265625, 417.87835693359375], [285.07391357421875, 261.42724609375, 320.38446044921875, 306.35662841796875]], 'scores': [0.9284290671348572, 0.9209296107292175, 0.38220539689064026, 0.3029947578907013], 'class_indices': [0.0, 0.0, 62.0, 67.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 1 cell phone, 208.6ms\n",
      "Speed: 1.9ms preprocess, 208.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[90.00506591796875, 164.8922576904297, 504.1796875, 479.56219482421875], [0.27587127685546875, 288.57867431640625, 107.24852752685547, 479.27813720703125], [287.01092529296875, 253.95098876953125, 342.35064697265625, 330.2508544921875], [0.1352252960205078, 411.1731262207031, 27.721660614013672, 434.6418151855469]], 'scores': [0.9443238377571106, 0.9017938375473022, 0.5283812880516052, 0.48493143916130066], 'class_indices': [0.0, 0.0, 67.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 210.7ms\n",
      "Speed: 1.7ms preprocess, 210.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[79.66819763183594, 163.12945556640625, 500.9461669921875, 479.579345703125], [0.269622802734375, 287.88433837890625, 104.47479248046875, 479.57281494140625], [86.96247863769531, 358.74151611328125, 205.6835174560547, 417.57733154296875], [215.8150634765625, 284.17913818359375, 250.72296142578125, 346.47796630859375]], 'scores': [0.9388813376426697, 0.9272252917289734, 0.35216760635375977, 0.3419714570045471], 'class_indices': [0.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 1 cell phone, 208.5ms\n",
      "Speed: 1.8ms preprocess, 208.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[72.85865783691406, 164.37588500976562, 500.504638671875, 479.6147766113281], [0.2774658203125, 289.2047119140625, 107.17530822753906, 479.62664794921875], [86.94564819335938, 359.4199523925781, 205.52911376953125, 461.9104919433594], [279.361572265625, 261.2064208984375, 312.76104736328125, 304.439697265625]], 'scores': [0.9381263852119446, 0.9212962985038757, 0.5369418263435364, 0.2559332549571991], 'class_indices': [0.0, 0.0, 63.0, 67.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 2 cell phones, 207.7ms\n",
      "Speed: 2.2ms preprocess, 207.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[67.00872802734375, 165.8614044189453, 500.0103759765625, 479.60992431640625], [0.270660400390625, 288.4161376953125, 107.78886413574219, 479.62158203125], [87.23391723632812, 359.954345703125, 200.34286499023438, 459.30865478515625], [277.82427978515625, 259.83892822265625, 323.03961181640625, 345.13714599609375], [278.4501953125, 259.51080322265625, 312.6336669921875, 305.8121337890625]], 'scores': [0.9408550262451172, 0.9263759851455688, 0.4073808193206787, 0.37210357189178467, 0.3119027316570282], 'class_indices': [0.0, 0.0, 63.0, 67.0, 67.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 tv, 1 cell phone, 204.6ms\n",
      "Speed: 1.7ms preprocess, 204.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[76.11953735351562, 164.67831420898438, 500.3919982910156, 479.6186218261719], [0.2735748291015625, 289.11383056640625, 107.3260498046875, 479.5672607421875], [272.4183349609375, 259.12908935546875, 303.220703125, 303.02093505859375], [476.8712158203125, 413.4354248046875, 519.688720703125, 468.4012451171875], [87.19683837890625, 358.68511962890625, 205.9085693359375, 418.7703857421875]], 'scores': [0.94169682264328, 0.9182589054107666, 0.44873082637786865, 0.29343315958976746, 0.2528810203075409], 'class_indices': [0.0, 0.0, 67.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 tv, 1 cell phone, 206.5ms\n",
      "Speed: 1.8ms preprocess, 206.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[73.5087890625, 164.36074829101562, 499.32183837890625, 479.6464538574219], [0.28443145751953125, 288.5904541015625, 103.03189086914062, 479.57244873046875], [264.884765625, 264.7081604003906, 296.9390869140625, 302.8905334472656], [86.73590850830078, 358.83526611328125, 206.37237548828125, 417.57769775390625], [476.05218505859375, 413.8785095214844, 554.1992797851562, 479.6534729003906]], 'scores': [0.9433097243309021, 0.9115170836448669, 0.34025031328201294, 0.3187263607978821, 0.2841404974460602], 'class_indices': [0.0, 0.0, 67.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 1 cell phone, 207.4ms\n",
      "Speed: 2.3ms preprocess, 207.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[69.39581298828125, 164.00521850585938, 497.18634033203125, 479.6368103027344], [0.32073974609375, 287.22186279296875, 99.63430786132812, 479.50177001953125], [262.712890625, 266.6595458984375, 297.118896484375, 304.1219482421875], [86.67214965820312, 358.8120422363281, 205.6043701171875, 460.8368225097656]], 'scores': [0.9445188045501709, 0.9227824211120605, 0.4690552353858948, 0.27653560042381287], 'class_indices': [0.0, 0.0, 67.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 3 laptops, 211.5ms\n",
      "Speed: 2.4ms preprocess, 211.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[70.53668212890625, 163.46356201171875, 496.94598388671875, 479.65301513671875], [0.31792449951171875, 285.0240478515625, 95.40960693359375, 479.4744873046875], [0.17858123779296875, 410.789306640625, 27.42205810546875, 440.2896728515625], [86.46214294433594, 358.3165283203125, 205.9202423095703, 417.6136474609375], [86.30068969726562, 358.46417236328125, 206.04544067382812, 465.77423095703125], [475.0521240234375, 413.27313232421875, 553.8265380859375, 479.6204833984375]], 'scores': [0.9441633224487305, 0.9222468733787537, 0.3928166627883911, 0.37393519282341003, 0.3612503111362457, 0.2572171688079834], 'class_indices': [0.0, 0.0, 63.0, 63.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 laptop, 204.3ms\n",
      "Speed: 2.3ms preprocess, 204.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[77.75138854980469, 163.1709747314453, 496.17535400390625, 479.79876708984375], [0.2934722900390625, 288.64801025390625, 89.12702178955078, 479.50811767578125], [83.42137908935547, 358.70794677734375, 206.021728515625, 471.35540771484375], [49.80217361450195, 361.42388916015625, 86.35844421386719, 414.46295166015625], [475.3785400390625, 413.34375, 554.132080078125, 479.66168212890625]], 'scores': [0.9322935938835144, 0.912732720375061, 0.3899616301059723, 0.33135107159614563, 0.2730281352996826], 'class_indices': [0.0, 0.0, 63.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 laptop, 207.0ms\n",
      "Speed: 2.0ms preprocess, 207.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[66.93278503417969, 161.16485595703125, 496.77764892578125, 479.63653564453125], [0.17809295654296875, 290.08660888671875, 76.87415313720703, 478.51116943359375], [84.64886474609375, 359.10345458984375, 204.1832275390625, 417.49212646484375], [35.352325439453125, 359.8759765625, 82.04405212402344, 415.97210693359375], [0.1042633056640625, 447.7503662109375, 86.5191650390625, 479.7264404296875]], 'scores': [0.9429767727851868, 0.8684511780738831, 0.5452927947044373, 0.47931134700775146, 0.3058924973011017], 'class_indices': [0.0, 0.0, 63.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 3 chairs, 2 laptops, 203.6ms\n",
      "Speed: 1.9ms preprocess, 203.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[64.73468017578125, 161.8275146484375, 493.50787353515625, 479.62396240234375], [0.1159515380859375, 293.37939453125, 58.704002380371094, 455.585693359375], [31.374399185180664, 360.1995849609375, 83.22382354736328, 414.505859375], [0.0, 448.33673095703125, 87.32391357421875, 479.76544189453125], [85.66590118408203, 360.1240234375, 203.100341796875, 415.6055908203125], [56.25907897949219, 412.7662353515625, 144.30712890625, 468.9940185546875], [0.105926513671875, 411.33721923828125, 26.76724624633789, 455.29718017578125], [474.6544189453125, 414.11199951171875, 553.7664794921875, 479.63946533203125]], 'scores': [0.9437164664268494, 0.885864794254303, 0.8109290599822998, 0.5603851675987244, 0.552722156047821, 0.34145426750183105, 0.32803356647491455, 0.2891567051410675], 'class_indices': [0.0, 0.0, 0.0, 56.0, 63.0, 56.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 4 chairs, 1 laptop, 201.5ms\n",
      "Speed: 2.1ms preprocess, 201.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[63.128662109375, 161.5977783203125, 491.1854248046875, 479.62554931640625], [29.72447395324707, 359.89501953125, 81.99693298339844, 413.50567626953125], [85.2835693359375, 359.47308349609375, 201.59988403320312, 415.80731201171875], [0.1940155029296875, 361.96112060546875, 59.45490264892578, 455.28021240234375], [0.0, 447.17962646484375, 87.98382568359375, 479.74700927734375], [44.54109191894531, 412.03729248046875, 142.83831787109375, 466.4378662109375], [473.39208984375, 413.4873046875, 519.0892944335938, 467.5478515625], [473.4752502441406, 413.3312683105469, 553.5452880859375, 479.7409362792969]], 'scores': [0.942124605178833, 0.8026810884475708, 0.7709572315216064, 0.6310943365097046, 0.41986557841300964, 0.3539007306098938, 0.283459335565567, 0.28047481179237366], 'class_indices': [0.0, 0.0, 63.0, 0.0, 56.0, 56.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 3 chairs, 2 laptops, 204.4ms\n",
      "Speed: 2.5ms preprocess, 204.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[62.72625732421875, 161.98577880859375, 491.083251953125, 479.6290283203125], [0.10565948486328125, 366.6336669921875, 67.26444244384766, 455.488037109375], [27.16706085205078, 360.254150390625, 81.69203186035156, 414.1474609375], [0.07383537292480469, 411.5837097167969, 27.22745704650879, 456.0237731933594], [84.76957702636719, 359.89764404296875, 200.96873474121094, 415.861572265625], [0.0, 447.81463623046875, 87.66831970214844, 479.75689697265625], [52.1461296081543, 412.6990966796875, 143.25389099121094, 466.91497802734375], [474.1935119628906, 414.00299072265625, 553.5660400390625, 479.92034912109375]], 'scores': [0.939619779586792, 0.8594487309455872, 0.8386766314506531, 0.6969625949859619, 0.6688591241836548, 0.49706605076789856, 0.42001959681510925, 0.3712266981601715], 'class_indices': [0.0, 0.0, 0.0, 63.0, 63.0, 56.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 2 laptops, 202.6ms\n",
      "Speed: 2.0ms preprocess, 202.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[61.49591064453125, 162.07269287109375, 494.0052490234375, 479.7579345703125], [0.13971710205078125, 364.66070556640625, 64.09674072265625, 455.03997802734375], [27.1632080078125, 360.00103759765625, 81.73318481445312, 414.1763916015625], [84.97229766845703, 359.7887878417969, 199.1256103515625, 416.9902038574219], [0.06531524658203125, 411.6996765136719, 26.21157455444336, 455.7853088378906], [0.108123779296875, 447.2484130859375, 92.45689392089844, 479.7479248046875], [474.6204833984375, 413.6287841796875, 519.3758544921875, 468.048828125]], 'scores': [0.9308055639266968, 0.8375104665756226, 0.8200644850730896, 0.6713571548461914, 0.6372060775756836, 0.25655433535575867, 0.2510146200656891], 'class_indices': [0.0, 0.0, 0.0, 63.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 laptop, 207.0ms\n",
      "Speed: 1.7ms preprocess, 207.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[64.50662231445312, 161.2211456298828, 494.8656921386719, 479.73919677734375], [28.979143142700195, 359.8924560546875, 82.03370666503906, 413.74505615234375], [0.1970977783203125, 360.595947265625, 67.13350677490234, 455.46435546875], [85.08895111083984, 359.45123291015625, 199.49795532226562, 416.24908447265625], [0.0, 447.73681640625, 87.92658996582031, 479.758544921875]], 'scores': [0.9210277199745178, 0.826873779296875, 0.7974187731742859, 0.7363024950027466, 0.4439111351966858], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 2 chairs, 1 tv, 1 laptop, 208.4ms\n",
      "Speed: 1.6ms preprocess, 208.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[60.09173583984375, 162.2449951171875, 493.8404541015625, 479.75103759765625], [0.11641693115234375, 290.16595458984375, 64.11299133300781, 453.95489501953125], [85.07880401611328, 359.80364990234375, 198.4317626953125, 415.70770263671875], [32.78398895263672, 360.16412353515625, 80.75614929199219, 413.96441650390625], [0.2440032958984375, 447.8451232910156, 86.98698425292969, 479.7015075683594], [32.97297668457031, 360.08050537109375, 80.50471496582031, 447.42926025390625], [61.623226165771484, 412.6890563964844, 147.14834594726562, 463.7134704589844], [84.92989349365234, 359.4979248046875, 198.24630737304688, 416.1697998046875]], 'scores': [0.9312072396278381, 0.8755759000778198, 0.6191038489341736, 0.5777477025985718, 0.5747334957122803, 0.415844202041626, 0.27535247802734375, 0.25561630725860596], 'class_indices': [0.0, 0.0, 63.0, 0.0, 56.0, 0.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 3 chairs, 1 tv, 1 laptop, 207.6ms\n",
      "Speed: 1.4ms preprocess, 207.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.61552429199219, 161.1163330078125, 492.93878173828125, 479.522216796875], [0.13291168212890625, 274.9166564941406, 71.69009399414062, 454.8164978027344], [84.097412109375, 359.412841796875, 196.11575317382812, 416.371337890625], [0.2897186279296875, 448.2210693359375, 87.65676879882812, 479.758056640625], [83.98468017578125, 359.1375732421875, 196.16876220703125, 416.816650390625], [0.7796173095703125, 412.1331787109375, 145.38792419433594, 479.763916015625], [474.3436279296875, 413.5897216796875, 553.7625732421875, 479.64599609375]], 'scores': [0.9279969334602356, 0.8955117464065552, 0.5458680987358093, 0.3955664336681366, 0.3245217502117157, 0.26470455527305603, 0.26408451795578003], 'class_indices': [0.0, 0.0, 63.0, 56.0, 62.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 tv, 1 laptop, 201.7ms\n",
      "Speed: 1.8ms preprocess, 201.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[55.928497314453125, 161.94094848632812, 491.6503601074219, 479.5306091308594], [0.232574462890625, 274.8994445800781, 85.31281280517578, 461.4366760253906], [474.605224609375, 413.8023681640625, 553.903076171875, 479.65380859375], [401.56072998046875, 373.0792541503906, 451.65582275390625, 395.5666809082031], [85.33500671386719, 359.490966796875, 196.47984313964844, 417.300537109375]], 'scores': [0.9401037693023682, 0.7575153112411499, 0.31821104884147644, 0.2803657650947571, 0.2756146192550659], 'class_indices': [0.0, 0.0, 56.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 tv, 201.8ms\n",
      "Speed: 1.7ms preprocess, 201.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[59.50230407714844, 162.42698669433594, 492.1937255859375, 479.7125244140625], [0.20801544189453125, 288.59320068359375, 84.30502319335938, 458.91839599609375], [36.294044494628906, 360.1174621582031, 72.84750366210938, 383.1906433105469], [85.23658752441406, 359.15460205078125, 199.3629913330078, 417.60418701171875], [36.392181396484375, 359.76043701171875, 82.8289794921875, 413.12908935546875]], 'scores': [0.9281924366950989, 0.8017470240592957, 0.4157480299472809, 0.30783820152282715, 0.2588683068752289], 'class_indices': [0.0, 0.0, 0.0, 62.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 tv, 199.6ms\n",
      "Speed: 1.9ms preprocess, 199.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[60.53643798828125, 162.61627197265625, 492.19097900390625, 479.72406005859375], [0.187347412109375, 288.71771240234375, 82.43961334228516, 459.5804443359375], [473.89385986328125, 413.68359375, 519.5166625976562, 468.4173583984375], [85.1715087890625, 359.4971923828125, 199.53073120117188, 417.9459228515625]], 'scores': [0.9334621429443359, 0.5577661395072937, 0.2917700707912445, 0.2614027261734009], 'class_indices': [0.0, 0.0, 56.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 2 laptops, 199.7ms\n",
      "Speed: 1.7ms preprocess, 199.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[59.11283874511719, 163.21957397460938, 491.87420654296875, 479.7300720214844], [0.14739227294921875, 294.11016845703125, 71.02072143554688, 456.90032958984375], [85.08058166503906, 359.3935546875, 197.91908264160156, 417.72705078125], [32.23591613769531, 359.7535400390625, 83.10893249511719, 415.5531005859375], [401.03839111328125, 372.9371643066406, 451.50244140625, 396.0318908691406], [0.46734619140625, 447.8856201171875, 88.73323822021484, 479.73870849609375], [474.09375, 413.33203125, 553.487548828125, 479.740966796875]], 'scores': [0.9428343176841736, 0.8802257776260376, 0.4503001272678375, 0.3777075409889221, 0.3243354856967926, 0.3228597044944763, 0.3052712082862854], 'class_indices': [0.0, 0.0, 63.0, 0.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 1 laptop, 203.4ms\n",
      "Speed: 1.4ms preprocess, 203.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[59.224884033203125, 163.52395629882812, 492.1033020019531, 479.7425842285156], [0.2016143798828125, 294.80133056640625, 69.23751831054688, 458.08477783203125], [84.96478271484375, 359.19464111328125, 196.98614501953125, 417.40838623046875], [34.06239318847656, 360.31488037109375, 81.84101867675781, 415.49755859375], [84.96837615966797, 359.1208190917969, 196.83465576171875, 417.3196105957031], [0.16425323486328125, 447.8427734375, 86.50743103027344, 479.73248291015625]], 'scores': [0.9272633194923401, 0.8814029693603516, 0.3962385654449463, 0.35460755228996277, 0.2782225012779236, 0.2611277401447296], 'class_indices': [0.0, 0.0, 63.0, 0.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 2 laptops, 1 cell phone, 214.2ms\n",
      "Speed: 1.6ms preprocess, 214.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[61.0177001953125, 163.72482299804688, 492.5179443359375, 479.7544250488281], [0.14951324462890625, 294.8291015625, 69.26068115234375, 458.210693359375], [33.374298095703125, 360.51385498046875, 83.03495025634766, 414.7510986328125], [85.04835510253906, 359.58331298828125, 198.0655059814453, 416.17181396484375], [0.25286102294921875, 411.4063415527344, 27.120630264282227, 435.7928771972656], [0.415771484375, 447.65826416015625, 87.68183135986328, 479.73321533203125], [268.5911865234375, 271.4363708496094, 302.6759033203125, 314.8257751464844]], 'scores': [0.9223096370697021, 0.8993130922317505, 0.679958701133728, 0.6199337840080261, 0.3415449559688568, 0.3233206272125244, 0.27027252316474915], 'class_indices': [0.0, 0.0, 0.0, 63.0, 63.0, 56.0, 67.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 laptop, 1 cell phone, 214.1ms\n",
      "Speed: 1.8ms preprocess, 214.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[58.43974304199219, 163.79591369628906, 494.25054931640625, 479.707763671875], [0.1905364990234375, 295.93389892578125, 69.07302856445312, 457.37652587890625], [33.68431091308594, 360.3465270996094, 82.61666107177734, 415.3564758300781], [84.6171646118164, 359.3931579589844, 199.54318237304688, 416.8352355957031], [256.3795166015625, 272.1931457519531, 294.4576416015625, 347.2007141113281], [474.3392333984375, 413.81842041015625, 519.4480590820312, 468.309326171875]], 'scores': [0.9253158569335938, 0.8639388084411621, 0.4616374373435974, 0.4515955150127411, 0.27122530341148376, 0.26077184081077576], 'class_indices': [0.0, 0.0, 0.0, 63.0, 67.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 laptop, 1 cell phone, 201.2ms\n",
      "Speed: 2.0ms preprocess, 201.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.46806335449219, 163.33456420898438, 493.15777587890625, 479.5201721191406], [0.13845062255859375, 296.698486328125, 67.34854888916016, 456.8653564453125], [0.197723388671875, 447.80181884765625, 87.68951416015625, 479.71820068359375], [250.16908264160156, 276.0451354980469, 289.1165771484375, 347.1986999511719], [83.45699310302734, 359.7459716796875, 196.26358032226562, 417.568359375], [32.07379913330078, 360.25213623046875, 83.17988586425781, 415.44378662109375], [474.4072265625, 413.48187255859375, 553.896484375, 479.98651123046875]], 'scores': [0.9387552738189697, 0.882404088973999, 0.5507239103317261, 0.5011186599731445, 0.46126988530158997, 0.4066961705684662, 0.3823530077934265], 'class_indices': [0.0, 0.0, 56.0, 67.0, 63.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 2 laptops, 1 cell phone, 198.7ms\n",
      "Speed: 1.7ms preprocess, 198.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[52.93009948730469, 162.63766479492188, 492.23883056640625, 479.4936828613281], [0.1233062744140625, 296.9212951660156, 68.2756118774414, 457.0527038574219], [85.15939331054688, 359.7193298339844, 194.1551513671875, 416.8429870605469], [32.25914001464844, 360.13226318359375, 84.1187744140625, 414.68658447265625], [248.1863250732422, 279.55548095703125, 290.12738037109375, 346.56414794921875], [401.4327392578125, 372.91278076171875, 451.33612060546875, 395.61700439453125], [0.445098876953125, 447.7109375, 87.88276672363281, 479.7413330078125], [473.87322998046875, 413.4033203125, 553.5874633789062, 479.7779541015625]], 'scores': [0.9356372356414795, 0.8565362095832825, 0.5518471002578735, 0.5132313966751099, 0.37634730339050293, 0.27073490619659424, 0.2696278691291809, 0.26602670550346375], 'class_indices': [0.0, 0.0, 63.0, 0.0, 67.0, 63.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 laptop, 1 cell phone, 199.7ms\n",
      "Speed: 2.1ms preprocess, 199.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[51.72148132324219, 162.0510711669922, 491.84405517578125, 479.55267333984375], [0.18636322021484375, 296.3985595703125, 66.97087097167969, 455.99847412109375], [32.316558837890625, 360.0376281738281, 81.68511962890625, 415.5959777832031], [251.73721313476562, 279.9886169433594, 285.7424011230469, 346.4187316894531], [85.10009002685547, 359.58428955078125, 194.20660400390625, 417.1502685546875], [0.3817138671875, 447.11358642578125, 88.00454711914062, 479.7454833984375]], 'scores': [0.9353849291801453, 0.8580225110054016, 0.4337940514087677, 0.38061144948005676, 0.3615518808364868, 0.28067103028297424], 'class_indices': [0.0, 0.0, 0.0, 67.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 1 laptop, 209.5ms\n",
      "Speed: 1.7ms preprocess, 209.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[51.65533447265625, 161.8538055419922, 492.617431640625, 479.62054443359375], [0.2049407958984375, 296.95428466796875, 66.72218322753906, 456.0966796875], [0.345184326171875, 447.7281494140625, 86.03395080566406, 479.7291259765625], [32.59272003173828, 360.11334228515625, 82.59799194335938, 416.19476318359375], [85.2711181640625, 359.566162109375, 193.7242431640625, 416.80029296875], [85.34326171875, 359.591064453125, 193.56307983398438, 416.86676025390625]], 'scores': [0.9290930032730103, 0.8710060715675354, 0.4018074572086334, 0.39100030064582825, 0.3615896999835968, 0.3425264358520508], 'class_indices': [0.0, 0.0, 56.0, 0.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 1 laptop, 215.8ms\n",
      "Speed: 1.9ms preprocess, 215.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[49.1722412109375, 161.50186157226562, 492.08154296875, 479.6260681152344], [0.13961029052734375, 297.2110290527344, 67.22286224365234, 457.9961853027344], [32.56382751464844, 360.371337890625, 82.09285736083984, 414.628662109375], [84.8773422241211, 359.3607177734375, 191.86032104492188, 416.1029052734375], [84.93073272705078, 359.38507080078125, 192.177978515625, 416.62274169921875]], 'scores': [0.931779146194458, 0.8674008250236511, 0.6785458326339722, 0.46054312586784363, 0.28743115067481995], 'class_indices': [0.0, 0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 1 laptop, 201.3ms\n",
      "Speed: 1.6ms preprocess, 201.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[46.385955810546875, 161.756591796875, 492.2712707519531, 479.51861572265625], [0.1785125732421875, 298.60211181640625, 67.99247741699219, 455.40850830078125], [32.850059509277344, 360.2115478515625, 82.29117584228516, 413.9822998046875], [85.06494903564453, 359.4530029296875, 191.2996826171875, 415.9786376953125], [85.07661437988281, 359.35650634765625, 191.5564727783203, 416.26141357421875], [0.456878662109375, 447.6688537597656, 87.62850952148438, 479.7485046386719]], 'scores': [0.9395623207092285, 0.8626005053520203, 0.614706814289093, 0.4769577085971832, 0.3559854030609131, 0.26373714208602905], 'class_indices': [0.0, 0.0, 0.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 2 laptops, 202.1ms\n",
      "Speed: 2.0ms preprocess, 202.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[47.61482238769531, 162.45538330078125, 491.77020263671875, 479.37841796875], [0.17891693115234375, 299.668212890625, 66.96481323242188, 455.9769287109375], [32.53382110595703, 359.7427978515625, 82.33942413330078, 416.5281982421875], [0.17344284057617188, 411.3462219238281, 27.392475128173828, 438.5086364746094], [474.3367004394531, 413.584228515625, 553.68505859375, 479.626708984375], [84.35391235351562, 359.469970703125, 190.91708374023438, 417.4993896484375], [85.0296401977539, 359.370361328125, 191.45303344726562, 417.3275146484375], [0.3452606201171875, 447.22784423828125, 88.5704345703125, 479.75213623046875]], 'scores': [0.9365398287773132, 0.8625298142433167, 0.4503691792488098, 0.39351949095726013, 0.36194807291030884, 0.32328683137893677, 0.3028874099254608, 0.29727646708488464], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0, 63.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 1 laptop, 201.4ms\n",
      "Speed: 1.7ms preprocess, 201.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[48.3013916015625, 162.11846923828125, 491.0814208984375, 479.55767822265625], [0.12807464599609375, 299.0770263671875, 67.15599822998047, 456.6937255859375], [32.23439025878906, 360.295654296875, 82.52660369873047, 414.6016845703125], [85.12411499023438, 359.5415344238281, 191.78469848632812, 416.4189147949219], [0.38326263427734375, 447.560791015625, 86.59735107421875, 479.75634765625], [85.15115356445312, 359.3911437988281, 192.02899169921875, 416.4170837402344], [473.6873779296875, 413.41766357421875, 553.49951171875, 479.75860595703125]], 'scores': [0.9339402914047241, 0.884269654750824, 0.6405205130577087, 0.4518893361091614, 0.373963326215744, 0.3344900608062744, 0.25654423236846924], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 2 laptops, 211.8ms\n",
      "Speed: 2.4ms preprocess, 211.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[51.26300048828125, 162.28504943847656, 490.76031494140625, 479.6038818359375], [0.12896728515625, 298.99847412109375, 67.34288024902344, 458.76824951171875], [31.874267578125, 359.813232421875, 81.96473693847656, 414.62628173828125], [85.38059997558594, 359.183837890625, 192.62648010253906, 416.64013671875], [85.37161254882812, 359.40704345703125, 192.35205078125, 416.35076904296875], [473.2079772949219, 413.27362060546875, 553.472412109375, 479.79046630859375], [399.2839660644531, 373.208984375, 451.1520690917969, 396.2073974609375]], 'scores': [0.9297528862953186, 0.7908932566642761, 0.5661713480949402, 0.39089834690093994, 0.34618160128593445, 0.28429028391838074, 0.277518093585968], 'class_indices': [0.0, 0.0, 0.0, 62.0, 63.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 2 laptops, 1 cell phone, 211.6ms\n",
      "Speed: 1.7ms preprocess, 211.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[51.48992919921875, 162.44761657714844, 491.65252685546875, 479.628173828125], [0.1612396240234375, 298.668212890625, 67.11756896972656, 456.2381591796875], [32.40656280517578, 359.17572021484375, 81.8163833618164, 414.39581298828125], [257.0714111328125, 282.06524658203125, 287.31884765625, 308.67462158203125], [85.4588623046875, 359.5513916015625, 194.96041870117188, 416.1595458984375], [85.41960144042969, 359.4830322265625, 195.0078887939453, 416.43426513671875], [400.0539245605469, 372.95782470703125, 451.0518493652344, 395.78399658203125]], 'scores': [0.9317862391471863, 0.8570890426635742, 0.713646650314331, 0.528483510017395, 0.49342790246009827, 0.3201286792755127, 0.25696489214897156], 'class_indices': [0.0, 0.0, 0.0, 67.0, 63.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 4 chairs, 1 laptop, 203.4ms\n",
      "Speed: 1.8ms preprocess, 203.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[53.23667907714844, 162.99099731445312, 491.4285888671875, 479.7533264160156], [0.1308441162109375, 299.115234375, 67.30659484863281, 456.426025390625], [33.621368408203125, 360.0220947265625, 81.6108169555664, 414.336669921875], [0.566131591796875, 447.79962158203125, 86.1932144165039, 479.75421142578125], [85.0561294555664, 359.77410888671875, 195.11203002929688, 416.21868896484375], [63.945098876953125, 412.8662109375, 144.77072143554688, 461.50537109375], [473.61865234375, 413.5931396484375, 553.7054443359375, 479.5552978515625], [473.6043701171875, 413.5036315917969, 519.0303955078125, 467.7497863769531]], 'scores': [0.939094066619873, 0.8854908347129822, 0.7247839570045471, 0.5729237198829651, 0.5139631628990173, 0.33160191774368286, 0.26122361421585083, 0.25997641682624817], 'class_indices': [0.0, 0.0, 0.0, 56.0, 63.0, 56.0, 56.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 3 chairs, 1 tv, 3 laptops, 198.4ms\n",
      "Speed: 2.1ms preprocess, 198.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[50.0657958984375, 162.21975708007812, 490.48968505859375, 479.5500793457031], [0.11295318603515625, 299.15789794921875, 66.50935363769531, 457.41729736328125], [31.779983520507812, 359.9481201171875, 81.50041198730469, 414.4095458984375], [84.692626953125, 359.51812744140625, 192.87969970703125, 416.483642578125], [0.09498977661132812, 411.063720703125, 26.804981231689453, 447.1690673828125], [0.237884521484375, 447.84210205078125, 87.41778564453125, 479.73370361328125], [472.781982421875, 413.4034118652344, 553.4342041015625, 479.7010192871094], [84.77098083496094, 359.35723876953125, 193.3022003173828, 416.93890380859375], [398.7210388183594, 372.9716491699219, 451.0437316894531, 397.5530700683594], [472.564208984375, 413.5687255859375, 519.3345336914062, 468.416015625]], 'scores': [0.9389617443084717, 0.8993709087371826, 0.6216784119606018, 0.5434045195579529, 0.479386568069458, 0.40455660223960876, 0.31815725564956665, 0.31597378849983215, 0.27587610483169556, 0.26427844166755676], 'class_indices': [0.0, 0.0, 0.0, 63.0, 63.0, 56.0, 56.0, 62.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 4 chairs, 3 laptops, 199.8ms\n",
      "Speed: 2.0ms preprocess, 199.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[50.178070068359375, 162.22320556640625, 490.5840148925781, 479.39569091796875], [0.14446258544921875, 299.83477783203125, 66.88235473632812, 457.08782958984375], [30.48222541809082, 360.36273193359375, 81.27702331542969, 414.623779296875], [85.00747680664062, 359.4784240722656, 192.91384887695312, 416.1981506347656], [0.6989669799804688, 447.97149658203125, 86.78026580810547, 479.74176025390625], [472.8680419921875, 413.4122314453125, 519.1368408203125, 468.42919921875], [472.9972229003906, 413.28314208984375, 553.3262939453125, 479.72998046875], [63.69651794433594, 412.69805908203125, 143.06333923339844, 461.38812255859375], [0.07342147827148438, 410.8416748046875, 27.11455535888672, 433.527587890625], [399.1165466308594, 372.96746826171875, 451.4873962402344, 397.28350830078125]], 'scores': [0.9405643939971924, 0.8966571688652039, 0.7014864683151245, 0.6580875515937805, 0.43357786536216736, 0.3199659585952759, 0.3198474943637848, 0.2984120845794678, 0.26250123977661133, 0.2518054246902466], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0, 56.0, 56.0, 56.0, 63.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 2 laptops, 200.0ms\n",
      "Speed: 1.6ms preprocess, 200.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[50.98536682128906, 162.65020751953125, 490.7254638671875, 479.5689697265625], [0.164764404296875, 299.266357421875, 67.13722229003906, 459.29779052734375], [29.82171058654785, 360.35382080078125, 80.88053131103516, 414.54083251953125], [85.31831359863281, 359.546875, 195.06541442871094, 416.5897216796875], [472.718505859375, 413.7464599609375, 519.2079467773438, 467.9912109375], [85.15106201171875, 359.43267822265625, 195.13534545898438, 416.87799072265625], [399.24395751953125, 373.13995361328125, 451.12628173828125, 397.08843994140625]], 'scores': [0.9372115135192871, 0.8007592558860779, 0.5767576694488525, 0.4461735188961029, 0.33240392804145813, 0.30301228165626526, 0.2831915318965912], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0, 62.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 1 laptop, 200.9ms\n",
      "Speed: 2.0ms preprocess, 200.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[51.137451171875, 163.1382293701172, 490.2745361328125, 479.56402587890625], [0.182037353515625, 299.0, 68.01889038085938, 455.8050537109375], [85.03501892089844, 359.71514892578125, 194.68299865722656, 415.65020751953125], [30.749725341796875, 360.0936279296875, 82.239501953125, 415.3428955078125], [84.85699462890625, 359.6240234375, 194.76626586914062, 415.9776611328125]], 'scores': [0.933726966381073, 0.8386710286140442, 0.5632526874542236, 0.501739501953125, 0.357577383518219], 'class_indices': [0.0, 0.0, 63.0, 0.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 2 laptops, 193.5ms\n",
      "Speed: 1.7ms preprocess, 193.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[52.04289245605469, 165.04583740234375, 490.42730712890625, 479.51629638671875], [0.1658935546875, 299.50677490234375, 68.42626953125, 455.59307861328125], [31.007186889648438, 359.89404296875, 82.66827392578125, 416.92572021484375], [84.97147369384766, 359.60552978515625, 193.57479858398438, 418.1519775390625], [472.16937255859375, 413.618408203125, 519.2413330078125, 468.35546875], [397.8516845703125, 373.092041015625, 451.182373046875, 398.14520263671875], [85.18954467773438, 359.5311279296875, 193.89859008789062, 417.8106689453125]], 'scores': [0.9322882890701294, 0.784233033657074, 0.44649314880371094, 0.4293447732925415, 0.39981895685195923, 0.3921767771244049, 0.2624344229698181], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 2 laptops, 1 cell phone, 197.8ms\n",
      "Speed: 2.0ms preprocess, 197.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[49.77973937988281, 167.4308624267578, 491.60516357421875, 479.75274658203125], [0.1718902587890625, 299.71612548828125, 67.0552978515625, 459.15814208984375], [30.019254684448242, 360.1510925292969, 82.19938659667969, 415.2842712402344], [84.87787628173828, 359.7432861328125, 191.6875, 416.33184814453125], [473.49273681640625, 413.8660583496094, 519.0911254882812, 467.9964294433594], [84.91778564453125, 359.62091064453125, 191.949951171875, 416.55328369140625], [399.0101318359375, 372.8843688964844, 451.6112060546875, 396.9023132324219], [265.9063415527344, 266.57928466796875, 295.7225646972656, 310.82061767578125]], 'scores': [0.9369553327560425, 0.7718666195869446, 0.5600489377975464, 0.4648531675338745, 0.3876284956932068, 0.3266350328922272, 0.29956763982772827, 0.25223401188850403], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0, 62.0, 63.0, 67.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 2 chairs, 1 tv, 1 laptop, 215.5ms\n",
      "Speed: 2.3ms preprocess, 215.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[48.699615478515625, 169.4063720703125, 494.6832580566406, 479.431640625], [0.15885162353515625, 299.4267883300781, 68.94158935546875, 454.8580017089844], [85.03430938720703, 359.45849609375, 190.25250244140625, 416.8681640625], [30.988798141479492, 360.4944152832031, 82.84855651855469, 415.3725280761719], [0.2639617919921875, 447.3236083984375, 86.21652221679688, 479.73748779296875], [85.135986328125, 359.3682861328125, 190.51260375976562, 416.6800537109375], [474.5883483886719, 413.685791015625, 553.484375, 479.9068603515625], [0.10401725769042969, 299.49615478515625, 27.395292282104492, 370.53851318359375]], 'scores': [0.9384733438491821, 0.7236550450325012, 0.5229438543319702, 0.46476784348487854, 0.3675841689109802, 0.3276977241039276, 0.3230936527252197, 0.27764958143234253], 'class_indices': [0.0, 0.0, 63.0, 0.0, 56.0, 62.0, 56.0, 0.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 1 laptop, 212.7ms\n",
      "Speed: 2.0ms preprocess, 212.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[50.05511474609375, 170.12063598632812, 496.3543701171875, 479.6962585449219], [0.09439849853515625, 300.4897766113281, 66.59381866455078, 457.1324768066406], [32.16870880126953, 360.32293701171875, 81.88834381103516, 415.25531005859375], [85.14457702636719, 359.33837890625, 193.0695037841797, 417.1483154296875], [0.4577484130859375, 448.01947021484375, 86.65645599365234, 479.76092529296875], [85.0985107421875, 359.3990478515625, 192.87490844726562, 417.42578125], [475.2575988769531, 413.551513671875, 553.881103515625, 479.7000732421875]], 'scores': [0.9399880170822144, 0.8276433348655701, 0.515584409236908, 0.3775130808353424, 0.3704306483268738, 0.3603931963443756, 0.31703513860702515], 'class_indices': [0.0, 0.0, 0.0, 62.0, 56.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 1 laptop, 206.1ms\n",
      "Speed: 1.9ms preprocess, 206.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.44877624511719, 171.16900634765625, 495.88104248046875, 479.71380615234375], [0.05075836181640625, 300.38055419921875, 69.25152587890625, 457.57415771484375], [33.1661376953125, 360.3997802734375, 81.85401916503906, 414.98187255859375], [84.88806915283203, 359.5987548828125, 195.6414794921875, 417.64178466796875], [84.74981689453125, 359.5186767578125, 195.646728515625, 417.7393798828125]], 'scores': [0.9369014501571655, 0.6512951850891113, 0.563317060470581, 0.48555728793144226, 0.34106558561325073], 'class_indices': [0.0, 0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 laptop, 200.4ms\n",
      "Speed: 1.5ms preprocess, 200.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[70.5118408203125, 171.625, 494.24920654296875, 479.7249755859375], [0.121673583984375, 299.96417236328125, 67.99266052246094, 455.46417236328125], [84.79755401611328, 359.29345703125, 206.2586669921875, 416.9454345703125], [34.55394744873047, 360.80010986328125, 80.49057006835938, 414.17071533203125], [0.1266937255859375, 447.9424743652344, 89.64189910888672, 479.7268981933594]], 'scores': [0.9328866004943848, 0.8042930960655212, 0.5848816633224487, 0.5542237758636475, 0.4224373996257782], 'class_indices': [0.0, 0.0, 63.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 3 laptops, 204.1ms\n",
      "Speed: 1.8ms preprocess, 204.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[71.03082275390625, 172.2723388671875, 492.2198486328125, 479.72515869140625], [0.0937347412109375, 300.41607666015625, 71.80542755126953, 456.99713134765625], [32.92372131347656, 360.1416015625, 81.11608123779297, 414.44586181640625], [85.26289367675781, 358.69451904296875, 205.86399841308594, 418.259033203125], [0.16683197021484375, 447.79522705078125, 86.5875244140625, 479.73162841796875], [85.20321655273438, 358.68939208984375, 205.72549438476562, 418.06512451171875], [0.1331634521484375, 411.40264892578125, 27.07769203186035, 433.03851318359375], [474.12451171875, 413.5243225097656, 554.3494873046875, 479.9944152832031], [401.2928466796875, 372.9951477050781, 450.9388427734375, 396.0302429199219]], 'scores': [0.9334557056427002, 0.7939044237136841, 0.6519048810005188, 0.41680312156677246, 0.37616267800331116, 0.34155309200286865, 0.3227083384990692, 0.30152279138565063, 0.25308918952941895], 'class_indices': [0.0, 0.0, 0.0, 63.0, 56.0, 62.0, 63.0, 56.0, 63.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 2 chairs, 1 tv, 1 laptop, 196.9ms\n",
      "Speed: 1.6ms preprocess, 196.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[54.63433837890625, 172.5380859375, 492.07952880859375, 479.61639404296875], [0.10822296142578125, 300.3060302734375, 67.198486328125, 456.461669921875], [84.34317779541016, 359.4402770996094, 204.98739624023438, 416.4554138183594], [33.288475036621094, 359.9295654296875, 81.38099670410156, 414.119140625], [474.1327209472656, 413.662841796875, 554.1668701171875, 479.656982421875], [84.29922485351562, 359.1700439453125, 204.89593505859375, 416.825927734375], [0.4395751953125, 447.48052978515625, 84.09194946289062, 479.74346923828125]], 'scores': [0.9407567977905273, 0.8846043348312378, 0.6920109987258911, 0.6856391429901123, 0.3383660316467285, 0.30848902463912964, 0.2951350510120392], 'class_indices': [0.0, 0.0, 63.0, 0.0, 56.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 1 laptop, 201.7ms\n",
      "Speed: 1.7ms preprocess, 201.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[32.1202392578125, 173.76976013183594, 494.70050048828125, 479.5303955078125], [0.13127899169921875, 299.66033935546875, 67.22772216796875, 456.03692626953125], [33.670066833496094, 361.0011901855469, 81.74929809570312, 415.3838195800781], [84.30648803710938, 360.001953125, 168.31732177734375, 417.3377685546875], [84.39105224609375, 360.0199890136719, 168.28884887695312, 417.8824768066406], [474.8885498046875, 413.78985595703125, 553.8253173828125, 479.62591552734375]], 'scores': [0.936390221118927, 0.8656002283096313, 0.512018620967865, 0.3706057071685791, 0.2816326320171356, 0.2574765384197235], 'class_indices': [0.0, 0.0, 0.0, 62.0, 63.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 1 tv, 195.6ms\n",
      "Speed: 1.8ms preprocess, 195.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[64.37953186035156, 174.9498291015625, 498.151123046875, 479.51763916015625], [0.13477325439453125, 300.72882080078125, 66.75837707519531, 455.87640380859375], [31.34535789489746, 360.4111328125, 81.83780670166016, 414.8936767578125], [85.24237060546875, 359.9832763671875, 165.17010498046875, 417.145263671875], [476.1348876953125, 413.6246337890625, 519.6611328125, 468.791748046875]], 'scores': [0.8832640051841736, 0.8179682493209839, 0.6000147461891174, 0.4343883991241455, 0.34535878896713257], 'class_indices': [0.0, 0.0, 0.0, 62.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 2 chairs, 1 tv, 200.0ms\n",
      "Speed: 1.7ms preprocess, 200.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[31.385391235351562, 177.5821990966797, 503.3438720703125, 479.50079345703125], [0.1831512451171875, 300.248046875, 66.53749084472656, 457.6578369140625], [31.1220645904541, 360.24334716796875, 81.10550689697266, 415.1842041015625], [474.8052978515625, 413.655517578125, 519.5501708984375, 468.0621337890625], [85.18521118164062, 359.4833984375, 193.38021850585938, 416.744140625], [219.01040649414062, 273.29937744140625, 266.37701416015625, 349.94952392578125], [474.81109619140625, 413.5705261230469, 544.7783203125, 479.9001770019531]], 'scores': [0.9201244711875916, 0.8371025323867798, 0.49016526341438293, 0.46983641386032104, 0.4281708300113678, 0.2803926467895508, 0.27630409598350525], 'class_indices': [0.0, 0.0, 0.0, 56.0, 62.0, 0.0, 56.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 2 tvs, 1 laptop, 203.7ms\n",
      "Speed: 1.7ms preprocess, 203.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[32.8153076171875, 189.55271911621094, 509.12652587890625, 479.4471435546875], [0.1658477783203125, 301.10198974609375, 67.4404525756836, 455.79998779296875], [216.89639282226562, 273.670654296875, 282.9999084472656, 359.6337890625], [30.853675842285156, 361.103515625, 81.83880615234375, 415.2647705078125], [472.4344482421875, 414.57354736328125, 519.1117553710938, 468.60198974609375], [85.24628448486328, 359.9466857910156, 204.98687744140625, 417.1221008300781], [393.645751953125, 373.442138671875, 451.418701171875, 406.0748291015625], [85.17715454101562, 359.13897705078125, 205.06439208984375, 417.35675048828125]], 'scores': [0.9232950210571289, 0.8454027771949768, 0.7450187802314758, 0.5463200211524963, 0.4527584910392761, 0.44119444489479065, 0.44005516171455383, 0.33774086833000183], 'class_indices': [0.0, 0.0, 0.0, 0.0, 56.0, 63.0, 62.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 1 tv, 1 laptop, 209.2ms\n",
      "Speed: 2.1ms preprocess, 209.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[39.109771728515625, 199.6231689453125, 503.4671325683594, 479.517578125], [468.17572021484375, 413.65582275390625, 519.0975952148438, 479.86907958984375], [218.78477478027344, 273.3907775878906, 294.0372314453125, 373.3103332519531], [0.1253662109375, 300.58978271484375, 67.33589172363281, 460.70855712890625], [31.133148193359375, 360.2128601074219, 82.74018096923828, 415.0461120605469], [85.11382293701172, 359.46478271484375, 205.46722412109375, 416.173095703125], [394.28656005859375, 373.4261474609375, 451.10052490234375, 413.5465087890625]], 'scores': [0.9241429567337036, 0.7610819935798645, 0.7413123250007629, 0.7108805775642395, 0.6439612507820129, 0.6041806936264038, 0.5225581526756287], 'class_indices': [0.0, 56.0, 0.0, 0.0, 0.0, 63.0, 62.0]}]\n",
      "Video frames: 1\n",
      "\n",
      "0: 480x640 4 persons, 3 chairs, 2 tvs, 1 laptop, 208.1ms\n",
      "Speed: 1.8ms preprocess, 208.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Predictions made: [{'boxes': [[0.12187957763671875, 300.64398193359375, 66.13796997070312, 459.27655029296875], [107.67236328125, 206.7051544189453, 508.26751708984375, 479.57550048828125], [217.41648864746094, 273.17779541015625, 306.0975341796875, 382.11334228515625], [471.48626708984375, 414.18988037109375, 519.62548828125, 479.66900634765625], [30.516921997070312, 360.64532470703125, 82.69792175292969, 415.46710205078125], [0.31435394287109375, 448.05096435546875, 85.91414642333984, 479.73211669921875], [85.4412841796875, 359.271728515625, 205.68685913085938, 417.3494873046875], [85.50362396240234, 359.44671630859375, 205.64102172851562, 417.56524658203125], [419.54071044921875, 373.4261474609375, 450.89581298828125, 405.0552978515625], [62.48794174194336, 413.6595458984375, 145.16741943359375, 463.6033935546875]], 'scores': [0.8962481021881104, 0.8935000896453857, 0.7411300539970398, 0.6764896512031555, 0.5962650179862976, 0.4426878094673157, 0.4299631714820862, 0.4158726930618286, 0.3434230089187622, 0.2789180278778076], 'class_indices': [0.0, 0.0, 0.0, 56.0, 0.0, 56.0, 62.0, 63.0, 62.0, 56.0]}]\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, List\n",
    "from ultralytics import YOLO\n",
    "from inference import InferencePipeline\n",
    "from inference.core.interfaces.stream.sinks import render_boxes\n",
    "from inference.core.interfaces.camera.entities import VideoFrame\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class MyModel:\n",
    "\n",
    "    def __init__(self, weights_path: str):\n",
    "        # Load the YOLO model\n",
    "        self._model = YOLO(weights_path)\n",
    "        print(\"Modèle chargé avec succès\")\n",
    "\n",
    "    def infer(self, video_frames: List[VideoFrame]) -> List[Any]:\n",
    "        print('Video frames:', len(video_frames))\n",
    "        \n",
    "        # Convert the list of images to the format expected by YOLO\n",
    "        images = [v.image for v in video_frames]\n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        images_np = [np.array(img) for img in images]\n",
    "\n",
    "        # Make predictions\n",
    "        results = self._model(images_np)\n",
    "\n",
    "        # Extract the necessary data from results\n",
    "        predictions = []\n",
    "        for result in results:\n",
    "            boxes = result.boxes.xyxy.tolist() if result.boxes else []\n",
    "            scores = result.boxes.conf.tolist() if result.boxes else []\n",
    "            class_indices = result.boxes.cls.tolist() if result.boxes else []\n",
    "            \n",
    "            # Create a dictionary  for each prediction\n",
    "            prediction = {\n",
    "                'boxes': boxes,\n",
    "                'scores': scores,\n",
    "                'class_indices': class_indices\n",
    "            }\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        print(\"Predictions made:\", predictions)\n",
    "        return predictions\n",
    "        # for result in results:\n",
    "        #     if hasattr(result, 'boxes') and hasattr(result, 'scores') and hasattr(result, 'class_indices'):\n",
    "        #         boxes = result.boxes.xyxy.tolist()  # Assuming xyxy format, adjust if necessary\n",
    "        #         scores = result.scores.tolist()\n",
    "        #         class_indices = result.class_indices.tolist()\n",
    "                \n",
    "        #         # Create a dictionary for each prediction\n",
    "        #         prediction = {\n",
    "        #             'boxes': boxes,\n",
    "        #             'scores': scores,\n",
    "        #             'class_indices': class_indices\n",
    "        #         }\n",
    "        #         predictions.append(prediction)\n",
    "        #     else:\n",
    "        #         print(\"Unexpected result format:\", result)\n",
    "\n",
    "        # print(\"Predictions made:\", predictions)\n",
    "        # return predictions\n",
    "\n",
    "my_model = MyModel(\"../shifumi_trained.pt\")\n",
    "pipeline = InferencePipeline.init_with_custom_logic(\n",
    "    on_video_frame=my_model.infer,\n",
    "    video_reference=0,  # Ensure this is the correct device ID for your webcam\n",
    "    on_prediction=render_boxes,  # Function to run after each prediction\n",
    ")\n",
    "\n",
    "pipeline.start()\n",
    "pipeline.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
